<div class="container">

<table style="width: 100%;"><tr>
<td>Choose the number of principal components via reconstruction error</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>
Choose the number of principal components via reconstruction error
</h2>

<h3>Description</h3>

<p>Choose the number of principal components via reconstruction error.
</p>


<h3>Usage</h3>

<pre><code class="language-R">pc.choose(x, graph = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>

<p>A numerical matrix with more rows than columns.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>graph</code></td>
<td>

<p>Should the plot of the PRESS values appear? Default value is TRUE.
</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>SVD stands for Singular Value Decomposition of a rectangular matrix. That is any matrix, not only a square one in contrast to the Spectral Decomposition with eigenvalues and eigenvectors, produced by principal component analysis (PCA). Suppose we have a <code class="reqn">n \times p</code> matrix <code class="reqn">\bf X</code>. Then using SVD we can write the matrix as </p>
<p style="text-align: center;"><code class="reqn">
{\bf X}={\bf UDV}^{T},
</code>
</p>

<p>where <code class="reqn">\bf U</code> is an orthonormal matrix containing the eigenvectors of <code class="reqn">{\bf XX}^T</code>, the <code class="reqn">\bf V</code> is an orthonormal matrix containing the eigenvectors of <code class="reqn">{\bf X}^T{\bf X}</code> and <code class="reqn">D</code> is a <code class="reqn">p \times p</code> diagonal matrix containing the <code class="reqn">r</code> non zero singular values <code class="reqn">d_1,\ldots,d_r</code> (square root of the eigenvalues) of <code class="reqn">{\bf XX}^T</code> (or <code class="reqn">{\bf X}^T{\bf X}</code>) and the remaining <code class="reqn">p-r</code> elements of the diagonal are zero. We remind that the maximum rank of an <code class="reqn">n \times p</code> matrix is equal to <code class="reqn">\min\{n,p\}</code>. Using the SVD decomposition equaiton above, each column of <code class="reqn">\bf X</code> can be written as
</p>
<p style="text-align: center;"><code class="reqn">
{\bf x}_j=\sum_{k=1}^r{\bf u}_kd_k{\bf v}_{jk}.
</code>
</p>

<p>This means that we can reconstruct the matrix <code class="reqn">\bf X</code> using less columns (if <code class="reqn">n&gt;p</code>) than it has.
</p>
<p style="text-align: center;"><code class="reqn">
\tilde{{\bf x}}^{m}_j=\sum_{k=1}^m{\bf u}_kd_k{\bf v}_{jk},
</code>
</p>

<p>where <code class="reqn">m&lt;r</code>.
</p>
<p>The reconstructed matrix will have some discrepancy of course, but it is the level of discrepancy we are interested in. If we center the matrix <code class="reqn">\bf X</code>, subtract the column means from every column, and perform the SVD again, we will see that the orthonormal matrix <code class="reqn">\bf V</code> contains the eigenvectors of the covariance matrix of the original, the un-centred, matrix <code class="reqn">\bf X</code>.
</p>
<p>Coming back to the a matrix of <code class="reqn">n</code> observations and <code class="reqn">p</code> variables, the question was how many principal components to retain. We will give an answer to this using SVD to reconstruct the matrix. We describe the steps of this algorithm below.
1. Center the matrix by subtracting from each variable its mean <code class="reqn">{\bf Y}={\bf X}-{\bf m}</code>
</p>
<p>2. Perform SVD on the centred matrix <code class="reqn">\bf Y</code>.
</p>
<p>3. Choose a number from <code class="reqn">1</code> to <code class="reqn">r</code> (the rank of the matrix) and reconstruct the matrix. Let us denote by <code class="reqn">\widetilde{{\bf Y}}^{m}</code> the reconstructed matrix.
</p>
<p>4. Calculate the sum of squared differences between the reconstructed and the original values
</p>
<p style="text-align: center;"><code class="reqn">
PRESS\left(m\right)=\sum_{i=1}^n\sum_{j=1}^p\left(\tilde{y}^{m}_{ij}-y_{ij}\right)^2, m=1,..,r.
</code>
</p>

<p>5. Plot <code class="reqn">PRESS\left(m\right)</code> for all the values of <code class="reqn">m</code> and choose graphically the number of principal components.
</p>
<p>The graphical way of choosing the number of principal components is not the best and there alternative ways of making a decision (see for example Jolliffe (2002)).
</p>


<h3>Value</h3>

<p>A list including:
</p>
<table>
<tr style="vertical-align: top;">
<td><code>values</code></td>
<td>

<p>The eigenvalues of the covariance matrix.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>cumprop</code></td>
<td>

<p>The cumulative proportion of the eigenvalues of the covariance matrix.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>per</code></td>
<td>

<p>The differences in the cumulative proportion of the eigenvalues of the covariance matrix.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>press</code></td>
<td>

<p>The reconstruction error <code class="reqn">\sqrt{\sum_{ij}{(x_{ij}-\hat{x}_{ij})^2}}</code> for each number of eigenvectors.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>runtime</code></td>
<td>

<p>The runtime of the algorithm.
</p>
</td>
</tr>
</table>
<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>References</h3>

<p>Jolliffe I.T. (2002). Principal Component Analysis.
</p>


<h3>See Also</h3>

<p><code>eigci
</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">x &lt;- as.matrix(iris[, 1:4])
a &lt;- pc.choose(x, graph = FALSE)
</code></pre>


</div>