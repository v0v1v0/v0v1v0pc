<div class="container">

<table style="width: 100%;"><tr>
<td>tunecpfa</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>
Tuning for Classification with Parallel Factor Analysis
</h2>

<h3>Description</h3>

<p>Fits Richard A. Harshman's Parallel Factor Analysis-1 (Parafac) model or Parallel Factor Analysis-2 (Parafac2) model to a three-way or four-way data array. Allows for multiple constraint options on tensor modes. Uses component weights from a single mode of the model as predictors to tune parameters for one or more classification methods via a k-fold cross-validation procedure. Supports binary and multiclass classification.
</p>


<h3>Usage</h3>

<pre><code class="language-R">tunecpfa(x, y, model = c("parafac", "parafac2"), nfac = 1, nfolds = 10,
         method = c("PLR", "SVM", "RF", "NN", "RDA", "GBM"), 
         family = c("binomial", "multinomial"), parameters = list(), 
         foldid = NULL, prior = NULL, cmode = NULL, parallel = FALSE, 
         cl = NULL, verbose = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>

<p>For Parafac or Parafac2, a three-way or four-way data array. For Parafac2, can be a list of length <code>K</code> where the <code>k</code>-th element is a matrix or three-way array associated with the <code>k</code>-th element. Array or list must contain real numbers. See note below.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>y</code></td>
<td>

<p>A vector containing at least two unique class labels. Should be a factor that contains two or more levels . For binary case, ensure the order of factor levels (left to right) is such that negative class is first and positive class is second.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>model</code></td>
<td>

<p>Character designating the Parafac model to use, either <code>model = "parafac"</code> to fit the Parafac model or <code>model = "parafac2"</code> to fit the Parafac2 model.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nfac</code></td>
<td>

<p>Number of components for each Parafac or Parafac2 model to fit. Default is <code>nfac = 1</code>.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nfolds</code></td>
<td>

<p>Numeric setting number of folds for k-fold cross-validation. Must be 2 or greater. Default is <code>nfolds = 10</code>.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>method</code></td>
<td>

<p>Character vector indicating classification methods to use. Possible methods include penalized logistic regression (PLR); support vector machine (SVM); random forest (RF); feed-forward neural network (NN); regularized discriminant analysis (RDA); and gradient boosting machine (GBM). If none selected, default is to use all methods.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>family</code></td>
<td>

<p>Character value specifying binary classification (<code>family = "binomial"</code>) or multiclass classification (<code>family = "multinomial"</code>). If not provided, number of levels of input <code>y</code> is used, where two levels is binary, and where three or more levels is multiclass.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>parameters</code></td>
<td>

<p>List containing arguments related to classification methods. When specified, must contain one or more of the following:
</p>

<dl>
<dt>alpha</dt>
<dd>
<p>Values for penalized logistic regression alpha parameter; default is <code>alpha = seq(0, 1, length = 6)</code>. Must be numeric and contain only real numbers between 0 and 1, inclusive.
</p>
</dd>
<dt>lambda</dt>
<dd>
<p>Optional user-supplied lambda sequence for <code>cv.glmnet</code> for penalized logistic regression. Default is NULL.
</p>
</dd>
<dt>cost</dt>
<dd>
<p>Values for support vector machine cost parameter; default is <code>cost = c(1, 2, 4, 8, 16, 32, 64)</code>. Must be numeric and contain only real numbers greater than or equal to zero.
</p>
</dd>
<dt>gamma</dt>
<dd>
<p>Values for support vector machine gamma parameter; default is <code>gamma = c(0, 0.01, 0.1, 1, 10, 100, 1000)</code>. Must be numeric and greater than or equal to 0.
</p>
</dd>
<dt>ntree</dt>
<dd>
<p>Values for random forest number of trees parameter; default is <code>ntree = c(100, 200, 400, 600, 800, 1600, 3200)</code>. Must be numeric and contain only integers greater than or equal to 1.
</p>
</dd>
<dt>nodesize</dt>
<dd>
<p>Values for random forest node size parameter; default is <code>nodesize = c(1, 2, 4, 8, 16, 32, 64)</code>. Must be numeric and contain only integers greater than or equal to 1.
</p>
</dd>
<dt>size</dt>
<dd>
<p>Values for neural network size parameter; default is <code>size = c(1, 2, 4, 8, 16, 32, 64)</code>. Must be numeric and contain only integers greater than or equal to 0.
</p>
</dd>
<dt>decay</dt>
<dd>
<p>Values for neural network decay parameter; default is <code>decay = c(0.001, 0.01, 0.1, 1, 2, 4, 8, 16)</code>. Must be numeric and contain only real numbers.
</p>
</dd>
<dt>rda.alpha</dt>
<dd>
<p>Values for regularized discriminant analysis alpha parameter; default is <code>rda.alpha = seq(0, 0.999, length = 6)</code>. Must be numeric and contain only real numbers between 0 (inclusive) and 1 (exclusive).
</p>
</dd>
<dt>delta</dt>
<dd>
<p>Values for regularized discriminant analysis delta parameter; default is <code>delta = c(0, 0.1, 1, 2, 3, 4)</code>. Must be numeric and contain only real numbers greater than or equal to 0.
</p>
</dd>
<dt>eta</dt>
<dd>
<p>Values for gradient boosting machine eta parameter; default is <code>eta = c(0.1, 0.3, 0.5, 0.7, 0.9)</code>. Must be numeric and contain only real numbers greater than 0 and less than 1.
</p>
</dd>
<dt>max.depth</dt>
<dd>
<p>Values for gradient boosting machine max.depth parameter; default is <code>max.depth = c(1, 2, 3, 4)</code>. Must be numeric and contain only integers greater than or equal to 1.
</p>
</dd>
<dt>subsample</dt>
<dd>
<p>Values for gradient boosting machine subsample parameter; default is <code>subsample = c(0.6, 0.7, 0.8, 0.9)</code>. Must be numeric and contain only real numbers greater than 0 and less than or equal to 1.
</p>
</dd>
<dt>nrounds</dt>
<dd>
<p>Values for gradient boosting machine nrounds parameter; default is <code>nrounds = c(100, 200, 300, 500)</code>. Must be numeric and contain only integers greater than or equal to 1.
</p>
</dd>
</dl>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>foldid</code></td>
<td>

<p>Vector containing fold IDs for k-fold cross-validation. Can be of class integer, numeric, or data frame. Should contain integers from 1 through the number of folds. If not provided, fold IDs are generated randomly for observations using 1 through the number of folds <code>nfolds</code>.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>prior</code></td>
<td>

<p>Prior probabilities of class membership. If unspecified, the class proportions for input <code>y</code> are used. If specified, the probabilities should be in the order of the factor levels of input <code>y</code>.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>cmode</code></td>
<td>

<p>Integer value of 1, 2, or 3 (or 4 if <code>x</code> is a four-way array) specifying the mode whose component weights will be predictors for classification. Defaults to the last mode of the inputted array (i.e., defaults to 3 for three-way array, and to 4 for four-way array). If <code>model = "parafac2"</code>, last mode will be used.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>parallel</code></td>
<td>

<p>Logical indicating if parallel computing should be implemented. If TRUE, the package <b>parallel</b> is used for parallel computing. For all classification methods except penalized logistic regression, the <b>doParallel</b> package is used as a wrapper. Defaults to FALSE, which implements sequential computing.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>cl</code></td>
<td>

<p>Cluster for parallel computing, which is used when <code>parallel = TRUE</code>. Note that if <code>parallel = TRUE</code> and <code>cl = NULL</code>, then the cluster is defined as <code>makeCluster(detectCores())</code>.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>verbose</code></td>
<td>

<p>If TRUE, progress is printed.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>

<p>Additional arguments to be passed to function <code>parafac</code> for fitting a Parafac model or function <code>parafac2</code> for fitting a Parafac2 model. Example: can impose different constraints on different modes of the input array using the argument <code>const</code>. See help file for function <code>parafac</code> or for function <code>parafac2</code> for additional details.
</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>After fitting a Parafac or Parafac2 model with package <b>multiway</b> (see <code>parafac</code> or <code>parafac2</code> in <b>multiway</b> for details), the estimated classification mode weight matrix is passed to one or several of six classification methodsâ€“including penalized logistic regression (PLR); support vector machine (SVM); random forest (RF); feed-forward neural network (NN); regularized discriminant analysis (RDA); and gradient boosting machine (GBM).
</p>
<p>Package <b>glmnet</b> fits models for PLR. PLR tunes penalty parameter lambda while the elastic net parameter alpha is set by the user (see the help file for function <code>cv.glmnet</code> in package <b>glmnet</b>). For SVM, package <b>e1071</b> is used with a radial basis kernel. Penalty parameter cost and radial basis parameter gamma are used (see <code>svm</code> in package <b>e1071</b>). For RF, package <b>randomForest</b> is used and implements Breiman's random forest algorithm. The number of predictors sampled at each node split is set at the default of sqrt(R), where R is the number of Parafac or Parafac2 components. Two tuning parameters allowed are ntree, the number of trees to be grown, and nodesize, the minimum size of terminal nodes (see <code>randomForest</code> in package <b>randomForest</b>). For NN, package <b>nnet</b> fits a single-hidden-layer, feed-forward neural network model. Penalty parameters size (i.e., number of hidden layer units) and decay (i.e., weight decay) are used (see <b>nnet</b>). For RDA, package <b>rda</b> fits a shrunken centroids regularized discriminant analysis model. Tuning parameters include rda.alpha, the shrinkage penalty for the within-class covariance matrix, and delta, the shrinkage penalty of class centroids towards the overall dataset centroid. For GBM, package <b>xgboost</b> fits a gradient boosting machine model. Four tuning parameters are allowed: (1) eta, the learning rate; (2) max.depth, the maximum tree depth; (3) subsample, the fraction of samples per tree; and (4) nrounds, the number of boosting trees to build.
</p>
<p>For all six methods, k-fold cross-validation is implemented to tune classification parameters where the number of folds is set by argument <code>nfolds</code>.
</p>


<h3>Value</h3>

<p>Returns an object of class <code>tunecpfa</code> with the following elements:
</p>
<table>
<tr style="vertical-align: top;">
<td><code>opt.model</code></td>
<td>

<p>List containing optimal model for tuned classification methods for each Parafac or Parafac2 model that was fit.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>opt.param</code></td>
<td>

<p>Data frame containing optimal parameters for tuned classification methods.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>kcv.error</code></td>
<td>

<p>Data frame containing KCV misclassification error for optimal parameters for tuned classification methods.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>est.time</code></td>
<td>

<p>Data frame containing times for fitting Parafac or Parafac2 model and for tuning classification methods.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>method</code></td>
<td>

<p>Numeric indicating classification methods used. Value of '1' indicates 'PLR'; value of '2' indicates 'SVM'; value of '3' indicates 'RF'; value of '4' indicates 'NN'; value of '5' indicates 'RDA'; and value of '6' indicates 'GBM'.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>

<p>Three-way or four-way array used. If a list was used with <code>model = "parafac2"</code>, returns list of matrices or three-way arrays used.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>y</code></td>
<td>

<p>Factor containing class labels used. Note that output <code>y</code> is recoded such that the input labels of <code>y</code> are converted to numeric integers from 0 through the number of levels, which are then applied as labels for output <code>y</code>. 
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Aweights</code></td>
<td>

<p>List containing estimated A weights for each Parafac or Parafac2 model that was fit.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Bweights</code></td>
<td>

<p>List containing estimated B weights for each Parafac or Parafac2 model that was fit.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Cweights</code></td>
<td>

<p>List containing estimated C weights for each Parafac or Parafac2 model that was fit. Null if inputted argument <code>x</code> was a three-way array.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Phi</code></td>
<td>

<p>If <code>model = "parafac2"</code>, a list containing estimated <code>Phi</code> from the Parafac2 model. <code>Phi</code> is the common cross product matrix shared by all levels of the last mode (see help file for function <code>parafac2</code> in package <b>multiway</b> for additional details). NULL if <code>model = "parafac"</code>.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>const</code></td>
<td>

<p>Constraints used in fitting Parafac or Parafac2 models. If argument <code>const</code> was not inputted, no constraints will be used.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>cmode</code></td>
<td>

<p>Integer value of 1, 2, or 3 (or 4 if <code>x</code> is a four-way array) specifying mode whose component weights were predictors for classification.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>family</code></td>
<td>

<p>Character value specifying whether classification was binary (<code>family = "binomial"</code>) or multiclass (<code>family = "multinomial"</code>).
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>xdim</code></td>
<td>

<p>Numeric value specifying number of levels for each mode of input <code>x</code>. If <code>model = "parafac2"</code>, number of levels for first mode is designated as <code>NA</code> because the number of levels can differ across levels of the last mode.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lxdim</code></td>
<td>

<p>Numeric value specifying number of modes of input <code>x</code>.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>train.weights</code></td>
<td>

<p>List containing classification component weights for each fit Parafac or Parafac2 model, for possibly different numbers of components. The weights used to train classifiers.
</p>
</td>
</tr>
</table>
<h3>Note</h3>

<p>For fitting the Parafac model, if argument <code>cmode</code> is not null, input array <code>x</code> is reshaped with function <code>aperm</code> such that the <code>cmode</code> dimension of <code>x</code> is ordered last. Estimated mode A and B (and mode C for a four-way array) weights that are outputted as <code>Aweights</code> and <code>Bweights</code> (and <code>Cweights</code>) reflect this permutation. For example, if <code>x</code> is a four-way array and <code>cmode = 2</code>, the original input modes 1, 2, 3, and 4 will correspond to output modes 1, 3, 4, 2. Here, output A = input 1; B = 3, and C = 4 (i.e., the second mode specified by <code>cmode</code> has been moved to the D mode/last mode). For <code>model = "parafac2"</code>, classification mode is assumed to be the last mode (i.e., mode C for three-way array and mode D for four-way array).
</p>
<p>In addition, note that the following combination of arguments will give an error: <code>nfac = 1, family = "multinomial", method = "PLR"</code>. The issue arises from providing <code>glmnet::cv.glmnet</code> input <code>x</code> a matrix with a single column. The issue is resolved for <code>family = "binomial"</code> because a column of 0s is appended to the single column, but this solution does not appear to work for the multiclass case. As such, this combination of arguments is not currently allowed. This issue will be resolved in a future update.
</p>


<h3>Author(s)</h3>

<p>Matthew A. Snodgress &lt;snodg031@umn.edu&gt;
</p>


<h3>References</h3>

<p>Breiman, L. (2001). Random forests. Machine Learning, 45(1), 5-32.
</p>
<p>Chen, T., He, T., Benesty, M., Khotilovich, V., Tang, Y., Cho, H., Chen, K., Mitchell, R., Cano, I., Zhou, T., Li, M., Xie, J., Lin, M., Geng, Y., Li, Y., Yuan, J. (2024). xgboost: Extreme gradient boosting. R Package Version 1.7.7.1.
</p>
<p>Cortes, C. and Vapnik, V. (1995). Support-vector networks. Machine Learning, 20(3), 273-297.
</p>
<p>Friedman, J. H. (2001). Greedy function approximation: a gradient boosting machine. Annals of Statistics, 29(5), 1189-1232.
</p>
<p>Friedman, J. H. (1989). Regularized discriminant analysis. Journal of the American Statistical Association, 84(405), 165-175.
</p>
<p>Friedman, J. Hastie, T., and Tibshirani, R. (2010). Regularization paths for generalized linear models via coordinate descent. Journal of Statistical Software, 33(1), 1-22.
</p>
<p>Guo, Y., Hastie, T., and Tibshirani, R. (2007). Regularized linear discriminant analysis and its application in microarrays. Biostatistics, 8(1), 86-100.
</p>
<p>Guo Y., Hastie T., and Tibshirani, R. (2023). rda: Shrunken centroids regularized discriminant analysis. R Package Version 1.2-1.
</p>
<p>Harshman, R. (1970). Foundations of the PARAFAC procedure: Models and conditions for an "explanatory" multimodal factor analysis. UCLA Working Papers in Phonetics, 16, 1-84.
</p>
<p>Harshman, R. (1972). PARAFAC2: Mathematical and technical notes. UCLA Working Papers in Phonetics, 22, 30-44.
</p>
<p>Harshman, R. and Lundy, M. (1994). PARAFAC: Parallel factor analysis. Computational Statistics and Data Analysis, 18, 39-72.
</p>
<p>Helwig, N. (2017). Estimating latent trends in multivariate longitudinal data via Parafac2 with functional and structural constraints. Biometrical Journal, 59(4), 783-803.
</p>
<p>Helwig, N. (2019). multiway: Component models for multi-way data. R Package Version 1.0-6.
</p>
<p>Liaw, A. and Wiener, M. (2002). Classification and regression by randomForest. R News 2(3), 18â€“22.
</p>
<p>Meyer, D., Dimitriadou, E., Hornik, K., Weingessel, A., and Leisch, F. (2023). e1071: Misc functions of the Department of Statistics, Probability Theory Group (Formerly: E1071), TU Wien. R Package Version 1.7-13.
</p>
<p>Ripley, B. (1994). Neural networks and related methods for classification. Journal of the Royal Statistical Society: Series B (Methodological), 56(3), 409-437.
</p>
<p>Venables, W. and Ripley, B. (2002). Modern applied statistics with S. Fourth Edition. Springer, New York. ISBN 0-387-95457-0.
</p>
<p>Zou, H. and Hastie, T. (2005). Regularization and variable selection via the elastic net. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 67(2), 301-320.
</p>


<h3>Examples</h3>

<pre><code class="language-R">########## Parafac example with 3-way array and binary response ##########

# set seed and specify dimensions of a three-way tensor
set.seed(3)
mydim &lt;- c(10, 11, 80)
nf &lt;- 3

# create correlation matrix between response and third mode's weights 
rho.cc &lt;- .35 
rho.cy &lt;- .75 
cormat.values &lt;- c(1, rho.cc, rho.cc, rho.cy, rho.cc, 1, rho.cc, rho.cy, 
                   rho.cc, rho.cc, 1, rho.cy, rho.cy, rho.cy, rho.cy, 1)
cormat &lt;- matrix(cormat.values, nrow = (nf + 1), ncol = (nf + 1))
 
# sample from a multivariate normal with specified correlation structure
ymean &lt;- Cmean &lt;- 2
mu &lt;- as.matrix(c(Cmean, Cmean, Cmean, ymean)) 
eidecomp &lt;- eigen(cormat, symmetric = TRUE)
L.sqrt &lt;- diag(eidecomp$values^0.5)
cormat.sqrt &lt;- eidecomp$vectors %*% L.sqrt %*% t(eidecomp$vectors)
Z &lt;- matrix(rnorm(mydim[3] * (nf + 1)), nrow = mydim[3], ncol = (nf + 1))
Xw &lt;- rep(1, mydim[3]) %*% t(mu) + Z %*% cormat.sqrt
Cmat &lt;- Xw[, 1:nf]

# create a random three-way data tensor with C weights related to a response
Amat &lt;- matrix(rnorm(mydim[1] * nf), nrow = mydim[1], ncol = nf)
Bmat &lt;- matrix(runif(mydim[2] * nf), nrow = mydim[2], ncol = nf)
Xmat &lt;- tcrossprod(Amat, krprod(Cmat, Bmat))
Xmat &lt;- array(Xmat, dim = mydim)
Emat &lt;- array(rnorm(prod(mydim)), dim = mydim)
Emat &lt;- nscale(Emat, 0, ssnew = sumsq(Xmat))  
X &lt;- Xmat + Emat

# create a binary response by dichotomizing at the specified response mean
y &lt;- factor(as.numeric(Xw[ , (nf + 1)] &gt; ymean))

# initialize
alpha &lt;- seq(0, 1, length = 2)
gamma &lt;- c(0, 0.01)
cost &lt;- c(1, 2)
ntree &lt;- c(100, 200)
nodesize &lt;- c(1, 2)
size &lt;- c(1, 2)
decay &lt;- c(0, 1)
rda.alpha &lt;- c(0.1, 0.6)
delta &lt;- c(0.1, 2)
eta &lt;- c(0.3, 0.7)
max.depth &lt;- c(1, 2)
subsample &lt;- c(0.75)
nrounds &lt;- c(100)
method &lt;- c("PLR", "SVM", "RF", "NN", "RDA", "GBM")
family &lt;- "binomial"
parameters &lt;- list(alpha = alpha, gamma = gamma, cost = cost, ntree = ntree,
                   nodesize = nodesize, size = size, decay = decay, 
                   rda.alpha = rda.alpha, delta = delta, eta = eta,
                   max.depth = max.depth, subsample = subsample,
                   nrounds = nrounds)
model &lt;- "parafac"
nfolds &lt;- 3
nstart &lt;- 3

# constrain first mode weights to be orthogonal
const &lt;- c("orthog", "uncons", "uncons")

# fit Parafac models and use third mode to tune classification methods
tune.object &lt;- tunecpfa(x = X, y = y, model = model, nfac = nf, 
                        nfolds = nfolds, method = method, family = family, 
                        parameters = parameters, parallel = FALSE, 
                        const = const, nstart = nstart)

# print tuning object
tune.object
</code></pre>


</div>