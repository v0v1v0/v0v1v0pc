<div class="container">

<table style="width: 100%;"><tr>
<td>sm.compositionality</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Spike's segmentation and measure of additive compositionality.</h2>

<h3>Description</h3>

<p>Implementation of the Spike-Montague segmentation and measure of additive
compositionality (Spike 2016), which finds the most predictive associations
between meaning features and substrings. Computation is deterministic and
fast.
</p>


<h3>Usage</h3>

<pre><code class="language-R">sm.compositionality(x, y, groups = NULL, strict = FALSE)

sm.segmentation(x, y, strict = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>a list or vector of character sequences specifying the signals to
be analysed. Alternatively, <code>x</code> can also be a formula of the format
<code>s ~ m1 + m2 + ...</code>, where <code>s</code> and <code>m1</code>, <code>m2</code>, etc.
specify the column names of the signals and meaning features found in the
data frame that is passed as the second argument.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>y</code></td>
<td>
<p>a matrix or data frame with as many rows as there are signals,
indicating the presence/value of the different meaning dimensions along
columns (see section Meaning data format). If <code>x</code> is a formula, the
<code>y</code> data frame can contain any number of columns, but only the ones
whose column name is specified in the formula will be considered.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>groups</code></td>
<td>
<p>a list or vector with as many items as strings, used to split
<code>strings</code> and <code>meanings</code> into data sets for which
compositionality measures are computed separately.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>strict</code></td>
<td>
<p>logical: if <code>TRUE</code>, perform additional filtering of
candidate segments. In particular, it removes combinations of segments
(across meanings) which overlap in at least one of the strings where they
co-occur. For convenience, it also removes segments which are shorter
substrings of longer candidates (for the same meaning feature).</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>The algorithm works on compositional meanings that can be expressed as sets
of categorical meaning features (see below), and does not take the order
of elements into account. Rather than looking directly at how complex
meanings are expressed, the measure really captures the degree to which a
homonymy- and synonymy-free signalling system exists at the level of
<em>individual semantic features</em>.
</p>
<p>The segmentation algorithm provided by <code>sm.segmentation()</code> scans through
all sub-strings found in <code>strings</code> to find the pairings of meaning features
and sub-strings whose respective presence is <em>most predictive of each
other</em>. Mathematically, for every meaning feature <code class="reqn">f\in M</code>, it finds
the sub-string <code class="reqn">s_{ij}</code> from the set of strings <code class="reqn">S</code> that yields the
highest <em>mutual predictability</em> across all signals,
</p>
<p style="text-align: center;"><code class="reqn">mp(f,S) = \max_{s_{ij}\in S}\ P(f|s_{ij}) \cdot P(s_{ij}|f)\;.</code>
</p>

<p>Based on the mutual predictability levels obtained for the individual
meaning features, <code>sm.compositionality</code> then computes the mean mutual
predictability weighted by the individual features' relative frequencies of
attestation, i.e.
</p>
<p style="text-align: center;"><code class="reqn">mp(M,S) = \sum_{f\in M} freq_f \cdot mp(f,S)\;,</code>
</p>

<p>as a measure of the overall compositionality of the signalling system.
</p>
<p>Since mutual predictability is determined seperately for every meaning
feature, the most predictive sub-strings posited for different meaning
features as returned by <code>sm.segmentation()</code> can overlap, and even coincide
completely. Such results are generally indicative of either limited data
(in particular frequent co-occurrence of the meaning features in question),
or spurious results in the absence of a consistent signalling system. The
latter will also be indicated by the significance level of the given mutual
predictability.
</p>


<h3>Value</h3>

<p><code>sm.segmentation</code> provides detailed information about the most
predictably co-occurring segments for every meaning feature. It returns
a data frame with one row for every meaning feature, in descending order
of the mutual predictability from (and to) their corresponding string
segments. The data frame has the following columns:
</p>

<dl>
<dt><code>N</code></dt>
<dd>
<p>The number of signal-meaning pairings in which this
meaning feature was attested.</p>
</dd>
<dt><code>mp</code></dt>
<dd>
<p>The highest mutual predictability between this
meaning feature and one (or more) segments that was found.</p>
</dd>
<dt><code>p</code></dt>
<dd>
<p>Significance levels of the given mutual predictability,
i.e. the probability that the given mutual predictability level could
be reached by chance. The calculation depends on the frequency of the
meaning feature as well as the number and relative frequency of all
substrings across all signals (see below).</p>
</dd>
<dt><code>ties</code></dt>
<dd>
<p>The number of substrings found in <code>strings</code>
which have this same level of mutual predictability with the meaning
feature.</p>
</dd>
<dt><code>segments</code></dt>
<dd>
<p>For <code>strict=FALSE</code>: a list containing the
<code>ties</code> substrings in descending order of their length (the
ordering is for convenience only and not inherently meaningful). When
<code>strict=TRUE</code>, the lists of segments for each meaning feature
are all of the same length, with a meaningful relationship of the
order of segments across the different rows: every set of segments
which are found in the same position for each of the different
meaning features constitute a valid segmentation where the segments
occurrences in the actual signals do not overlap.</p>
</dd>
</dl>
<p><code>sm.compositionality</code> calculates the weighted average of the
mutual predictability of all meaning features and their most predictably
co-occurring strings, as computed by <code>sm.segmentation</code>. The function
returns a data frame of three columns:
<code>N</code> is the total number of signals (utterances) on which the computation
was based, <code>M</code> the number of distinct meaning features attested across
all signals, and <code>meanmp</code> the mean mutual predictability across all these
features, weighted by the features' relative frequency. When <code>groups</code> is
not <code>NULL</code>, the data frame contains one row for every group.
</p>


<h3>Null distribution and p-value calculation</h3>

<p>A perfectly unambiguous mapping between a meaning feature to a specific
string segment will always yield a mutual predictability of <code>1</code>. In the
absence of such a regular mapping, on the other hand, chance co-occurrences
of strings and meanings will in most cases stop the mutual predictability
from going all the way down to <code>0</code>. In order to help distinguish chance
co-occurrence levels from significant signal-meaning associations,
<code>sm.segmentation()</code> provides significance levels for the mutual
predictability levels obtained for each meaning feature.
</p>
<p>What is the baseline level of association between a meaning feature and a
set of sub-strings that we would expect to be due to chance co-occurrences?
This depends on several factors, from the number of data points on which the
analysis is based to the frequency of the meaning feature in question and,
perhaps most importantly, the overall makeup of the different substrings
that are present in the signals. Since every substring attested in the data
is a candidate for signalling the presence of a meaning feature, the
absolute number of different substrings greatly affects the likelihood of
chance signal-meaning associations. (Diversity of the set of substrings is
in turn heavily influenced by the size of the underlying alphabet, a factor
which is often not appreciated.)
</p>
<p>For every candidate substring, the degree of association with a specific
meaning feature that we would expect by chance is again dependent on the
absolute number of signals in which the substring is attested.
</p>
<p>Starting from the simplest case, take a meaning that is featured in <code class="reqn">m</code>
of the total <code class="reqn">n</code> signals (where <code class="reqn">0 &lt; m \leq n</code>). Assume next that
there is a string segment that is attested in <code class="reqn">s</code> of these signals
(where again <code class="reqn">0 &lt; s \leq n</code>). The degree of association between the
meaning feature and string segment is dependent on the number of times that
they co-occur, which can be no more than <code class="reqn">c_{max} = min(m, s)</code> times.
The null probability of getting a given number of co-occurrences can be
obtained by considering all possible reshufflings of the meaning feature in
question across all signals: if <code class="reqn">s</code> signals contain a given substring,
how many of <code class="reqn">s</code> randomly drawn signals from the pool of <code class="reqn">n</code> signals
would contain the meaning feature if a total of <code class="reqn">m</code> signals in the pool
did? Approached from this angle, the likelihood of the number of
co-occurrences follows the
<a href="https://en.wikipedia.org/wiki/Hypergeometric_distribution">hypergeometric distribution</a>,
with <code class="reqn">c</code> being the number of successes when taking <code class="reqn">s</code> draws without
replacement from a population of size <code class="reqn">n</code> with fixed number of successes
<code class="reqn">m</code>.
</p>
<p>For every number of co-occurrences <code class="reqn">c \in [0, c_{max}]</code>, one can
compute the corresponding mutual probability level as
<code class="reqn">p(c|s) \cdot p(c|m)</code> to obtain the null distribution of mutual
predictability levels between a meaning feature and <em>one</em> substring of a
particular frequency <code class="reqn">s</code>:
</p>
<p style="text-align: center;"><code class="reqn">Pr(mp = p(c|s) \cdot p(c|m)) = f(k=c; N=n, K=m, n=s)</code>
</p>

<p>From this, we can now derive the null distribution for the entire set of
attested substrings as follows: making the simplifying assumption that the
occurrences of different substrings are independent of each other, we first
aggregate over the null distributions of all the individual substrings to
obtain the mean probability <code class="reqn">p=Pr(X\ge mp)</code> of finding a given mutual
predictability level at least as high as <code class="reqn">mp</code> for one randomly drawn
string from the entire population of substrings. Assuming the total number
of candidate substrings is <code class="reqn">|S|</code>, the overall null probability that at
least one of them would yield a mutual predictability at least as high is
</p>
<p style="text-align: center;"><code class="reqn">Pr(X\ge 0), X \equiv B(n=|S|, p=p)\;.</code>
</p>

<p>Note that, since the null distribution also depends on the frequency with
which the meaning feature is attested, the significance levels corresponding
to a given mutual predictability level are not necessarily identical for
all meaning features, even within one analysis.
</p>
<p>(In theory, one can also compute an overall p-value of the weighted mean
mutual predictability as calculated by <code>sm.compositionality</code>. However, the
significance levels for the individual meaning features are much more
insightful and should therefore be consulted directly.)
</p>


<h3>Meaning data format</h3>

<p>The <code>meanings</code> argument can be a matrix or data frame in one of two formats.
If it is a matrix of logicals (<code>TRUE</code>/<code>FALSE</code> values), then the columns are
assumed to refer to meaning <em>features</em>, with individual cells indicating
whether the meaning feature is present or absent in the signal represented
by that row (see <code>binaryfeaturematrix()</code> for an explanation). If <code>meanings</code>
is a data frame or matrix of any other type, it is assumed that the columns
specify different meaning dimensions, with the cell values showing the
levels with which the different dimensions can be realised. This
dimension-based representation is automatically converted to a
feature-based one by calling <code>binaryfeaturematrix()</code>. As a consequence,
whatever the actual types of the columns in the meaning matrix, <em>they will
be treated as categorical factors</em> for the purpose of this algorithm, also
discarding any explicit knowledge of which 'meaning dimension' they might
belong to.
</p>


<h3>References</h3>

<p>Spike, M. 2016 <em>Minimal requirements for the cultural
evolution of language</em>. PhD thesis, The University of Edinburgh.
<a href="http://hdl.handle.net/1842/25930">http://hdl.handle.net/1842/25930</a>.
</p>


<h3>See Also</h3>

<p><code>binaryfeaturematrix()</code>, <code>ssm.compositionality()</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R"># perfect communication system for two meaning features (which are marked
# as either present or absent)
sm.compositionality(c("a", "b", "ab"),
  cbind(a=c(TRUE, FALSE, TRUE), b=c(FALSE, TRUE, TRUE)))
sm.segmentation(c("a", "b", "ab"),
  cbind(a=c(TRUE, FALSE, TRUE), b=c(FALSE, TRUE, TRUE)))

# not quite perfect communication system
sm.compositionality(c("as", "bas", "basf"),
  cbind(a=c(TRUE, FALSE, TRUE), b=c(FALSE, TRUE, TRUE)))
sm.segmentation(c("as", "bas", "basf"),
  cbind(a=c(TRUE, FALSE, TRUE), b=c(FALSE, TRUE, TRUE)))

# same communication system, but force candidate segments to be non-overlapping
# via the 'strict' option
sm.segmentation(c("as", "bas", "basf"),
  cbind(a=c(TRUE, FALSE, TRUE), b=c(FALSE, TRUE, TRUE)), strict=TRUE)


# the function also accepts meaning-dimension based matrix definitions:
print(twobytwoanimals &lt;- enumerate.meaningcombinations(c(animal=2, colour=2)))

# note how there are many more candidate segments than just the full length
# ones. the less data we have, the more likely it is that shorter substrings
# will be just as predictable as the full segments that contain them.
sm.segmentation(c("greendog", "bluedog", "greencat", "bluecat"), twobytwoanimals)

# perform the same analysis, but using the formula interface
print(twobytwosignalingsystem &lt;- cbind(twobytwoanimals,
  signal=c("greendog", "bluedog", "greencat", "bluecat")))

sm.segmentation(signal ~ colour + animal, twobytwosignalingsystem)

# since there is no overlap in the constituent characters of the identified
# 'morphemes', they are all tied in their mutual predictiveness with the
# (shorter) substrings they contain
#
# to reduce the pool of candidate segments to those which are
# non-overlapping and of maximal length, again use the 'strict=TRUE' option:

sm.segmentation(signal ~ colour + animal, twobytwosignalingsystem, strict=TRUE)

</code></pre>


</div>