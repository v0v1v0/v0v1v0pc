<div class="container">

<table style="width: 100%;"><tr>
<td>Tuning of the k-NN algorithm for compositional data</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>
Tuning of the k-NN algorithm for compositional data
</h2>

<h3>Description</h3>

<p>Tuning of the k-NN algorithm for compositional data with and without using the
power or the <code class="reqn">\alpha</code>-transformation. In addition, estimation of the rate
of correct classification via K-fold cross-validation.
</p>


<h3>Usage</h3>

<pre><code class="language-R">compknn.tune(x, ina, nfolds = 10, k = 2:5, mesos = TRUE,
a = seq(-1, 1, by = 0.1), apostasi = "ESOV", folds = NULL,
stratified = TRUE, seed = NULL, graph = FALSE)

alfaknn.tune(x, ina, nfolds = 10, k = 2:5, mesos = TRUE,
a = seq(-1, 1, by = 0.1), apostasi = "euclidean", rann = FALSE,
folds = NULL, stratified = TRUE, seed = NULL, graph = FALSE)

aitknn.tune(x, ina, nfolds = 10, k = 2:5, mesos = TRUE,
a = seq(-1, 1, by = 0.1), apostasi = "euclidean", rann = FALSE,
folds = NULL, stratified = TRUE, seed = NULL, graph = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>

<p>A matrix with the available compositional data. Zeros are allowed, but you
must be careful to choose strictly positive values of <code class="reqn">\alpha</code> or not
to set apostasi= "Ait".
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>ina</code></td>
<td>

<p>A group indicator variable for the available data.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nfolds</code></td>
<td>

<p>The number of folds to be used. This is taken into consideration only if
the folds argument is not supplied.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>k</code></td>
<td>

<p>A vector with the nearest neighbours to consider.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>mesos</code></td>
<td>

<p>This is used in the non standard algorithm. If TRUE, the arithmetic mean of
the distances is calculated, otherwise the harmonic mean is used (see details).
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>a</code></td>
<td>

<p>A grid of values of <code class="reqn">\alpha</code> to be used only if the distance chosen allows
for it.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>apostasi</code></td>
<td>

<p>The type of distance to use. For the compk.knn this can be one of the following:
"ESOV", "taxicab", "Ait", "Hellinger", "angular" or "CS". See the references
for them. For the alfa.knn this can be either "euclidean" or "manhattan".
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>rann</code></td>
<td>

<p>If you have large scale datasets and want a faster k-NN search, you can use
kd-trees implemented in the R package "Rnanoflann". In this case you must set this
argument equal to TRUE. Note however, that in this case, the only available
distance is by default "euclidean".
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>folds</code></td>
<td>

<p>If you have the list with the folds supply it here. You can also leave it NULL
and it will create folds.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>stratified</code></td>
<td>

<p>Do you want the folds to be created in a stratified way? TRUE or FALSE.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>seed</code></td>
<td>

<p>You can specify your own seed number here or leave it NULL.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>graph</code></td>
<td>

<p>If set to TRUE a graph with the results will appear.
</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>The k-NN algorithm is applied for the compositional data. There are many metrics
and possibilities to choose from. The algorithm finds the k nearest
observations to a new observation and allocates it to the class which appears
most times in the neighbours.
</p>


<h3>Value</h3>

<p>A list including:
</p>
<table>
<tr style="vertical-align: top;">
<td><code>per</code></td>
<td>

<p>A matrix or a vector (depending on the distance chosen) with the averaged over
all folds rates of correct classification for all hyper-parameters
(<code class="reqn">\alpha</code> and k).
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>performance</code></td>
<td>

<p>The estimated rate of correct classification.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>best_a</code></td>
<td>

<p>The best value of <code class="reqn">\alpha</code>. This is returned for "ESOV" and "taxicab" only.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>best_k</code></td>
<td>

<p>The best number of nearest neighbours.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>runtime</code></td>
<td>

<p>The run time of the cross-validation procedure.
</p>
</td>
</tr>
</table>
<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>
and Giorgos Athineou &lt;gioathineou@gmail.com&gt;.
</p>


<h3>References</h3>

<p>Tsagris, Michail (2014). The k-NN algorithm for compositional data:
a revised approach with and without zero values present.
Journal of Data Science, 12(3): 519–534.
https://arxiv.org/pdf/1506.05216.pdf
</p>
<p>Friedman Jerome, Trevor Hastie and Robert Tibshirani (2009).
The elements of statistical learning,
2nd edition. Springer, Berlin
</p>
<p>Tsagris M., Preston S. and Wood A.T.A. (2016). Improved classification for
compositional data using the <code class="reqn">\alpha</code>-transformation.
Journal of Classification, 33(2): 243–261.
http://arxiv.org/pdf/1106.1451.pdf
</p>
<p>Connie Stewart (2017). An approach to measure distance between
compositional diet estimates containing essential zeros.
Journal of Applied Statistics 44(7): 1137–1152.
</p>
<p>Clarotto L., Allard D. and Menafoglio A. (2022).
A new class of <code class="reqn">\alpha</code>-transformations for the spatial analysis
of Compositional Data. Spatial Statistics, 47.
</p>
<p>Endres, D. M. and Schindelin, J. E. (2003). A new metric for probability distributions.
Information Theory, IEEE Transactions on 49, 1858–1860.
</p>
<p>Osterreicher, F. and Vajda, I. (2003). A new class of metric divergences on
probability spaces and its applicability in statistics.
Annals of the Institute of Statistical Mathematics 55, 639–653.
</p>


<h3>See Also</h3>

<p><code>comp.knn, alfarda.tune, cv.dda, cv.compnb
</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">x &lt;- as.matrix(iris[, 1:4])
x &lt;- x/ rowSums(x)
ina &lt;- iris[, 5]
mod1 &lt;- compknn.tune(x, ina, a = seq(1, 1, by = 0.1) )
mod2 &lt;- alfaknn.tune(x, ina, a = seq(-1, 1, by = 0.1) )
</code></pre>


</div>