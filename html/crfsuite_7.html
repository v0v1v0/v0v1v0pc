<div class="container">

<table style="width: 100%;"><tr>
<td>crf_evaluation</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Basic classification evaluation metrics for multi-class labelling</h2>

<h3>Description</h3>

<p>The accuracy, precision, recall, specificity, F1 measure and support metrics are provided for each label in a one-versus the rest setting.
</p>


<h3>Usage</h3>

<pre><code class="language-R">crf_evaluation(
  pred,
  obs,
  labels = na.exclude(unique(c(as.character(pred), as.character(obs)))),
  labels_overall = setdiff(labels, "O")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>pred</code></td>
<td>
<p>a factor with predictions</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>obs</code></td>
<td>
<p>a factor with gold labels</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>labels</code></td>
<td>
<p>a character vector of possible values that <code>pred</code> and <code>obs</code> can take. Defaults to the values in the data</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>labels_overall</code></td>
<td>
<p>a character vector of either labels which is either the same as <code>labels</code> or a subset of <code>labels</code> in order to compute a weighted average of the by-label statistics</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>a list with 2 elements:
</p>

<ul>
<li>
<p>bylabel: data.frame with the accuracy, precision, recall, specificity, F1 score and support (number of occurrences) for each label
</p>
</li>
<li>
<p>overall: a vector containing 
</p>

<ul>
<li>
<p>the overall accuracy
</p>
</li>
<li>
<p>the metrics precision, recall, specificity and F1 score which are weighted averages of these metrics from list element <code>bylabel</code>, where the weight is the support
</p>
</li>
<li>
<p>the metrics precision, recall, specificity and F1 score which are averages of these metrics from list element <code>bylabel</code> giving equal weight to each label
</p>
</li>
</ul>
</li>
</ul>
<h3>Examples</h3>

<pre><code class="language-R">pred &lt;- sample(LETTERS, 1000, replace = TRUE)
gold &lt;- sample(LETTERS, 1000, replace = TRUE)
crf_evaluation(pred = pred, obs = gold, labels = LETTERS) 


x &lt;- ner_download_modeldata("conll2002-nl")
x &lt;- crf_cbind_attributes(x, terms = c("token", "pos"), 
                          by = c("doc_id", "sentence_id"))
crf_train &lt;- subset(x, data == "ned.train")
crf_test &lt;- subset(x, data == "testa")
attributes &lt;- grep("token|pos", colnames(x), value=TRUE)
model &lt;- crf(y = crf_train$label, 
             x = crf_train[, attributes], 
             group = crf_train$doc_id, 
             method = "lbfgs") 
             
## Use the model to score on existing tokenised data
scores &lt;- predict(model, 
                  newdata = crf_test[, attributes], 
                  group = crf_test$doc_id)
crf_evaluation(pred = scores$label, obs = crf_test$label)
crf_evaluation(pred = scores$label, obs = crf_test$label, 
  labels = c("O", 
             "B-ORG", "I-ORG", "B-PER", "I-PER", 
             "B-LOC", "I-LOC", "B-MISC", "I-MISC"))
             
         
library(udpipe)
pred &lt;- txt_recode(scores$label, 
                   from = c("B-ORG", "I-ORG", "B-PER", "I-PER", 
                            "B-LOC", "I-LOC", "B-MISC", "I-MISC"),
                   to = c("ORG", "ORG", "PER", "PER", 
                          "LOC", "LOC", "MISC", "MISC"))
obs &lt;- txt_recode(crf_test$label, 
                  from = c("B-ORG", "I-ORG", "B-PER", "I-PER", 
                           "B-LOC", "I-LOC", "B-MISC", "I-MISC"),
                  to = c("ORG", "ORG", "PER", "PER", 
                         "LOC", "LOC", "MISC", "MISC"))
crf_evaluation(pred = pred, obs = obs, 
               labels = c("ORG", "LOC", "PER", "MISC", "O"))


</code></pre>


</div>