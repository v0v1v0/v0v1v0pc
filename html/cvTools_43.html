<div class="container">

<table style="width: 100%;"><tr>
<td>cvSelect</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Model selection based on cross-validation</h2>

<h3>Description</h3>

<p>Combine cross-validation results for various models into one object and
select the model with the best prediction performance.
</p>


<h3>Usage</h3>

<pre><code class="language-R">cvSelect(
  ...,
  .reshape = FALSE,
  .selectBest = c("min", "hastie"),
  .seFactor = 1
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>objects inheriting from class <code>"cv"</code> or <code>"cvSelect"</code>
that contain cross-validation results.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>.reshape</code></td>
<td>
<p>a logical indicating whether objects with more than one
column of cross-validation results should be reshaped to have only one
column (see “Details”).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>.selectBest</code></td>
<td>
<p>a character string specifying a criterion for selecting
the best model.  Possible values are <code>"min"</code> (the default) or
<code>"hastie"</code>.  The former selects the model with the smallest prediction
error.  The latter is useful for nested models or for models with a tuning
parameter controlling the complexity of the model (e.g., penalized
regression).  It selects the most parsimonious model whose prediction error
is no larger than <code>.seFactor</code> standard errors above the prediction error
of the best overall model.  Note that the models are thereby assumed to be
ordered from the most parsimonious one to the most complex one.  In
particular a one-standard-error rule is frequently applied.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>.seFactor</code></td>
<td>
<p>a numeric value giving a multiplication factor of the
standard error for the selection of the best model.  This is ignored if
<code>.selectBest</code> is <code>"min"</code>.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Keep in mind that objects inheriting from class <code>"cv"</code> or
<code>"cvSelect"</code> may contain multiple columns of cross-validation
results.  This is the case if the response is univariate but the
<code>predict</code> method of the fitted model returns a
matrix.
</p>
<p>The <code>.reshape</code> argument determines how to handle such objects.  If
<code>.reshape</code> is <code>FALSE</code>, all objects are required to have the same
number of columns and the best model for each column is selected.  A typical
use case for this behavior would be if the investigated models contain
cross-validation results for a raw and a reweighted fit.  It might then be
of interest to researchers to compare the best model for the raw estimators
with the best model for the reweighted estimators.
</p>
<p>If <code>.reshape</code> is <code>TRUE</code>, objects with more than one column of
results are first transformed with <code>cvReshape</code> to have only one
column.  Then the best overall model is selected.
</p>
<p>It should also be noted that the argument names of <code>.reshape</code>,
<code>.selectBest</code> and <code>.seFacor</code> start with a dot to avoid conflicts
with the argument names used for the objects containing cross-validation
results.
</p>


<h3>Value</h3>

<p>An object of class <code>"cvSelect"</code> with the following components:
</p>
<table>
<tr style="vertical-align: top;">
<td><code>n</code></td>
<td>
<p>an integer giving the number of observations.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>K</code></td>
<td>
<p>an integer vector giving the number of folds used in
cross-validation for the respective model.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>R</code></td>
<td>
<p>an integer vector giving the number of replications used in
cross-validation for the respective model.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>best</code></td>
<td>
<p>an integer vector giving the indices of the models with
the best prediction performance.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>cv</code></td>
<td>
<p>a data frame containing the estimated prediction errors for
the models.  For models for which repeated cross-validation was performed,
those are average values over all replications.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>se</code></td>
<td>
<p>a data frame containing the estimated standard errors of the
prediction loss for the models.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>selectBest</code></td>
<td>
<p>a character string specifying the criterion used for
selecting the best model.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>seFactor</code></td>
<td>
<p>a numeric value giving the multiplication factor of
the standard error used for the selection of the best model.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>reps</code></td>
<td>
<p>a data frame containing the estimated prediction errors
from all replications for those models for which repeated cross-validation
was performed.  This is only returned if repeated cross-validation was
performed for at least one of the models.</p>
</td>
</tr>
</table>
<h3>Note</h3>

<p>Even though the function allows to compare cross-validation results
obtained with a different number of folds or a different number of
replications, such comparisons should be made with care.  Hence warnings
are issued in those cases.  For maximum comparability, the same data folds
should be used in cross-validation for all models to be compared.
</p>


<h3>Author(s)</h3>

<p>Andreas Alfons
</p>


<h3>References</h3>

<p>Hastie, T., Tibshirani, R. and Friedman, J. (2009) <em>The Elements of
Statistical Learning: Data Mining, Inference, and Prediction</em>.  Springer,
2nd edition.
</p>


<h3>See Also</h3>

<p><code>cvFit</code>, <code>cvTuning</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">library("robustbase")
data("coleman")
set.seed(1234)  # set seed for reproducibility

# set up folds for cross-validation
folds &lt;- cvFolds(nrow(coleman), K = 5, R = 10)


## compare LS, MM and LTS regression

# perform cross-validation for an LS regression model
fitLm &lt;- lm(Y ~ ., data = coleman)
cvFitLm &lt;- cvLm(fitLm, cost = rtmspe, 
    folds = folds, trim = 0.1)

# perform cross-validation for an MM regression model
fitLmrob &lt;- lmrob(Y ~ ., data = coleman)
cvFitLmrob &lt;- cvLmrob(fitLmrob, cost = rtmspe, 
    folds = folds, trim = 0.1)

# perform cross-validation for an LTS regression model
fitLts &lt;- ltsReg(Y ~ ., data = coleman)
cvFitLts &lt;- cvLts(fitLts, cost = rtmspe, 
    folds = folds, trim = 0.1)

# compare cross-validation results
cvSelect(LS = cvFitLm, MM = cvFitLmrob, LTS = cvFitLts)
</code></pre>


</div>