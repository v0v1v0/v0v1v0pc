<div class="container">

<table style="width: 100%;"><tr>
<td>reduce.nn</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>
Reduce Training Set for a k-NN Classifier
</h2>

<h3>Description</h3>

<p>Reduce training set for a k-NN classifier. Used after <code>condense</code>.
</p>


<h3>Usage</h3>

<pre><code class="language-R">reduce.nn(train, ind, class)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>train</code></td>
<td>

<p>matrix for training set
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>ind</code></td>
<td>

<p>Initial list of members of the training set (from <code>condense</code>).
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>class</code></td>
<td>

<p>vector of classifications for test set
</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>All the members of the training set are tried in random order.
Any which when dropped do not cause any members of the training set to
be wrongly classified are dropped.
</p>


<h3>Value</h3>

<p>Index vector of cases to be retained.
</p>


<h3>References</h3>

<p>Gates, G.W. (1972) The reduced nearest neighbor rule.
<em>IEEE Trans. Information Theory</em>
<b>IT-18</b>, 431â€“432.
</p>
<p>Ripley, B. D. (1996)
<em>Pattern Recognition and Neural Networks.</em> Cambridge.
</p>
<p>Venables, W. N. and Ripley, B. D. (2002)
<em>Modern Applied Statistics with S.</em> Fourth edition.  Springer.
</p>


<h3>See Also</h3>

<p><code>condense</code>, <code>multiedit</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">train &lt;- rbind(iris3[1:25,,1], iris3[1:25,,2], iris3[1:25,,3])
test &lt;- rbind(iris3[26:50,,1], iris3[26:50,,2], iris3[26:50,,3])
cl &lt;- factor(c(rep("s",25), rep("c",25), rep("v",25)))
keep &lt;- condense(train, cl)
knn(train[keep,], test, cl[keep])
keep2 &lt;- reduce.nn(train, keep, cl)
knn(train[keep2,], test, cl[keep2])
</code></pre>


</div>