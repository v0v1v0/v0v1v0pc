<div class="container">

<table style="width: 100%;"><tr>
<td>conquer.reg</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Penalized Convolution-Type Smoothed Quantile Regression</h2>

<h3>Description</h3>

<p>Fit sparse quantile regression models in high dimensions via regularized conquer methods with "lasso", "elastic-net", "group lasso", "sparse group lasso", "scad" and "mcp" penalties. 
For "scad" and "mcp", the iteratively reweighted <code class="reqn">\ell_1</code>-penalized algorithm is complemented with a local adpative majorize-minimize algorithm.
</p>


<h3>Usage</h3>

<pre><code class="language-R">conquer.reg(
  X,
  Y,
  lambda = 0.2,
  tau = 0.5,
  kernel = c("Gaussian", "logistic", "uniform", "parabolic", "triangular"),
  h = 0,
  penalty = c("lasso", "elastic", "group", "sparse-group", "scad", "mcp"),
  para.elastic = 0.5,
  group = NULL,
  weights = NULL,
  para.scad = 3.7,
  para.mcp = 3,
  epsilon = 0.001,
  iteMax = 500,
  phi0 = 0.01,
  gamma = 1.2,
  iteTight = 3
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>X</code></td>
<td>
<p>An <code class="reqn">n</code> by <code class="reqn">p</code> design matrix. Each row is a vector of observations with <code class="reqn">p</code> covariates.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Y</code></td>
<td>
<p>An <code class="reqn">n</code>-dimensional response vector.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lambda</code></td>
<td>
<p>(<strong>optional</strong>) Regularization parameter. Can be a scalar or a sequence. If the input is a sequence, the function will sort it in ascending order, and run the regression accordingly. Default is 0.2.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>tau</code></td>
<td>
<p>(<strong>optional</strong>) Quantile level (between 0 and 1). Default is 0.5.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>kernel</code></td>
<td>
<p>(<strong>optional</strong>) A character string specifying the choice of kernel function. Default is "Gaussian". Choices are "Gaussian", "logistic", "uniform", "parabolic" and "triangular".</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>h</code></td>
<td>
<p>(<strong>optional</strong>) Bandwidth/smoothing parameter. Default is <code class="reqn">\max\{0.5 * (log(p) / n)^{0.25}, 0.05\}</code>. The default will be used if the input value is less than or equal to 0.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>penalty</code></td>
<td>
<p>(<strong>optional</strong>) A character string specifying the penalty. Default is "lasso" (Tibshirani, 1996). The other options are "elastic" for elastic-net (Zou and Hastie, 2005), "group" for group lasso (Yuan and Lin, 2006), "sparse-group" for sparse group lasso (Simon et al., 2013), "scad" (Fan and Li, 2001) and "mcp" (Zhang, 2010).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>para.elastic</code></td>
<td>
<p>(<strong>optional</strong>) The mixing parameter between 0 and 1 (usually noted as <code class="reqn">\alpha</code>) for elastic-net. The penalty is defined as <code class="reqn">\alpha ||\beta||_1 + (1 - \alpha) ||\beta||_2^2</code>. Default is 0.5.
Setting <code>para.elastic = 1</code> gives the lasso penalty, and setting <code>para.elastic = 0</code> yields the ridge penalty. Only specify it when <code>penalty = "elastic"</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>group</code></td>
<td>
<p>(<strong>optional</strong>) A <code class="reqn">p</code>-dimensional vector specifying group indices. Only specify it if <code>penalty = "group"</code> or <code>penalty = "sparse-group"</code>. 
For example, if <code class="reqn">p = 10</code>, and we assume the first 3 coefficients belong to the first group, and the last 7 coefficients belong to the second group, then the argument should be <code>group = c(rep(1, 3), rep(2, 7))</code>. If not specified, then the penalty will be the classical lasso.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>weights</code></td>
<td>
<p>(<strong>optional</strong>) A vector specifying groups weights for group Lasso and sparse group Lasso. The length must be equal to the number of groups. If not specified, the default weights are square roots of group sizes. 
For example , if <code>group = c(rep(1, 3), rep(2, 7))</code>, then the default weights are <code class="reqn">\sqrt{3}</code> for the first group, and <code class="reqn">\sqrt{7}</code> for the second group.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>para.scad</code></td>
<td>
<p>(<strong>optional</strong>) The constant parameter for "scad". Default value is 3.7. Only specify it if <code>penalty = "scad"</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>para.mcp</code></td>
<td>
<p>(<strong>optional</strong>) The constant parameter for "mcp". Default value is 3. Only specify it if <code>penalty = "mcp"</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>epsilon</code></td>
<td>
<p>(<strong>optional</strong>) A tolerance level for the stopping rule. The iteration will stop when the maximum magnitude of the change of coefficient updates is less than <code>epsilon</code>. Default is 0.001.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>iteMax</code></td>
<td>
<p>(<strong>optional</strong>) Maximum number of iterations. Default is 500.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>phi0</code></td>
<td>
<p>(<strong>optional</strong>) The initial quadratic coefficient parameter in the local adaptive majorize-minimize algorithm. Default is 0.01.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>gamma</code></td>
<td>
<p>(<strong>optional</strong>) The adaptive search parameter (greater than 1) in the local adaptive majorize-minimize algorithm. Default is 1.2.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>iteTight</code></td>
<td>
<p>(<strong>optional</strong>) Maximum number of tightening iterations in the iteratively reweighted <code class="reqn">\ell_1</code>-penalized algorithm. Only specify it if the penalty is scad or mcp. Default is 3.</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>An object containing the following items will be returned:
</p>

<dl>
<dt><code>coeff</code></dt>
<dd>
<p>If the input <code>lambda</code> is a scalar, then <code>coeff</code> returns a <code class="reqn">(p + 1)</code> vector of estimated coefficients, including the intercept. If the input <code>lambda</code> is a sequence, then <code>coeff</code> returns a <code class="reqn">(p + 1)</code> by <code class="reqn">nlambda</code> matrix, where <code class="reqn">nlambda</code> refers to the length of <code>lambda</code> sequence.</p>
</dd>
<dt><code>bandwidth</code></dt>
<dd>
<p>Bandwidth value.</p>
</dd>
<dt><code>tau</code></dt>
<dd>
<p>Quantile level.</p>
</dd>
<dt><code>kernel</code></dt>
<dd>
<p>Kernel function.</p>
</dd>
<dt><code>penalty</code></dt>
<dd>
<p>Penalty type.</p>
</dd>
<dt><code>lambda</code></dt>
<dd>
<p>Regularization parameter(s).</p>
</dd>
<dt><code>n</code></dt>
<dd>
<p>Sample size.</p>
</dd>
<dt><code>p</code></dt>
<dd>
<p>Number of the covariates.</p>
</dd>
</dl>
<h3>References</h3>

<p>Belloni, A. and Chernozhukov, V. (2011). <code class="reqn">\ell_1</code> penalized quantile regression in high-dimensional sparse models. Ann. Statist., 39, 82-130.
</p>
<p>Fan, J. and Li, R. (2001). Variable selection via nonconcave regularized likelihood and its oracle properties. J. Amer. Statist. Assoc., 96, 1348-1360.
</p>
<p>Fan, J., Liu, H., Sun, Q. and Zhang, T. (2018). I-LAMM for sparse learning: Simultaneous control of algorithmic complexity and statistical error. Ann. Statist., 46, 814-841.
</p>
<p>Koenker, R. and Bassett, G. (1978). Regression quantiles. Econometrica, 46, 33-50.
</p>
<p>Simon, N., Friedman, J., Hastie, T. and Tibshirani, R. (2013). A sparse-group lasso. J. Comp. Graph. Statist., 22, 231-245.
</p>
<p>Tibshirani, R. (1996). Regression shrinkage and selection via the lasso. J. R. Statist. Soc. Ser. B, 58, 267â€“288.
</p>
<p>Tan, K. M., Wang, L. and Zhou, W.-X. (2022). High-dimensional quantile regression: convolution smoothing and concave regularization. J. Roy. Statist. Soc. Ser. B, 84, 205-233.
</p>
<p>Yuan, M. and Lin, Y. (2006). Model selection and estimation in regression with grouped variables., J. Roy. Statist. Soc. Ser. B, 68, 49-67.
</p>
<p>Zhang, C.-H. (2010). Nearly unbiased variable selection under minimax concave penalty. Ann. Statist., 38, 894-942.
</p>
<p>Zou, H. and Hastie, T. (2005). Regularization and variable selection via the elastic net. J. R. Statist. Soc. Ser. B, 67, 301-320.
</p>


<h3>See Also</h3>

<p>See <code>conquer.cv.reg</code> for regularized quantile regression with cross-validation.
</p>


<h3>Examples</h3>

<pre><code class="language-R">n = 200; p = 500; s = 10
beta = c(rep(1.5, s), rep(0, p - s))
X = matrix(rnorm(n * p), n, p)
Y = X %*% beta + rt(n, 2)

## Regularized conquer with lasso penalty at tau = 0.7
fit.lasso = conquer.reg(X, Y, lambda = 0.05, tau = 0.7, penalty = "lasso")
beta.lasso = fit.lasso$coeff

## Regularized conquer with elastic-net penalty at tau = 0.7
fit.elastic = conquer.reg(X, Y, lambda = 0.1, tau = 0.7, penalty = "elastic", para.elastic = 0.7)
beta.elastic = fit.elastic$coeff

## Regularized conquer with scad penalty at tau = 0.7
fit.scad = conquer.reg(X, Y, lambda = 0.13, tau = 0.7, penalty = "scad")
beta.scad = fit.scad$coeff

## Regularized conquer with group lasso at tau = 0.7
beta = c(rep(1.3, 5), rep(1.5, 5), rep(0, p - s))
err = rt(n, 2)
Y = X %*% beta + err
group = c(rep(1, 5), rep(2, 5), rep(3, p - s))
fit.group = conquer.reg(X, Y, lambda = 0.05, tau = 0.7, penalty = "group", group = group)
beta.group = fit.group$coeff
</code></pre>


</div>