<div class="container">

<table style="width: 100%;"><tr>
<td>baseline</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Create baseline evaluations</h2>

<h3>Description</h3>

<p><a href="https://lifecycle.r-lib.org/articles/stages.html#maturing"><img src="../help/figures/lifecycle-maturing.svg" alt="[Maturing]"></a>
</p>
<p>Create a baseline evaluation of a test set.
</p>
<p>In modelling, a <em>baseline</em> is a result that
is meaningful to compare the results from our models to. For instance, in
classification, we usually want our results to be better than <em>random guessing</em>.
E.g. if we have three classes, we can expect an accuracy of <code>33.33%</code>, as for every
observation we have <code>1/3</code> chance of guessing the correct class. So our model should achieve
a higher accuracy than <code>33.33%</code> before it is more useful to us than guessing.
</p>
<p>While this expected value is often fairly straightforward to find analytically, it
only represents what we can expect on average. In reality, it's possible to get far better
results than that by guessing.
<strong><code>baseline()</code></strong> (<code>binomial</code>, <code>multinomial</code>)
finds the range of likely values by evaluating multiple sets
of random predictions and summarizing them with a set of useful descriptors.
If random guessing frequently obtains an accuracy of <code>40%</code>, perhaps our model
should have better performance than this, before we declare it better than guessing.
</p>


<h4><strong>How</strong></h4>

<p>When <code>`family`</code> is <code>binomial</code>: evaluates <code>`n`</code> sets of random predictions
against the dependent variable, along with a set of all <code>0</code> predictions and
a set of all <code>1</code> predictions. See also <code>baseline_binomial()</code>.
</p>
<p>When <code>`family`</code> is <code>multinomial</code>: creates <em>one-vs-all</em> (binomial)
baseline evaluations for <code>`n`</code> sets of random predictions against the dependent variable,
along with sets of "all class x,y,z,..." predictions.
See also <code>baseline_multinomial()</code>.
</p>
<p>When <code>`family`</code> is <code>gaussian</code>: fits baseline models (<code>y ~ 1</code>) on <code>`n`</code> random
subsets of <code>`train_data`</code> and evaluates each model on <code>`test_data`</code>. Also evaluates a
model fitted on all rows in <code>`train_data`</code>.
See also <code>baseline_gaussian()</code>.
</p>



<h4><strong>Wrapper functions</strong></h4>

<p>Consider using one of the wrappers, as they are simpler to use and understand:
<strong><code>baseline_gaussian()</code></strong>,
<strong><code>baseline_multinomial()</code></strong>, and
<strong><code>baseline_binomial()</code></strong>.
</p>



<h3>Usage</h3>

<pre><code class="language-R">baseline(
  test_data,
  dependent_col,
  family,
  train_data = NULL,
  n = 100,
  metrics = list(),
  positive = 2,
  cutoff = 0.5,
  random_generator_fn = runif,
  random_effects = NULL,
  min_training_rows = 5,
  min_training_rows_left_out = 3,
  REML = FALSE,
  parallel = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>test_data</code></td>
<td>
<p><code>data.frame</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>dependent_col</code></td>
<td>
<p>Name of dependent variable in the supplied test and training sets.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>family</code></td>
<td>
<p>Name of family. (Character)
</p>
<p>Currently supports <code>"gaussian"</code>, <code>"binomial"</code> and <code>"multinomial"</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>train_data</code></td>
<td>
<p><code>data.frame</code>. Only used when <code>`family`</code> is <code>"gaussian"</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>n</code></td>
<td>
<p>Number of random samplings to perform. (Default is <code>100</code>)
</p>
<p>For <code>gaussian</code>: The number of random samplings of <code>`train_data`</code> to fit baseline models on.
</p>
<p>For <code>binomial</code> and <code>multinomial</code>: The number of sets of random predictions to evaluate.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>metrics</code></td>
<td>
<p><code>list</code> for enabling/disabling metrics.
</p>
<p>E.g. <code>list("RMSE" = FALSE)</code> would remove <code>RMSE</code> from the regression results,
and <code>list("Accuracy" = TRUE)</code> would add the regular <code>Accuracy</code> metric
to the classification results.
Default values (<code>TRUE</code>/<code>FALSE</code>) will be used for the remaining available metrics.
</p>
<p>You can enable/disable all metrics at once by including
<code>"all" = TRUE/FALSE</code> in the <code>list</code>. This is done prior to enabling/disabling
individual metrics, why f.i. <code>list("all" = FALSE, "RMSE" = TRUE)</code>
would return only the <code>RMSE</code> metric.
</p>
<p>The <code>list</code> can be created with
<code>gaussian_metrics()</code>,
<code>binomial_metrics()</code>, or
<code>multinomial_metrics()</code>.
</p>
<p>Also accepts the string <code>"all"</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>positive</code></td>
<td>
<p>Level from dependent variable to predict.
Either as character (<em>preferable</em>) or level index (<code>1</code> or <code>2</code> - alphabetically).
</p>
<p>E.g. if we have the levels <code>"cat"</code> and <code>"dog"</code> and we want <code>"dog"</code> to be the positive class,
we can either provide <code>"dog"</code> or <code>2</code>, as alphabetically, <code>"dog"</code> comes after <code>"cat"</code>.
</p>
<p><strong>Note:</strong> For <em>reproducibility</em>, it's preferable to <strong>specify the name directly</strong>, as
different <code>locales</code> may sort the levels differently.
</p>
<p>Used when calculating confusion matrix metrics and creating <code>ROC</code> curves.
</p>
<p>N.B. Only affects evaluation metrics, not the returned predictions.
</p>
<p>N.B. <strong>Binomial only</strong>. (Character or Integer)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>cutoff</code></td>
<td>
<p>Threshold for predicted classes. (Numeric)
</p>
<p>N.B. <strong>Binomial only</strong></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>random_generator_fn</code></td>
<td>
<p>Function for generating random numbers when <code>type</code> is <code>"multinomial"</code>.
The <code>softmax</code> function is applied to the generated numbers to transform them to probabilities.
</p>
<p>The first argument must be the number of random numbers to generate,
as no other arguments are supplied.
</p>
<p>To test the effect of using different functions,
see <code>multiclass_probability_tibble()</code>.
</p>
<p>N.B. <strong>Multinomial only</strong></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>random_effects</code></td>
<td>
<p>Random effects structure for the Gaussian baseline model. (Character)
</p>
<p>E.g. with <code>"(1|ID)"</code>, the model becomes <code>"y ~ 1 + (1|ID)"</code>.
</p>
<p>N.B. <strong>Gaussian only</strong></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>min_training_rows</code></td>
<td>
<p>Minimum number of rows in the random subsets of <code>`train_data`</code>.
</p>
<p><strong>Gaussian only</strong>. (Integer)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>min_training_rows_left_out</code></td>
<td>
<p>Minimum number of rows left out of the random subsets of <code>`train_data`</code>.
</p>
<p>I.e. a subset will maximally have the size:
</p>
<p><code>max_rows_in_subset = nrow(`train_data`) - `min_training_rows_left_out`</code>.
</p>
<p>N.B. <strong>Gaussian only</strong>. (Integer)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>REML</code></td>
<td>
<p>Whether to use Restricted Maximum Likelihood. (Logical)
</p>
<p>N.B. <strong>Gaussian only</strong>. (Integer)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>parallel</code></td>
<td>
<p>Whether to run the <code>`n`</code> evaluations in parallel. (Logical)
</p>
<p>Remember to register a parallel backend first.
E.g. with <code>doParallel::registerDoParallel</code>.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Packages used:
</p>


<h4>Models</h4>

<p>Gaussian: <code>stats::lm</code>, <code>lme4::lmer</code>
</p>



<h4>Results</h4>

<p><strong>Gaussian</strong>:
</p>
<p>r2m : <code>MuMIn::r.squaredGLMM</code>
</p>
<p>r2c : <code>MuMIn::r.squaredGLMM</code>
</p>
<p>AIC : <code>stats::AIC</code>
</p>
<p>AICc : <code>MuMIn::AICc</code>
</p>
<p>BIC : <code>stats::BIC</code>
</p>
<p><strong>Binomial</strong> and <strong>Multinomial</strong>:
</p>
<p>ROC and related metrics:
</p>
<p>Binomial: <code>pROC::roc</code>
</p>
<p>Multinomial: <code>pROC::multiclass.roc</code>
</p>



<h3>Value</h3>

<p><code>list</code> containing:
</p>

<ol>
<li>
<p> a <code>tibble</code> with summarized results (called <code>summarized_metrics</code>)
</p>
</li>
<li>
<p> a <code>tibble</code> with random evaluations (<code>random_evaluations</code>)
</p>
</li>
<li>
<p> a <code>tibble</code> with the summarized class level results
(<code>summarized_class_level_results</code>)
<strong>(Multinomial only)</strong>
</p>
</li>
</ol>
<p>—————————————————————-
</p>


<h4>Gaussian Results</h4>

<p>—————————————————————-
</p>
<p>The <strong>Summarized Results</strong> <code>tibble</code> contains:
</p>
<p>Average <strong><code>RMSE</code></strong>, <strong><code>MAE</code></strong>, <strong><code>NRMSE(IQR)</code></strong>,
<strong><code>RRSE</code></strong>, <strong><code>RAE</code></strong>, <strong><code>RMSLE</code></strong>.
</p>
<p>See the additional metrics (disabled by default) at <code>?gaussian_metrics</code>.
</p>
<p>The <strong>Measure</strong> column indicates the statistical descriptor used on the evaluations.
The row where <code>Measure == All_rows</code> is the evaluation when the baseline model
is trained on all rows in <code>`train_data`</code>.
</p>
<p>The <strong>Training Rows</strong> column contains the aggregated number of rows used from <code>`train_data`</code>,
when fitting the baseline models.
</p>
<p>....................................................................
</p>
<p>The <strong>Random Evaluations</strong> <code>tibble</code> contains:
</p>
<p>The <strong>non-aggregated metrics</strong>.
</p>
<p>A nested <code>tibble</code> with the <strong>predictions</strong> and targets.
</p>
<p>A nested <code>tibble</code> with the <strong>coefficients</strong> of the baseline models.
</p>
<p>Number of <strong>training rows</strong> used when fitting the baseline model on the training set.
</p>
<p>A nested <strong>Process</strong> information object with information
about the evaluation.
</p>
<p>Name of <strong>dependent</strong> variable.
</p>
<p>Name of <strong>fixed</strong> effect (bias term only).
</p>
<p><strong>Random</strong> effects structure (if specified).
</p>

<p>—————————————————————-
</p>


<h4>Binomial Results</h4>

<p>—————————————————————-
</p>
<p>Based on the generated test set predictions,
a confusion matrix and <code>ROC</code> curve are used to get the following:
</p>
<p><code>ROC</code>:
</p>
<p><strong><code>AUC</code></strong>, <strong><code>Lower CI</code></strong>, and <strong><code>Upper CI</code></strong>
</p>
<p>Note, that the <code>ROC</code> curve is only computed when <code>AUC</code> is enabled.
</p>
<p><code>Confusion Matrix</code>:
</p>
<p><strong><code>Balanced Accuracy</code></strong>,
<strong><code>Accuracy</code></strong>,
<strong><code>F1</code></strong>,
<strong><code>Sensitivity</code></strong>,
<strong><code>Specificity</code></strong>,
<strong><code>Positive Predictive Value</code></strong>,
<strong><code>Negative Predictive Value</code></strong>,
<strong><code>Kappa</code></strong>,
<strong><code>Detection Rate</code></strong>,
<strong><code>Detection Prevalence</code></strong>,
<strong><code>Prevalence</code></strong>, and
<strong><code>MCC</code></strong> (Matthews correlation coefficient).
</p>
<p>....................................................................
</p>
<p>The <strong>Summarized Results</strong> <code>tibble</code> contains:
</p>
<p>The <strong>Measure</strong> column indicates the statistical descriptor used on the evaluations.
The row where <code>Measure == All_0</code> is the evaluation when all predictions are <code>0</code>.
The row where <code>Measure == All_1</code> is the evaluation when all predictions are <code>1</code>.
</p>
<p>The <strong>aggregated metrics</strong>.
</p>
<p>....................................................................
</p>
<p>The <strong>Random Evaluations</strong> <code>tibble</code> contains:
</p>
<p>The <strong>non-aggregated metrics</strong>.
</p>
<p>A nested <code>tibble</code> with the <strong>predictions</strong> and targets.
</p>
<p>A <code>list</code> of <strong>ROC</strong> curve objects (if computed).
</p>
<p>A nested <code>tibble</code> with the <strong>confusion matrix</strong>.
The <code>Pos_</code> columns tells you whether a row is a
True Positive (<code>TP</code>), True Negative (<code>TN</code>), False Positive (<code>FP</code>),
or False Negative (<code>FN</code>), depending on which level is the "positive" class.
I.e. the level you wish to predict.
</p>
<p>A nested <strong>Process</strong> information object with information
about the evaluation.
</p>
<p>Name of <strong>dependent</strong> variable.
</p>

<p>—————————————————————-
</p>


<h4>Multinomial Results</h4>

<p>—————————————————————-
</p>
<p>Based on the generated test set predictions,
one-vs-all (binomial) evaluations are performed and aggregated
to get the same metrics as in the <code>binomial</code> results
(excluding <code>MCC</code>, <code>AUC</code>, <code>Lower CI</code> and <code>Upper CI</code>),
with the addition of <strong>Overall Accuracy</strong> and <em>multiclass</em>
<strong>MCC</strong> in the summarized results.
It is possible to enable multiclass <strong>AUC</strong> as well, which has been
disabled by default as it is slow to calculate when there's a large set of classes.
</p>
<p>Since we use macro-averaging, <strong><code>Balanced Accuracy</code></strong> is the macro-averaged
metric, <em>not</em> the macro sensitivity as sometimes used.
</p>
<p>Note: we also refer to the <em>one-vs-all evaluations</em> as the <em>class level results</em>.
</p>
<p>....................................................................
</p>
<p>The <strong>Summarized Results</strong> <code>tibble</code> contains:
</p>
<p>Summary of the random evaluations.
</p>
<p><strong>How</strong>: First, the one-vs-all binomial evaluations are aggregated by repetition,
then, these aggregations are summarized. Besides the
metrics from the binomial evaluations (see <em>Binomial Results</em> above), it
also includes <strong><code>Overall Accuracy</code></strong> and <em>multiclass</em> <strong><code>MCC</code></strong>.
</p>
<p>The <strong>Measure</strong> column indicates the statistical descriptor used on the evaluations.
The <strong>Mean</strong>, <strong>Median</strong>, <strong>SD</strong>, <strong>IQR</strong>, <strong>Max</strong>, <strong>Min</strong>,
<strong>NAs</strong>, and <strong>INFs</strong> measures describe the <em>Random Evaluations</em> <code>tibble</code>,
while the <strong>CL_Max</strong>, <strong>CL_Min</strong>, <strong>CL_NAs</strong>, and
<strong>CL_INFs</strong> describe the <strong>C</strong>lass <strong>L</strong>evel results.
</p>
<p>The rows where <code>Measure == All_&lt;&lt;class name&gt;&gt;</code> are the evaluations when all
the observations are predicted to be in that class.
</p>
<p>....................................................................
</p>
<p>The <strong>Summarized Class Level Results</strong> <code>tibble</code> contains:
</p>
<p>The (nested) summarized results for each class, with the same metrics and descriptors as
the <em>Summarized Results</em> <code>tibble</code>. Use <code>tidyr::unnest</code>
on the <code>tibble</code> to inspect the results.
</p>
<p><strong>How</strong>: The one-vs-all evaluations are summarized by class.
</p>
<p>The rows where <code>Measure == All_0</code> are the evaluations when none of the observations
are predicted to be in that class, while the rows where <code>Measure == All_1</code> are the
evaluations when all of the observations are predicted to be in that class.
</p>
<p>....................................................................
</p>
<p>The <strong>Random Evaluations</strong> <code>tibble</code> contains:
</p>
<p>The repetition results with the same metrics as the <em>Summarized Results</em> <code>tibble</code>.
</p>
<p><strong>How</strong>: The one-vs-all evaluations are aggregated by repetition.
If a metric contains one or more <code>NAs</code> in the one-vs-all evaluations, it
will lead to an <code>NA</code> result for that repetition.
</p>
<p>Also includes:
</p>
<p>A nested <code>tibble</code> with the one-vs-all binomial evaluations (<strong>Class Level Results</strong>),
including nested <strong>Confusion Matrices</strong> and the
<strong>Support</strong> column, which is a count of how many observations from the
class is in the test set.
</p>
<p>A nested <code>tibble</code> with the <strong>predictions</strong> and targets.
</p>
<p>A <code>list</code> of <strong>ROC</strong> curve objects.
</p>
<p>A nested <code>tibble</code> with the multiclass <strong>confusion matrix</strong>.
</p>
<p>A nested <strong>Process</strong> information object with information
about the evaluation.
</p>
<p>Name of <strong>dependent</strong> variable.
</p>



<h3>Author(s)</h3>

<p>Ludvig Renbo Olsen, <a href="mailto:r-pkgs@ludvigolsen.dk">r-pkgs@ludvigolsen.dk</a>
</p>


<h3>See Also</h3>

<p>Other baseline functions: 
<code>baseline_binomial()</code>,
<code>baseline_gaussian()</code>,
<code>baseline_multinomial()</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">
# Attach packages
library(cvms)
library(groupdata2) # partition()
library(dplyr) # %&gt;% arrange()
library(tibble)

# Data is part of cvms
data &lt;- participant.scores

# Set seed for reproducibility
set.seed(1)

# Partition data
partitions &lt;- partition(data, p = 0.7, list_out = TRUE)
train_set &lt;- partitions[[1]]
test_set &lt;- partitions[[2]]

# Create baseline evaluations
# Note: usually n=100 is a good setting

# Gaussian
baseline(
  test_data = test_set, train_data = train_set,
  dependent_col = "score", random_effects = "(1|session)",
  n = 2, family = "gaussian"
)

# Binomial
baseline(
  test_data = test_set, dependent_col = "diagnosis",
  n = 2, family = "binomial"
)

# Multinomial

# Create some data with multiple classes
multiclass_data &lt;- tibble(
  "target" = rep(paste0("class_", 1:5), each = 10)
) %&gt;%
  dplyr::sample_n(35)

baseline(
  test_data = multiclass_data,
  dependent_col = "target",
  n = 4, family = "multinomial"
)

# Parallelize evaluations

# Attach doParallel and register four cores
# Uncomment:
# library(doParallel)
# registerDoParallel(4)

# Binomial
baseline(
  test_data = test_set, dependent_col = "diagnosis",
  n = 4, family = "binomial"
  #, parallel = TRUE   # Uncomment
)

# Gaussian
baseline(
  test_data = test_set, train_data = train_set,
  dependent_col = "score", random_effects = "(1|session)",
  n = 4, family = "gaussian"
  #, parallel = TRUE   # Uncomment
)

# Multinomial
(mb &lt;- baseline(
  test_data = multiclass_data,
  dependent_col = "target",
  n = 6, family = "multinomial"
  #, parallel = TRUE   # Uncomment
))

# Inspect the summarized class level results
# for class_2
mb$summarized_class_level_results %&gt;%
  dplyr::filter(Class == "class_2") %&gt;%
  tidyr::unnest(Results)

# Multinomial with custom random generator function
# that creates very "certain" predictions
# (once softmax is applied)

rcertain &lt;- function(n) {
  (runif(n, min = 1, max = 100)^1.4) / 100
}

baseline(
  test_data = multiclass_data,
  dependent_col = "target",
  n = 6, family = "multinomial",
  random_generator_fn = rcertain
  #, parallel = TRUE  # Uncomment
)

</code></pre>


</div>