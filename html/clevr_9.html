<div class="container">

<table style="width: 100%;"><tr>
<td>completeness</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Completeness Between Clusterings</h2>

<h3>Description</h3>

<p>Computes the completeness between two clusterings, such
as a predicted and ground truth clustering.
</p>


<h3>Usage</h3>

<pre><code class="language-R">completeness(true, pred)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>true</code></td>
<td>
<p>ground truth clustering represented as a membership
vector. Each entry corresponds to an element and the value identifies
the assigned cluster. The specific values of the cluster identifiers
are arbitrary.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>pred</code></td>
<td>
<p>predicted clustering represented as a membership
vector.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Completeness is an entropy-based measure of the similarity
between two clusterings, say <code class="reqn">t</code> and <code class="reqn">p</code>. The completeness
is high if <em>all</em> members of a given cluster in <code class="reqn">t</code> are assigned
to a single cluster in <code class="reqn">p</code>. The completeness ranges between 0
and 1, where 1 indicates perfect completeness.
</p>


<h3>References</h3>

<p>Rosenberg, A. and Hirschberg, J. "V-measure: A conditional entropy-based external cluster evaluation measure." <em>Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</em> (EMNLP-CoNLL), (2007).
</p>


<h3>See Also</h3>

<p><code>homogeneity</code> evaluates the <em>homogeneity</em>, which is a dual
measure to <em>completeness</em>. <code>v_measure</code> evaluates the harmonic mean of
<em>completeness</em> and <em>homogeneity</em>.
</p>


<h3>Examples</h3>

<pre><code class="language-R">true &lt;- c(1,1,1,2,2)  # ground truth clustering
pred &lt;- c(1,1,2,2,2)  # predicted clustering
completeness(true, pred)

</code></pre>


</div>