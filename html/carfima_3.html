<div class="container">

<table style="width: 100%;"><tr>
<td>carfima</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Fitting a CARFIMA(p, H, q) model via frequentist or Bayesian machinery</h2>

<h3>Description</h3>

<p>A general-order CARFIMA(<code class="reqn">p, H, q</code>) model for <code class="reqn">p&gt;q</code> is
</p>
<p style="text-align: center;"><code class="reqn">Y_t^{(p)} -\alpha_p Y_t^{(p-1)} -\cdots- \alpha_1 Y_t = \sigma(B_{t, H}^{(1)}+\beta_1B_{t, H}^{(2)}+\cdots+\beta_q B_{t, H}^{(q+1)}),</code>
</p>

<p>where <code class="reqn">B_{t, H} = B_t^H</code> is the standard fractional Brownian motion, <code class="reqn">H</code> is the Hurst parameter, and the superscript <code class="reqn">(j)</code> indicates <code class="reqn">j</code>-fold differentiation with respect to <code class="reqn">t</code>; see Equation (1) of Tsai and Chan (2005) for details. The model has <code class="reqn">p+q+2</code> unknown model parameters; <code class="reqn">p</code> <code class="reqn">\alpha_j</code>'s, <code class="reqn">q</code> <code class="reqn">\beta_j</code>'s, <code class="reqn">H</code>, and <code class="reqn">\sigma</code>. Also, the model can account for heteroscedastic measurement errors, if the information about measurement error standard deviations is known.<br></p>
<p>The function <code>carfima</code> fits the model, producing either their maximum likelihood estimates (MLEs) with their asymptotic uncertainties or their posterior samples according to its argument, <code>method</code>. The MLEs except <code class="reqn">\sigma</code> are obtained from a profile likelihood by a global optimizer called the differential evolution algorithm on restricted ranges, i.e., (-0.99, -0.01) for each <code class="reqn">\alpha_j</code>, (0, 100) for each <code class="reqn">\beta_j</code>, and (0.51, 0.99) for <code class="reqn">H</code>; the MLE of <code class="reqn">\sigma</code> is then deterministically computed. The corresponding asymptotic uncertainties are based on a numerical hessian matrix calculation at the MLEs (see function <code>hessian</code> in <span class="pkg">pracma</span>). It also computes the Akaike Information Criterion (AIC) that is <code class="reqn">-2</code>(log likelihood <code class="reqn">-p-q-2</code>). The function <code>carfima</code> becomes numerically unstable if <code class="reqn">p&gt;2</code>, and thus it may produce numerical errors.<br></p>
<p>The Bayesian approach uses independent prior distributions for the unknown model parameters; a Uniform(-0.9999, -0.0001) prior for each <code class="reqn">\alpha_j</code>, a Uniform(0, 100) prior for each <code class="reqn">\beta_j</code>, a Uniform(0.5001, 0.9999) prior for <code class="reqn">H</code> for long memory process, and finally an inverse-Gamma(shape = 2.01, scale = <code class="reqn">10^3</code>) prior for <code class="reqn">\sigma^2</code>. Posterior propriety holds because the prior distributions are jointly proper. It also adopts appropriate proposal density functions; a truncated Normal(current state, proposal scale) between -0.9999 and -0.0001 for each <code class="reqn">\alpha_j</code>, a truncated Normal(current state, proposal scale) between 0 and 100 for each <code class="reqn">\beta_j</code>, a truncated Normal(current state, proposal scale) between 0.5001 and 0.9999 for <code class="reqn">H</code>, and fianlly a Normal(log(current state), proposal scale on a log scale) for <code class="reqn">\sigma^2</code>, i.e., <code class="reqn">\sigma^2</code> is updated on a log scale. We sample the full posterior using Metropolis-Hastings within Gibbs sampler. It also adopts adaptive Markov chain Monte Carlo (MCMC) that updates the proposal scales every 100 iterations; if the acceptance rate of the most recent 100 proposals (at the end of the <code class="reqn">i</code>th 100 iterations) smaller than 0.3, then it multiplies <code class="reqn">\exp(-\min(0.1, 1/\sqrt{i}))</code> by the current proposal scale; if it is larger than 0.3, then it multiplies <code class="reqn">\exp(\min(0.1, 1/\sqrt{i}))</code> by the current proposal scale. The resulting Markov chain with this adaptive scheme converge to the stationary distribution because the adjustment factor, <code class="reqn">\exp(\pm\min(0.1, 1/\sqrt{i}))</code>, approaches unity as <code class="reqn">i</code> goes to infinity, satisfying the diminishing adaptation condition. The function <code>carfima</code> becomes numerically unstable if <code class="reqn">p&gt;2</code>, and thus it may produce numerical errors. The output returns the AIC for which we evaluate the log likelihood at the posterior medians of the unknown model parameters.
</p>


<h3>Usage</h3>

<pre><code class="language-R">carfima(Y, time, measure.error, ar.p, ma.q, method = "mle", 
               bayes.param.ini, bayes.param.scale, bayes.n.warm, bayes.n.sample)</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>Y</code></td>
<td>

<p>A vector of length <code class="reqn">k</code> for the observed data.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>time</code></td>
<td>

<p>A vector of length <code class="reqn">k</code> for the observation times.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>measure.error</code></td>
<td>

<p>(Optional) A vector for the <code class="reqn">k</code> measurement error standard deviations, if such information is available (especially for astronomical applications). A vector of zeros is automatically assigned, if nothing is specified.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>ar.p</code></td>
<td>

<p>A positive integer for the order of the AR model. <code>ar.p</code> must be greater than <code>ma.q</code>. If <code>ar.p</code> is greater than 2, numerical errors may occur.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>ma.q</code></td>
<td>

<p>A non-negative integer for the order of the MA model. <code>ma.q</code> must be smaller than <code>ar.p</code>.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>method</code></td>
<td>

<p>Either "mle" or "bayes". Method "mle" conducts the MLE-based inference, producing MLEs and asymptotic uncertainties of the model parameters. Method "bayes" draws posterior samples of the model parameters. 
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>bayes.param.ini</code></td>
<td>

<p>Only if <code>method</code> is "bayes". A vector of length <code class="reqn">p+q+2</code> for the initial values of <code class="reqn">p</code> <code class="reqn">\alpha_j</code>'s, <code class="reqn">q</code> <code class="reqn">\beta_j</code>'s, <code class="reqn">H</code>, and <code class="reqn">\sigma</code> to implement a Markov chain Monte Carlo method (Metropolis-Hastings within Gibbs sampler). When a CARFIMA(2, <code class="reqn">H</code>, 1) model is fitted, for example, users should set five initial values of <code class="reqn">\alpha_1</code>,  <code class="reqn">\alpha_2</code>, <code class="reqn">\beta_1</code>, <code class="reqn">H</code>, and <code class="reqn">\sigma</code>. 
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>bayes.param.scale</code></td>
<td>

<p>Only if <code>method</code> is "bayes". A vector of length <code class="reqn">p+q+2</code> for jumping (proposal) scales of the Metropolis-Hastings steps. Each number determines how far a Metropolis-Hastings step reaches out in each parameter space. Note that the last number of this vector is the jumping scale to update <code class="reqn">\sigma^2</code> on a log scale. The adaptive MCMC automatically adjusts these jumping scales during the run.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>bayes.n.warm</code></td>
<td>

<p>Only if <code>method</code> is "bayes". A scalar for the number of burn-ins, i.e., the number of the first few iterations to be discarded to remove the effect of initial values.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>bayes.n.sample</code></td>
<td>

<p>Only if <code>method</code> is "bayes". A scalar for the number of posterior samples for each parameter.
</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>The function <code>carfiam</code> produces MLEs, their asymptotic uncertainties, and AIC if <code>method</code> is "mle". It produces the posterior samples of the model parameters, acceptance rates, and AIC if <code>method</code> is "bayes".
</p>


<h3>Value</h3>

<p>The outcome of <code>carfima</code> is composed of:
</p>
<table>
<tr style="vertical-align: top;">
<td><code>mle</code></td>
<td>
<p>If <code>method</code> is "mle". Maximum likelihood estimates of the model parameters, <code class="reqn">p</code> <code class="reqn">\alpha_j</code>'s, <code class="reqn">q</code> <code class="reqn">\beta_j</code>'s, <code class="reqn">H</code>, and <code class="reqn">\sigma</code>. </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>se</code></td>
<td>
<p>If <code>method</code> is "mle". Asymptotic uncertainties (standard errors) of the MLEs.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>param</code></td>
<td>
<p>If <code>method</code> is "bayes". An <code class="reqn">m</code> by <code class="reqn">(p+q+2)</code> matrix where <code class="reqn">m</code> is the number of posterior draws (<code>bayes.n.sample</code>) and the columns correspond to parameters, <code class="reqn">p</code> <code class="reqn">\alpha_j</code>'s, <code class="reqn">q</code> <code class="reqn">\beta_j</code>'s, <code class="reqn">H</code>, and <code class="reqn">\sigma</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>accept</code></td>
<td>
<p>If <code>method</code> is "bayes". A vector of length <code class="reqn">p+q+2</code> for the acceptance rates of the Metropolis-Hastings steps.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>AIC</code></td>
<td>
<p>For both methods. Akaike Information Criterion, -2(log likelihood <code class="reqn">-p-q-2</code>). The log likelihood is evaluated at the MLEs if <code>method</code> is "mle" and at the posterior medians of the unknown model parameters if <code>method</code> is "bayes".</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>fitted.values</code></td>
<td>
<p>For both methods. A vector of length <code class="reqn">k</code> for the values of <code class="reqn">E(Y_{t_i}\vert Y_{&lt;t_i})</code>, <code class="reqn">i=1, 2, \ldots, k</code>, where <code class="reqn">k</code> is the number of observations and <code class="reqn">Y_{&lt;t_i}</code> represents all data observed before <code class="reqn">t_i</code>. Note that <code class="reqn">E(Y_{t_1}\vert Y_{&lt;t_1})=0</code>. MLEs of the model parameters are used to compute <code class="reqn">E(Y_{t_i}\vert Y_{&lt;t_i})</code>'s if <code>method</code> is "mle" and posterior medians of the model parameters are used if <code>method</code> is "bayes".</p>
</td>
</tr>
</table>
<h3>Author(s)</h3>

<p>Hyungsuk Tak, Henghsiu Tsai, Kisung You
</p>


<h3>References</h3>

<p>H. Tsai and K.S. Chan (2005) "Maximum Likelihood Estimation of Linear Continuous Time Long Memory Processes with Discrete Time Data," Journal of the Royal Statistical Society (Series B), 67 (5), 703-716. DOI: 10.1111/j.1467-9868.2005.00522.x
</p>
<p>H. Tsai and K.S. Chan (2000) "A Note on the Covariance Structure of a Continuous-time ARMA Process," Statistica Sinica, 10, 989-998. <br> Link: http://www3.stat.sinica.edu.tw/statistica/j10n3/j10n317/j10n317.htm
</p>


<h3>Examples</h3>

<pre><code class="language-R">  ##### Irregularly spaced observation time generation.

  length.time &lt;- 30
  time.temp &lt;- rexp(length.time, rate = 2)
  time &lt;- rep(NA, length.time + 1)
  time[1] &lt;- 0
  for (i in 2 : (length.time + 1)) {
    time[i] &lt;- time[i - 1] + time.temp[i - 1]
  }
  time &lt;- time[-1]

  ##### Data genration for CARFIMA(1, H, 0) based on the observation times. 

  parameter &lt;- c(-0.4, 0.8, 0.2) 
  # AR parameter alpha = -0.4
  # Hurst parameter = 0.8
  # Process uncertainty (standard deviation) sigma = 0.2

  me.sd &lt;- rep(0.05, length.time)
  # Known measurement error standard deviations 0.05 for all observations
  # If not known, remove the argument "measure.error = me.sd" in the following codes,
  # so that the default values (zero) are automatically assigned.

  y &lt;- carfima.sim(parameter = parameter, time = time, 
                   measure.error = me.sd, ar.p = 1, ma.q = 0)  

  ##### Fitting the CARFIMA(1, H, 0) model on the simulated data for MLEs.
  
  res &lt;- carfima(Y = y, time = time, measure.error = me.sd, 
                 method = "mle", ar.p = 1, ma.q = 0)
  
  # It takes a long time due to the differential evolution algorithm (global optimizer).
  # res$mle; res$se; res$AIC; res$fitted.values

  ##### Fitting the CARFIMA(1, H, 0) model on the simulated data for Bayesian inference.
  
  res &lt;- carfima(Y = y, time = time, measure.error = me.sd, 
                 method = "bayes", ar.p = 1, ma.q = 0, 
                 bayes.param.ini = parameter, 
                 bayes.param.scale = c(rep(0.2, length(parameter))), 
                 bayes.n.warm = 100, bayes.n.sample = 1000)
  
  # It takes a long time because the likelihood evaluation is computationally heavy.
  # The last number of bayes.param.scale is to update sigma2 (not sigma) on a log scale.
  # hist(res$param[, 1]); res$accept; res$AIC; res$fitted.values

  ##### Computing the log likelihood of the CARFIMA(1, H, 0) model given the parameters.
  loglik &lt;- carfima.loglik(Y = y, time = time, ar.p = 1, ma.q = 0,
                           measure.error = me.sd,
                           parameter = parameter, fitted = FALSE)
</code></pre>


</div>