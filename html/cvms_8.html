<div class="container">

<table style="width: 100%;"><tr>
<td>baseline_multinomial</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Create baseline evaluations</h2>

<h3>Description</h3>

<p><a href="https://lifecycle.r-lib.org/articles/stages.html#maturing"><img src="../help/figures/lifecycle-maturing.svg" alt="[Maturing]"></a>
</p>
<p>Create a baseline evaluation of a test set.
</p>
<p>In modelling, a <em>baseline</em> is a result that
is meaningful to compare the results from our models to. For instance, in
classification, we usually want our results to be better than <em>random guessing</em>.
E.g. if we have three classes, we can expect an accuracy of <code>33.33%</code>, as for every
observation we have <code>1/3</code> chance of guessing the correct class. So our model should achieve
a higher accuracy than <code>33.33%</code> before it is more useful to us than guessing.
</p>
<p>While this expected value is often fairly straightforward to find analytically, it
only represents what we can expect on average. In reality, it's possible to get far better
results than that by guessing.
<strong><code>baseline_multinomial()</code></strong>
finds the range of likely values by evaluating multiple sets
of random predictions and summarizing them with a set of useful descriptors.
</p>
<p>Technically, it creates <em>one-vs-all</em> (binomial) baseline evaluations
for the <code>`n`</code> sets of random predictions and summarizes them. Additionally,
sets of "all class x,y,z,..." predictions are evaluated.
</p>


<h3>Usage</h3>

<pre><code class="language-R">baseline_multinomial(
  test_data,
  dependent_col,
  n = 100,
  metrics = list(),
  random_generator_fn = runif,
  parallel = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>test_data</code></td>
<td>
<p><code>data.frame</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>dependent_col</code></td>
<td>
<p>Name of dependent variable in the supplied test and training sets.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>n</code></td>
<td>
<p>The number of sets of random predictions to evaluate. (Default is <code>100</code>)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>metrics</code></td>
<td>
<p><code>list</code> for enabling/disabling metrics.
</p>
<p>E.g. <code>list("F1" = FALSE)</code> would remove <code>F1</code> from the results,
and <code>list("Accuracy" = TRUE)</code> would add the regular <code>Accuracy</code> metric
to the results.
Default values (<code>TRUE</code>/<code>FALSE</code>) will be used for the remaining available metrics.
</p>
<p>You can enable/disable all metrics at once by including
<code>"all" = TRUE/FALSE</code> in the <code>list</code>. This is done prior to enabling/disabling
individual metrics, why f.i. <code>list("all" = FALSE, "Accuracy" = TRUE)</code>
would return only the <code>Accuracy</code> metric.
</p>
<p>The <code>list</code> can be created with
<code>multinomial_metrics()</code>.
</p>
<p>Also accepts the string <code>"all"</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>random_generator_fn</code></td>
<td>
<p>Function for generating random numbers.
The <code>softmax</code> function is applied to the generated numbers to transform them to probabilities.
</p>
<p>The first argument must be the number of random numbers to generate,
as no other arguments are supplied.
</p>
<p>To test the effect of using different functions,
see <code>multiclass_probability_tibble()</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>parallel</code></td>
<td>
<p>Whether to run the <code>`n`</code> evaluations in parallel. (Logical)
</p>
<p>Remember to register a parallel backend first.
E.g. with <code>doParallel::registerDoParallel</code>.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Packages used:
</p>
<p>Multiclass <code>ROC</code> curve and <code>AUC</code>:
<code>pROC::multiclass.roc</code>
</p>


<h3>Value</h3>

<p><code>list</code> containing:
</p>

<ol>
<li>
<p> a <code>tibble</code> with summarized results (called <code>summarized_metrics</code>)
</p>
</li>
<li>
<p> a <code>tibble</code> with random evaluations (<code>random_evaluations</code>)
</p>
</li>
<li>
<p> a <code>tibble</code> with the summarized class level results
(<code>summarized_class_level_results</code>)
</p>
</li>
</ol>
<p>....................................................................
</p>


<h4>Macro metrics</h4>

<p>Based on the generated predictions,
<em>one-vs-all</em> (binomial) evaluations are performed and aggregated
to get the following <strong>macro</strong> metrics:
</p>
<p><strong><code>Balanced Accuracy</code></strong>,
<strong><code>F1</code></strong>,
<strong><code>Sensitivity</code></strong>,
<strong><code>Specificity</code></strong>,
<strong><code>Positive Predictive Value</code></strong>,
<strong><code>Negative Predictive Value</code></strong>,
<strong><code>Kappa</code></strong>,
<strong><code>Detection Rate</code></strong>,
<strong><code>Detection Prevalence</code></strong>, and
<strong><code>Prevalence</code></strong>.
</p>
<p>In general, the metrics mentioned in
<code>binomial_metrics()</code>
can be enabled as macro metrics
(excluding <code>MCC</code>, <code>AUC</code>, <code>Lower CI</code>,
<code>Upper CI</code>, and the <code>AIC/AICc/BIC</code> metrics).
These metrics also has a weighted average
version.
</p>
<p><strong>N.B.</strong> we also refer to the <em>one-vs-all evaluations</em> as the <em>class level results</em>.
</p>



<h4>Multiclass metrics</h4>

<p>In addition, the <strong><code>Overall Accuracy</code></strong> and <em>multiclass</em>
<strong><code>MCC</code></strong> metrics are computed. <em>Multiclass</em> <code>AUC</code> can be enabled but
is slow to calculate with many classes.
</p>

<p>....................................................................
</p>
<p>The <strong>Summarized Results</strong> <code>tibble</code> contains:
</p>
<p>Summary of the random evaluations.
</p>
<p><strong>How</strong>: The one-vs-all binomial evaluations are aggregated by repetition and summarized. Besides the
metrics from the binomial evaluations, it
also includes <strong><code>Overall Accuracy</code></strong> and <em>multiclass</em> <strong><code>MCC</code></strong>.
</p>
<p>The <strong>Measure</strong> column indicates the statistical descriptor used on the evaluations.
The <strong>Mean</strong>, <strong>Median</strong>, <strong>SD</strong>, <strong>IQR</strong>, <strong>Max</strong>, <strong>Min</strong>,
<strong>NAs</strong>, and <strong>INFs</strong> measures describe the <em>Random Evaluations</em> <code>tibble</code>,
while the <strong>CL_Max</strong>, <strong>CL_Min</strong>, <strong>CL_NAs</strong>, and
<strong>CL_INFs</strong> describe the <strong>C</strong>lass <strong>L</strong>evel results.
</p>
<p>The rows where <code>Measure == All_&lt;&lt;class name&gt;&gt;</code> are the evaluations when all
the observations are predicted to be in that class.
</p>
<p>....................................................................
</p>
<p>The <strong>Summarized Class Level Results</strong> <code>tibble</code> contains:
</p>
<p>The (nested) summarized results for each class, with the same metrics and descriptors as
the <em>Summarized Results</em> <code>tibble</code>. Use <code>tidyr::unnest</code>
on the <code>tibble</code> to inspect the results.
</p>
<p><strong>How</strong>: The one-vs-all evaluations are summarized by class.
</p>
<p>The rows where <code>Measure == All_0</code> are the evaluations when none of the observations
are predicted to be in that class, while the rows where <code>Measure == All_1</code> are the
evaluations when all of the observations are predicted to be in that class.
</p>
<p>....................................................................
</p>
<p>The <strong>Random Evaluations</strong> <code>tibble</code> contains:
</p>
<p>The repetition results with the same metrics as the <em>Summarized Results</em> <code>tibble</code>.
</p>
<p><strong>How</strong>: The one-vs-all evaluations are aggregated by repetition.
If a metric contains one or more <code>NAs</code> in the one-vs-all evaluations, it
will lead to an <code>NA</code> result for that repetition.
</p>
<p>Also includes:
</p>
<p>A nested <code>tibble</code> with the one-vs-all binomial evaluations (<strong>Class Level Results</strong>),
including nested <strong>Confusion Matrices</strong> and the
<strong>Support</strong> column, which is a count of how many observations from the
class is in the test set.
</p>
<p>A nested <code>tibble</code> with the <strong>predictions</strong> and targets.
</p>
<p>A <code>list</code> of <strong>ROC</strong> curve objects.
</p>
<p>A nested <code>tibble</code> with the multiclass <strong>confusion matrix</strong>.
</p>
<p>A nested <strong>Process</strong> information object with information
about the evaluation.
</p>
<p>Name of <strong>dependent</strong> variable.
</p>


<h3>Author(s)</h3>

<p>Ludvig Renbo Olsen, <a href="mailto:r-pkgs@ludvigolsen.dk">r-pkgs@ludvigolsen.dk</a>
</p>


<h3>See Also</h3>

<p>Other baseline functions: 
<code>baseline()</code>,
<code>baseline_binomial()</code>,
<code>baseline_gaussian()</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">
# Attach packages
library(cvms)
library(groupdata2) # partition()
library(dplyr) # %&gt;% arrange()
library(tibble)

# Data is part of cvms
data &lt;- participant.scores

# Set seed for reproducibility
set.seed(1)

# Partition data
partitions &lt;- partition(data, p = 0.7, list_out = TRUE)
train_set &lt;- partitions[[1]]
test_set &lt;- partitions[[2]]

# Create baseline evaluations
# Note: usually n=100 is a good setting

# Create some data with multiple classes
multiclass_data &lt;- tibble(
  "target" = rep(paste0("class_", 1:5), each = 10)
) %&gt;%
  dplyr::sample_n(35)

baseline_multinomial(
  test_data = multiclass_data,
  dependent_col = "target",
  n = 4
)

# Parallelize evaluations

# Attach doParallel and register four cores
# Uncomment:
# library(doParallel)
# registerDoParallel(4)

# Make sure to uncomment the parallel argument
(mb &lt;- baseline_multinomial(
  test_data = multiclass_data,
  dependent_col = "target",
  n = 6
  #, parallel = TRUE  # Uncomment
))

# Inspect the summarized class level results
# for class_2
mb$summarized_class_level_results %&gt;%
  dplyr::filter(Class == "class_2") %&gt;%
  tidyr::unnest(Results)

# Multinomial with custom random generator function
# that creates very "certain" predictions
# (once softmax is applied)

rcertain &lt;- function(n) {
  (runif(n, min = 1, max = 100)^1.4) / 100
}

# Make sure to uncomment the parallel argument
baseline_multinomial(
  test_data = multiclass_data,
  dependent_col = "target",
  n = 6,
  random_generator_fn = rcertain
  #, parallel = TRUE  # Uncomment
)

</code></pre>


</div>