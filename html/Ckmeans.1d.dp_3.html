<div class="container">

<table style="width: 100%;"><tr>
<td>Univariate Clustering</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Optimal (Weighted) Univariate Clustering</h2>

<h3>Description</h3>

<p>Perform optimal univariate <code class="reqn">k</code>-means or <code class="reqn">k</code>-median clustering in linear (fastest), loglinear, or quadratic (slowest) time.
</p>


<h3>Usage</h3>

<pre><code class="language-R">Ckmeans.1d.dp(x, k=c(1,9), y=1,
              method=c("linear", "loglinear", "quadratic"),
              estimate.k=c("BIC", "BIC 3.4.12"))

Ckmedian.1d.dp(x, k=c(1,9), y=1,
               method=c("linear", "loglinear", "quadratic"),
               estimate.k=c("BIC", "BIC 3.4.12"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>a numeric vector of data to be clustered. All <code>NA</code> elements must be removed from <code>x</code> before calling this function. The function will run faster on sorted <code>x</code> (in non-decreasing order) than an unsorted input.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>k</code></td>
<td>
<p>either an exact integer number of clusters, or a vector of length two specifying the minimum and maximum numbers of clusters to be examined. The default is <code>c(1,9)</code>. When <code>k</code> is a range, the actual number of clusters is determined by Bayesian information criterion.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>y</code></td>
<td>
<p>a value of 1 (default) to specify equal weights of 1 for each element in <code>x</code>, or a numeric vector of unequal non-negative weights for each element in <code>x</code>. It is highly recommended to use positive (instead of zero) weights to account for the influence of every element. The weights have a strong impact on the clustering result. When the number of clusters <code>k</code> is given as a range, the weights should be linearly scaled to sum up to the observed sample size. Currently, <code>Ckmedian.1d.dp</code> only works with an equal weight of 1.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>method</code></td>
<td>
<p>a character string to specify the speedup method to the original cubic runtime dynamic programming. The default is <code>"linear"</code>. All methods generate the same optimal results but differ in runtime or memory usage. See Details.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>estimate.k</code></td>
<td>
<p>a character string to specify the method to estimate optimal <code>k</code>. This argument is effective only when a range for <code>k</code> is provided. The default is <code>"BIC"</code>. See Details.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p><code>Ckmean.1d.dp</code> minimizes unweighted or weighted within-cluster sum of squared distance (L2).
</p>
<p><code>Ckmedian.1d.dp</code> minimizes within-cluster sum of distance (L1). Only unweighted solution is implemented and guarantees optimality.
</p>
<p>In contrast to the heuristic <var>k</var>-means algorithms implemented in function <code>kmeans</code>, this function optimally assigns elements in numeric vector <code>x</code> into <code>k</code> clusters by dynamic programming (Wang and Song 2011; Song and Zhong 2020). It minimizes the total of within-cluster sums of squared distances (<var>withinss</var>) between each element and its corresponding cluster mean. When a range is provided for <code>k</code>, the exact number of clusters is determined by Bayesian information criterion (Song and Zhong 2020). Different from the heuristic <var>k</var>-means algorithms whose results may be non-optimal or change from run to run, the result of Ckmeans.1d.dp is guaranteed to be optimal and reproducible, and its advantage in efficiency and accuracy over heuristic <var>k</var>-means methods is most pronounced at large <var>k</var>.
</p>
<p>The <code>estimate.k</code> argument specifies the method to select optimal <code>k</code> based on the Gaussian mixture model using the Bayesian information criterion (BIC). When <code>estimate.k="BIC"</code>, it effectively deals with variance estimation for a cluster with identical values. When <code>estimate.k="BIC 3.4.12"</code>, it uses the code in version 3.4.12 and earlier to estimate <code>k</code>.
</p>
<p>The <code>method</code> argument specifies one of three options to speed up the original dynamic programming taking a runtime cubic in sample size <var>n</var>. The default <code>"linear"</code> option, giving a total runtime of <code class="reqn">O(n \lg n + kn)</code> or <code class="reqn">O(kn)</code> (if <code>x</code> is already sorted in ascending order) is the fastest option but uses the most memory (still <code class="reqn">O(kn)</code>) (Song and Zhong 2020); the <code>"loglinear"</code> option, with a runtime of <code class="reqn">O(kn \lg n)</code>, is slightly slower but uses the least memory (Song and Zhong 2020); the slowest <code>"quadratic"</code> option (Wang and Song 2011), with a runtime of <code class="reqn">O(kn^2)</code>, is provided for the purpose of testing on small data sets.
</p>
<p>When the sample size <var>n</var> is too large to create two <code class="reqn">k \times n</code> dynamic programming matrices in memory, we recommend the heuristic solutions implemented in the <code>kmeans</code> function in package <span class="pkg">stats</span>.
</p>


<h3>Value</h3>

<p>An object of class "<code>Ckmeans.1d.dp</code>" or "<code>Ckmedian.1d.dp</code>". It is a list containing the following components:
</p>
<table>
<tr style="vertical-align: top;">
<td><code>cluster</code></td>
<td>
<p>a vector of clusters assigned to each element in <code>x</code>. Each cluster is indexed by an integer from 1 to <code>k</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>centers</code></td>
<td>
<p>a numeric vector of the (weighted) means for each cluster.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>withinss</code></td>
<td>
<p>a numeric vector of the (weighted) within-cluster sum of squares for each cluster.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>size</code></td>
<td>
<p>a vector of the (weighted) number of elements in each cluster.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>totss</code></td>
<td>
<p>total sum of (weighted) squared distances between each element and the sample mean. This statistic is not dependent on the clustering result.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>tot.withinss</code></td>
<td>
<p>total sum of (weighted) within-cluster squared distances between each element and its cluster mean. This statistic is minimized given the number of clusters.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>betweenss</code></td>
<td>
<p>sum of (weighted) squared distances between each cluster mean and sample mean. This statistic is maximized given the number of clusters.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>xname</code></td>
<td>
<p>a character string. The actual name of the <code>x</code> argument.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>yname</code></td>
<td>
<p>a character string. The actual name of the <code>y</code> argument.</p>
</td>
</tr>
</table>
<p>Each class has a print and a plot method, which are described along with <code>print.Ckmeans.1d.dp</code> and <code>plot.Ckmeans.1d.dp</code>.
</p>


<h3>Author(s)</h3>

<p>Joe Song and Haizhou Wang
</p>


<h3>References</h3>

<p>Song M, Zhong H (2020).
“Efficient weighted univariate clustering maps outstanding dysregulated genomic zones in human cancers.”
<em>Bioinformatics</em>, <b>36</b>(20), 5027–5036.
<a href="https://doi.org/10.1093/bioinformatics/btaa613">doi:10.1093/bioinformatics/btaa613</a>.<br><br> Wang H, Song M (2011).
“Ckmeans.1d.dp: Optimal <code class="reqn">k</code>-means clustering in one dimension by dynamic programming.”
<em>The R Journal</em>, <b>3</b>(2), 29–33.
<a href="https://doi.org/10.32614/RJ-2011-015">doi:10.32614/RJ-2011-015</a>.
</p>


<h3>See Also</h3>

<p><code>ahist</code>, <code>plot.Ckmeans.1d.dp</code>, <code>print.Ckmeans.1d.dp</code> in this package.
</p>
<p><code>kmeans</code> in package <span class="pkg">stats</span> that implements several heuristic <code class="reqn">k</code>-means algorithms.
</p>


<h3>Examples</h3>

<pre><code class="language-R"># Ex. 1 The number of clusters is provided.
# Generate data from a Gaussian mixture model of three components
x &lt;- c(rnorm(50, sd=0.2), rnorm(50, mean=1, sd=0.3), rnorm(100,
       mean=-1, sd=0.25))
# Divide x into 3 clusters
k &lt;- 3

result &lt;- Ckmedian.1d.dp(x, k)

plot(result, main="Optimal univariate k-median given k")

result &lt;- Ckmeans.1d.dp(x, k)

plot(result, main="Optimal univariate k-means given k")

plot(x, col=result$cluster, pch=result$cluster, cex=1.5,
     main="Optimal univariate k-means clustering given k",
     sub=paste("Number of clusters given:", k))
abline(h=result$centers, col=1:k, lty="dashed", lwd=2)
legend("bottomleft", paste("Cluster", 1:k), col=1:k, pch=1:k,
       cex=1.5, bty="n")

# Ex. 2 The number of clusters is determined by Bayesian
#       information criterion
# Generate data from a Gaussian mixture model of three components
x &lt;- c(rnorm(50, mean=-3, sd=1), rnorm(50, mean=0, sd=.5),
       rnorm(50, mean=3, sd=1))
# Divide x into k clusters, k automatically selected (default: 1~9)

result &lt;- Ckmedian.1d.dp(x)

plot(result, main="Optimal univariate k-median with k estimated")

result &lt;- Ckmeans.1d.dp(x)

plot(result, main="Optimal univariate k-means with k estimated")

k &lt;- max(result$cluster)
plot(x, col=result$cluster, pch=result$cluster, cex=1.5,
     main="Optimal univariate k-means clustering with k estimated",
     sub=paste("Number of clusters is estimated to be", k))
abline(h=result$centers, col=1:k, lty="dashed", lwd=2)
legend("topleft", paste("Cluster", 1:k), col=1:k, pch=1:k,
       cex=1.5, bty="n")

# Ex. 3 Segmenting a time course using optimal weighted
#       univariate clustering
n &lt;- 160
t &lt;- seq(0, 2*pi*2, length=n)
n1 &lt;- 1:(n/2)
n2 &lt;- (max(n1)+1):n
y1 &lt;- abs(sin(1.5*t[n1]) + 0.1*rnorm(length(n1)))
y2 &lt;- abs(sin(0.5*t[n2]) + 0.1*rnorm(length(n2)))
y &lt;- c(y1, y2)

w &lt;- y^8 # stress the peaks
res &lt;- Ckmeans.1d.dp(t, k=c(1:10), w)
plot(res)
plot(t, w, main = "Time course weighted k-means",
     col=res$cluster, pch=res$cluster,
     xlab="Time t", ylab="Transformed intensity w",
     type="h")
abline(v=res$centers, col="chocolate", lty="dashed")
text(res$centers, max(w) * .95, cex=0.5, font=2,
     paste(round(res$size / sum(res$size) * 100), "/ 100"))
</code></pre>


</div>