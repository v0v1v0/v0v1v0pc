<div class="container">

<table style="width: 100%;"><tr>
<td>constructLearner</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>
Construction of Specific Learners for CVST
</h2>

<h3>Description</h3>

<p>These methods construct a <code>CVST.learner</code> object suitable for the
CVST method. These objects provide the common interface needed for the
<code>CV</code> and <code>fastCV</code> methods. We provide kernel
logistic regression, kernel ridge regression, support vector machines
and support vector regression as fully functional implementation templates.
</p>


<h3>Usage</h3>

<pre><code class="language-R">constructLearner(learn, predict)
constructKlogRegLearner()
constructKRRLearner()
constructSVMLearner()
constructSVRLearner()
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>learn</code></td>
<td>

<p>The learning methods which takes a <code>CVST.data</code> and list of
parameters and return a model.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>predict</code></td>
<td>

<p>The prediction method which takes a model and <code>CVST.data</code> and
returns the corresponding predictions.
</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>The nu-SVM and nu-SVR are build on top the corresponding implementations of
the <code>kernlab</code> package (see reference). In the list of parameters these
implementations expect an entry named <code>kernel</code>, which gives the
name of the kernel that should be used, an entry named <code>nu</code>
specifying the nu parameter, and an entry named <code>C</code> giving the C
parameter for the nu-SVR.
</p>
<p>The KRR and KLR also expect <code>kernel</code> and necessary other
parameters to construct the kernel. Both methods expect a lambda
parameter and KLR additonally a tol and maxiter parameter in the
parameter list.
</p>
<p>Note that the lambda of KRR/KLR and the C parameter of SVR are scaled
by the data set size to allow for comparable results in the fast CV loop.
</p>


<h3>Value</h3>

<p>Returns a learner of type <code>CVST.learner</code> suitable for <code>CV</code> and <code>fastCV</code>.
</p>


<h3>Author(s)</h3>

<p>Tammo Krueger &lt;tammokrueger@googlemail.com&gt;
</p>


<h3>References</h3>

<p>Alexandros Karatzoglou, Alexandros Smola, Kurt Hornik, Achim Zeileis.
kernlab - An S4 Package for Kernel Methods in R
<em>Journal of Statistical Software</em> Vol. 11, Issue 9, Nov 2004.
DOI: doi: <a href="https://doi.org/10.18637/jss.v011.i09">10.18637/jss.v011.i09</a>.
</p>
<p>Volker Roth.
Probabilistic discriminative kernel classifiers for multi-class problems.
In <em>Proceedings of the 23rd DAGM-Symposium on Pattern Recognition</em>, pages 246â€“253, 2001.
</p>


<h3>See Also</h3>

<p><code>CV</code>
<code>fastCV</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R"># SVM
ns = noisySine(100)
svm = constructSVMLearner()
p = list(kernel="rbfdot", sigma=100, nu=.1)
m = svm$learn(ns, p)
nsTest = noisySine(1000)
pred = svm$predict(m, nsTest)
sum(pred != nsTest$y) / getN(nsTest)
# Kernel logistic regression
klr = constructKlogRegLearner()
p = list(kernel="rbfdot", sigma=100, lambda=.1/getN(ns), tol=10e-6, maxiter=100)
m = klr$learn(ns, p)
pred = klr$predict(m, nsTest)
sum(pred != nsTest$y) / getN(nsTest)
# SVR
ns = noisySinc(100)
svr = constructSVRLearner()
p = list(kernel="rbfdot", sigma=100, nu=.1, C=1*getN(ns))
m = svr$learn(ns, p)
nsTest = noisySinc(1000)
pred = svr$predict(m, nsTest)
sum((pred - nsTest$y)^2) / getN(nsTest)
# Kernel ridge regression
krr = constructKRRLearner()
p = list(kernel="rbfdot", sigma=100, lambda=.1/getN(ns))
m = krr$learn(ns, p)
pred = krr$predict(m, nsTest)
sum((pred - nsTest$y)^2) / getN(nsTest)
</code></pre>


</div>