<div class="container">

<table style="width: 100%;"><tr>
<td>cocoScore</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Scoring Rule Based Model Assessment Procedure</h2>

<h3>Description</h3>

<p>The function calculates the log, quadratic and ranked probability scores for assessing relative performance of a fitted model as proposed by Czado et al. (2009).
</p>


<h3>Usage</h3>

<pre><code class="language-R">cocoScore(coco, val.num = 1e-10, julia = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>coco</code></td>
<td>
<p>An object of class coco</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>val.num</code></td>
<td>
<p>A non-negative real number which is used to stop the calculation
of the score in case of GP models. The default value is 1e-10</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>julia</code></td>
<td>
<p>if TRUE, the scores are computed with Julia.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Scoring rules assign a numerical score based on the predictive
distribution and the observed data  to measure the quality of probabilistic predictions.
They are provided here as a model selection tool and are computed as
averages over the relevant set of (in-sample) predictions. Scoring rules are, generally, negatively oriented
penalties that one seeks to minimize. The literature has developed a large number of scoring
rules and, unless there is a unique and clearly defined underlying decision problem,
there is no automatic choice of a (proper) scoring rule to be used in any given situation.
Therefore, the use of a variety of scoring rules may be appropriate to take advantage of
specific emphases and strengths. Three proper scoring rules
(for a definition of the concept of propriety see Gneiting and Raftery, 2007)
which Jung, McCabe and Tremayne (2016) found to be particularly useful are implemented.
For more information see the references listed below.
</p>


<h3>Value</h3>

<p>a list containing the log score, quadratic score and ranked probability score.
</p>


<h3>Author(s)</h3>

<p>Manuel Huth
</p>


<h3>References</h3>

<p>Czado, C. and Gneitling, T. and Held, L. (2009) Predictive Model Assessment for Count Data. <em>Biometrics</em>, <b>65</b>, 4, 1254–1261.
</p>
<p>Gneiting, T. and Raftery, A. E. (2007) Strictly proper scoring rules, prediction, and estimation. <em>Journal
of the American Statistical Association</em>, 102:359-378.
</p>
<p>Jung, Robert C., Brendan P. M. McCabe, and Andrew R. Tremayne. (2016). Model validation and diagnostics. <em>In Handbook of Discrete
Valued Time Series</em>. Edited by Richard A. Davis, Scott H. Holan, Robert Lund and Nalini Ravishanker. Boca Raton: Chapman and
Hall, pp. 189–218.
</p>
<p>Jung, R. C. and Tremayne, A. R. (2011) Convolution-closed models for count timeseries with applications. <em>Journal of Time Series Analysis</em>, <b>32</b>, 3, 268–280.
</p>


<h3>Examples</h3>

<pre><code class="language-R">lambda &lt;- 1
alpha &lt;- 0.4
set.seed(12345)
data &lt;- cocoSim(order = 1, type = "Poisson", par = c(lambda, alpha), length = 100)
#julia_installed = TRUE ensures that the fit object
#is compatible with the julia cocoScore implementation 
fit &lt;- cocoReg(order = 1, type = "Poisson", data = data)

#assessment using scoring rules - R implementation
score_r &lt;- cocoScore(fit)
</code></pre>


</div>