<div class="container">

<table style="width: 100%;"><tr>
<td>modelEval</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2> Statistical evaluation of predictions </h2>

<h3>Description</h3>

<p>Using predictions of given model produced by <code>predict.CoreModel</code> and correct labels,
computes  some statistics evaluating the quality of the model.
</p>


<h3>Usage</h3>

<pre><code class="language-R">modelEval(model=NULL, correctClass, predictedClass, 
          predictedProb=NULL, costMatrix=NULL, 
          priorClProb = NULL, avgTrainPrediction = NULL, beta = 1) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>model</code></td>
<td>
<p> The model structure as returned by <code>CoreModel</code>, or NULL if some other predictions are evaluated. </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>correctClass</code></td>
<td>
<p> A vector of correct class labels for classification problem and function values for regression problem. </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>predictedClass</code></td>
<td>
<p> A vector of predicted class labels for classification problem and function values for regression problem. </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>predictedProb</code></td>
<td>
<p> An optional matrix of predicted class probabilities for classification.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>costMatrix</code></td>
<td>
<p> Optional cost matrix can provide nonuniform costs for classification problems. </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>priorClProb</code></td>
<td>
<p> If <code>model=NULL</code> a vector of prior class probabilities shall be provided in case of classification. </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>avgTrainPrediction</code></td>
<td>
<p> If <code>model=NULL</code> mean of prediction values on training set shall be provided in case of regression. </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>beta</code></td>
<td>
<p>For two class problems <code>beta</code> controls the relative importance of precision and recall in F-measure.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>The function uses the <code>model</code> structure as returned by <code>CoreModel</code>,
<code>predictedClass</code> and <code>predictedProb</code> returned by 
<code>predict.CoreModel</code>. Predicted values are compared with true values 
and some statistics are computed  measuring the quality of predictions.
In classification only one of the <code>predictedClass</code> and <code>predictedProb</code> can be NULL
(one of them is computed from the other under assumption that class label is assigned to the most probable class). 
Some of the returned statistics  are defined only for two class problems, for which the 
confusion matrix specifying the number of instances of true/predicted class is
defined as follows,
</p>

<table>
<tr>
<td style="text-align: left;">
         true/predicted class </td>
<td style="text-align: center;">  positive           </td>
<td style="text-align: center;"> negative     </td>
</tr>
<tr>
<td style="text-align: left;">
                   positive   </td>
<td style="text-align: center;"> true positive (TP)  </td>
<td style="text-align: center;"> false negative (FN) </td>
</tr>
<tr>
<td style="text-align: left;">
                   negative   </td>
<td style="text-align: center;"> false positive (FP) </td>
<td style="text-align: center;"> true negative (TN)
       </td>
</tr>
</table>
<p>Optional cost matrix can provide nonuniform costs for classification problems. For regression
problem this parameter is ignored. The costs can be different from the ones used for building the model 
in <code>CoreModel</code> and prediction with the model in <code>predict.CoreModel</code>.
If no costs are supplied, uniform costs are assumed. 
The format of the matrix is <code>costMatrix(true_class, predicted_class)</code>. 
By default a uniform costs are assumed, i.e.,  <code>costMatrix(i, i) = 0</code>, and <code>costMatrix(i, j) = 1</code>, 
for <code>i</code> not equal to <code>j</code>. See the example below.
</p>
<p>If a non-CORElearn model is evaluated, one should set <code>model=NULL</code>, and  a vector of prior of class 
probabilities <code>priorClProb</code> shall be provided in case of classification,
and in case of regression <code>avgTrainPrediction</code> shall be the mean of prediction values 
(estimated on a e.g., training set).
</p>


<h3>Value</h3>

<p>For classification problem function returns list with the components
</p>
<table>
<tr style="vertical-align: top;">
<td><code>accuracy</code></td>
<td>
<p>classification accuracy, for two class problems this would equal 
</p>
<p style="text-align: center;"><code class="reqn">\rm{accuracy}=\frac{TP+TN}{TP+FN+FP+TN}</code>
</p>
 </td>
</tr>
<tr style="vertical-align: top;">
<td><code>averageCost</code></td>
<td>
<p>average classification cost</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>informationScore</code></td>
<td>
<p>information score statistics measuring information contents in the predicted probabilities</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>AUC</code></td>
<td>
<p>Area under the ROC curve</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>predictionMatrix</code></td>
<td>
<p>matrix of miss-classifications also confusion matrix</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>sensitivity</code></td>
<td>
<p>sensitivity for two class problems (also called accuracy of the positive class, i.e., acc+, or true positive rate),
</p>
<p style="text-align: center;"><code class="reqn">rm{sensitivity} = \frac{TP}{TP+FN}</code>
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>specificity</code></td>
<td>
<p>specificity for two class problems (also called accuracy of the negative class, i.e., acc-, or true negative rate),
</p>
<p style="text-align: center;"><code class="reqn">\rm{specificity} = \frac{TN}{TN+FP}</code>
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>brierScore</code></td>
<td>
<p>Brier score of predicted probabilities (the original Brier's definition which scores all the classes not only the correct one)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>kappa</code></td>
<td>
<p>Cohen's kappa statistics measuring randomness of the predictions; for perfect predictions kappa=1, for completely random predictions kappa=0 </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>precision</code></td>
<td>
<p>precision for two class problems
</p>
<p style="text-align: center;"><code class="reqn">\rm{precision} = \frac{TP}{TP+FP}</code>
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>recall</code></td>
<td>
<p>recall for two class problems (the same as sensitivity) </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>F-measure</code></td>
<td>
<p>F-measure giving a weighted score of precision and recall for two class problems
</p>
<p style="text-align: center;"><code class="reqn">F= \frac{(1+\beta^2)\cdot \rm{recall} \cdot \rm{precision}}{\beta^2 \cdot \rm{recall} + \rm{precision}}</code>
</p>
 </td>
</tr>
<tr style="vertical-align: top;">
<td><code>G-mean</code></td>
<td>
<p>geometric mean of positive and negative accuracy,
</p>
<p style="text-align: center;"><code class="reqn">G=\sqrt{\rm{senstivity} \cdot \rm{specificity}} </code>
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>KS</code></td>
<td>
<p>Kolmogorov-Smirnov statistics defined for binary classification problems, reports the distance between the probability distributions of positive class
for positive and negative instances, see (Hand, 2005), value 0 means no separation, and value 1 means perfect separation,
</p>
<p style="text-align: center;"><code class="reqn">KS = \max_t |TPR(t)-FPR(t)|</code>
</p>
 
<p>see definitions of TPR and FPR below</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>TPR</code></td>
<td>
<p>true positive rate <code class="reqn">TPR = \frac{TP}{TP+FN}</code> at maximal value of <code>KS</code> statistics</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>FPR</code></td>
<td>
<p>false positive rate <code class="reqn">FPR = \frac{FP}{FP+TN}</code> at maximal value of <code>KS</code> statistics</p>
</td>
</tr>
</table>
<p>For regression problem the returned list has components
</p>
<table>
<tr style="vertical-align: top;">
<td><code>MSE</code></td>
<td>
<p>square root of Mean Squared Error</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>RMSE</code></td>
<td>
<p>Relative Mean Squared Error</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>MAE</code></td>
<td>
<p>Mean Absolute Error</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>RMAE</code></td>
<td>
<p>Relative Mean Absolute Error</p>
</td>
</tr>
</table>
<h3>Author(s)</h3>

<p> Marko Robnik-Sikonja</p>


<h3>References</h3>

 
<p>Igor Kononenko, Matjaz Kukar: <em>Machine Learning and Data Mining: Introduction to Principles and Algorithms. </em>
Horwood, 2007
</p>
<p>David J.Hand: Good practice in retail credit scorecard assesment. <em>Journal of Operational Research Society</em>, 56:1109-1117, 2005)
</p>


<h3>See Also</h3>

<p><code>CORElearn</code>,
<code>CoreModel</code>,
<code>predict.CoreModel</code>.
</p>


<h3>Examples</h3>

<pre><code class="language-R"># use iris data

# build random forests model with certain parameters
model &lt;- CoreModel(Species ~ ., iris, model="rf", 
              selectionEstimator="MDL",minNodeWeightRF=5,
              rfNoTrees=100, maxThreads=1)

# prediction with node distribution
pred &lt;- predict(model, iris, rfPredictClass=FALSE)

# Model evaluation
mEval &lt;- modelEval(model, iris[["Species"]], pred$class, pred$prob)
print(mEval)

# use nonuniform cost matrix
noClasses &lt;- length(levels(iris[["Species"]]))
costMatrix &lt;- 1 - diag(noClasses)
costMatrix[3,1] &lt;- costMatrix[3,2] &lt;- 5 # assume class 3 is more valuable  
mEvalCost &lt;- modelEval(model, iris[["Species"]], pred$class, pred$prob, 
                       costMatrix=costMatrix)
print(mEvalCost)

destroyModels(model) # clean up

</code></pre>


</div>