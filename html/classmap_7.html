<div class="container">

<table style="width: 100%;"><tr>
<td>makeFV</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>
Constructs feature vectors from a kernel matrix.
</h2>

<h3>Description</h3>

<p>Constructs feature vectors from a kernel matrix.
</p>


<h3>Usage</h3>

<pre><code class="language-R">makeFV(kmat, transfmat = NULL, precS = 1e-12)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>kmat</code></td>
<td>
<p>a kernel matrix. If <code>transfmat</code> is <code>NULL</code>, we are dealing with training data and then <code>kmat</code> must be a square kernel matrix (of size <code class="reqn">n</code> by <code class="reqn">n</code> when there are <code class="reqn">n</code> cases). Such a PSD matrix kmat can e.g. be produced by <code>makeKernel</code> or by <code>kernlab::kernelMatrix</code>. If on the other hand <code>transfmat</code> is not <code>NULL</code>, we are dealing with a test set. See details for the precise working.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>transfmat</code></td>
<td>
<p>transformation matrix. If not <code>NULL</code>, it is the value <code>transfmat</code> of <code>makeFV</code> on training data. It has to be a square matrix, with as many rows as there were training data.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>precS</code></td>
<td>
<p>if not <code>NULL</code>, eigenvalues of <code>kmat</code> below <code>precS</code> will be set equal to precS.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>If <code>transfmat</code> is non-<code>NULL</code>, we are dealing with a test set.
Denote the number of cases in the test set by <code class="reqn">m \geq 1</code>. Each row of <code>kmat</code> of the test set then must contain the kernel values of a new case with all cases in the training set. Therefore the kernel matrix kmat must have dimensions <code class="reqn">m</code> by <code class="reqn">n</code>. The matrix <code>kmat</code> can e.g. be produced by <code>makeKernel</code>. It can also be obtained by running <code>kernlab::kernelMatrix</code> on the union of the training set and the test set, yielding an <code class="reqn">(n+m)</code> by <code class="reqn">(n+m)</code> matrix, from which one then takes the <code class="reqn">[(n+1):m , 1:n]</code> submatrix.
</p>


<h3>Value</h3>

<p>A list with components: <br></p>
<table>
<tr style="vertical-align: top;">
<td><code>Xf</code></td>
<td>
<p>When makeKV is applied to the training set, <code>Xf</code> has coordinates of <code class="reqn">n</code> points (vectors), the plain inner products of which equal the kernel matrix of the training set. That is, <code>kmat</code> = <code>Xf</code> <code>Xf</code>'. The <code>Xf</code> are expressed in an orthogonal basis in which the variance of the coordinates is decreasing, which is useful when plotting the first few coordinates. When <code>makeFV</code> is applied to a test set, <code>Xf</code> are coordinates of the feature vectors of the test set in the same space as those of the training set, and then <code>kmat</code> = <code>Xf</code> %*% <code>Xf of training data</code>.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>transfmat</code></td>
<td>
<p>square matrix for transforming kmat to <code>Xf</code>.</p>
</td>
</tr>
</table>
<h3>Author(s)</h3>

<p>Raymaekers J., Rousseeuw P.J., Hubert, M.
</p>


<h3>References</h3>

<p>Raymaekers J., Rousseeuw P.J., Hubert M. (2021). Class maps for visualizing classification results. <em>Technometrics</em>, appeared online. doi: <a href="https://doi.org/10.1080/00401706.2021.1927849">10.1080/00401706.2021.1927849</a>(link to open access pdf)
</p>


<h3>See Also</h3>

<p><code>makeKernel</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">library(e1071)
set.seed(1); X &lt;- matrix(rnorm(200 * 2), ncol = 2)
X[1:100, ] &lt;- X[1:100, ] + 2
X[101:150, ] &lt;- X[101:150, ] - 2
y &lt;- as.factor(c(rep("blue", 150), rep("red", 50)))
cols &lt;- c("deepskyblue3", "red")
plot(X, col = cols[as.numeric(y)], pch = 19)
# We now fit an SVM with radial basis kernel to the data:
svmfit &lt;- svm(y~.,  data = data.frame(X = X, y = y),  scale = FALSE,
             kernel = "radial", cost = 10, gamma = 1, probability = TRUE)
Kxx &lt;- makeKernel(X, svfit = svmfit)
outFV &lt;- makeFV(Kxx)
Xf &lt;- outFV$Xf # The data matrix in this feature space.
dim(Xf) # The feature vectors are high dimensional.
# The inner products of Xf match the kernel matrix:
max(abs(as.vector(Kxx - crossprod(t(Xf), t(Xf))))) # 3.005374e-13 # tiny, OK
range(rowSums(Xf^2)) # all points in Xf lie on the unit sphere.
pairs(Xf[, 1:5], col = cols[as.numeric(y)])
# In some of these we see spherical effects, e.g.
plot(Xf[, 1], Xf[, 5], col = cols[as.numeric(y)], pch = 19)
# The data look more separable here than in the original
# two-dimensional space.

# For more examples, we refer to the vignette:
## Not run: 
vignette("Support_vector_machine_examples")

## End(Not run)
</code></pre>


</div>