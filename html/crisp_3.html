<div class="container">

<table style="width: 100%;"><tr>
<td>crispCV</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>CRISP with Tuning Parameter Selection via Cross-Validation.</h2>

<h3>Description</h3>

<p>This function implements CRISP, which considers the problem of predicting an outcome variable on the basis of two covariates, using an interpretable yet non-additive model.
CRISP partitions the covariate space into blocks in a data-adaptive way, and fits a mean model within each block. Unlike other partitioning methods,
CRISP is fit using a non-greedy approach by solving a convex optimization problem, resulting in low-variance fits. This function differs
from the <code>crisp</code> function in that the tuning parameter, lambda, is automatically selected using K-fold cross-validation.
More details are provided in Petersen, A., Simon, N., and Witten, D. (2016). Convex Regression with Interpretable Sharp Partitions. Journal of Machine Learning Research, 17(94): 1-31 &lt;http://jmlr.org/papers/volume17/15-344/15-344.pdf&gt;.
</p>


<h3>Usage</h3>

<pre><code class="language-R">crispCV(y, X, q = NULL, lambda.min.ratio = 0.01, n.lambda = 50,
  lambda.seq = NULL, fold = NULL, n.fold = NULL, seed = NULL,
  within1SE = FALSE, rho = 0.1, e_abs = 10^-4, e_rel = 10^-3,
  varyrho = TRUE, double.run = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>y</code></td>
<td>
<p>An n-vector containing the response.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>X</code></td>
<td>
<p>An n x 2 matrix with each column containing a covariate.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>q</code></td>
<td>
<p>The desired granularity of the CRISP fit, <code>M.hat</code>, which will be a <code>q</code> by <code>q</code> matrix. <code>M.hat</code>
is a mean matrix whose element <code>M.hat[i,j]</code> contains the mean for pairs of covariate values within a quantile range
of the observed predictors <code>X[,1]</code> and <code>X[,2]</code>. For example, <code>M.hat[1,2]</code> represents the
mean of the observations with the first covariate value less than the <code>1/q</code>-quantile of <code>X[,1]</code>,
and the second covariate value between the <code>1/q</code>- and <code>2/q</code>-quantiles of <code>X[,2]</code>.
If left <code>NULL</code>, then <code>q=n</code> is used when n&lt;100, and <code>q=100</code> is used when n&gt;=100.
We recommend using <code>q&lt;=100</code> as higher values take longer to fit and provide an unneeded amount of granularity.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lambda.min.ratio</code></td>
<td>
<p>The smallest value for <code>lambda.seq</code>, as a fraction of the maximum lambda value, which is the data-derived
smallest value for which the fit is a constant value. The default is 0.01.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>n.lambda</code></td>
<td>
<p>The number of lambda values to consider - the default is 50.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lambda.seq</code></td>
<td>
<p>A user-supplied sequence of positive lambda values to consider. The typical usage is to calculate
<code>lambda.seq</code> using <code>lambda.min.ratio</code> and <code>n.lambda</code>, but providing <code>lambda.seq</code> overrides this. If provided,
<code>lambda.seq</code> should be a decreasing sequence of values, since CRISP relies on warm starts for speed.
Thus fitting the model for a whole sequence of lambda values is often faster than fitting for a single lambda value.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>fold</code></td>
<td>
<p>User-supplied fold numbers for cross-validation. If supplied, <code>fold</code> should be an n-vector with entries in 1,...,K when doing K-fold cross-validation. The default is to choose <code>fold</code> using <code>n.fold</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>n.fold</code></td>
<td>
<p>The number of folds, K, to use for the K-fold cross-validation selection of the tuning parameter, lambda. The default is 10 - specification of <code>fold</code> overrides use of <code>n.fold</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>seed</code></td>
<td>
<p>An optional number used with <code>set.seed()</code> at the beginning of the function. This is only relevant if <code>fold</code> is not specified by the user.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>within1SE</code></td>
<td>
<p>Logical value indicating how cross-validated tuning parameters should be chosen. If <code>within1SE=TRUE</code>, lambda is chosen to be the value corresponding to the most sparse model with cross-validation error within one standard error of the minimum cross-validation error. If <code>within1SE=FALSE</code>, lambda is chosen to be the value corresponding to the minimum cross-validation error.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>rho</code></td>
<td>
<p>The penalty parameter for our ADMM algorithm. The default is 0.1.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>e_abs, e_rel</code></td>
<td>
<p>Values used in the stopping criterion for our ADMM algorithm, and discussed in Appendix C.2 of the CRISP paper.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>varyrho</code></td>
<td>
<p>Should <code>rho</code> be varied from iteration to iteration? This is discussed in Appendix C.3 of the CRISP paper.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>double.run</code></td>
<td>
<p>The initial complete run of our ADMM algorithm will yield sparsity in z_1i and z_2i, but not
necessarily exact equality of the rows and columns of <code>M.hat</code>. If <code>double.run</code> is <code>TRUE</code>, then the algorithm
is run a second time to obtain <code>M.hat</code> with exact equality of the appropriate rows and columns. This issue
is discussed further in Appendix C.4 of the CRISP paper.</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>An object of class <code>crispCV</code>, which can be summarized using <code>summary</code>, plotted using <code>plot</code>, and used to predict outcome values for new covariates using <code>predict</code>.
</p>

<ul>
<li>
<p><code>lambda.cv</code>: Optimal lambda value chosen by K-fold cross-validation.
</p>
</li>
<li>
<p><code>index.cv</code>: The index of the model corresponding to the chosen tuning parameter, <code>lambda.cv</code>. That is, <code>lambda.cv=crisp.out$lambda.seq[index.cv]</code>.
</p>
</li>
<li>
<p><code>crisp.out</code>: An object of class <code>crisp</code> returned by <code>crisp</code>.
</p>
</li>
<li>
<p><code>mean.cv.error</code>: An m-vector containing cross-validation error where m is the length of <code>lambda.seq</code>. Note that <code>mean.cv.error[i]</code> contains the cross-validation error for the tuning parameter <code>crisp.out$lambda.seq[i]</code>.
</p>
</li>
<li>
<p><code>se.cv.error</code>: An m-vector containing cross-validation standard error where m is the length of <code>lambda.seq</code>. Note that <code>se.cv.error[i]</code> contains the standard error of the cross-validation error for the tuning parameter <code>crisp.out$lambda.seq[i]</code>.
</p>
</li>
<li>
<p>Other elements: As specified by the user.
</p>
</li>
</ul>
<h3>See Also</h3>

<p><code>crisp</code>, <code>plot</code>, <code>summary</code>, <code>predict</code>, <code>plot.cvError</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">## Not run: 
#See ?'crisp-package' for a full example of how to use this package

#generate data (using a very small 'n' for illustration purposes)
set.seed(1)
data &lt;- sim.data(n = 15, scenario = 2)

#fit model and select lambda using 2-fold cross-validation
#note: use larger 'n.fold' (e.g., 10) in practice
crispCV.out &lt;- crispCV(X = data$X, y = data$y, n.fold = 2)

## End(Not run)
</code></pre>


</div>