<div class="container">

<table style="width: 100%;"><tr>
<td>Cluster_Gauss_Newton_method</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Cluster_Gauss_Newton_method</h2>

<h3>Description</h3>

<p>Find multiple minimisers of the nonlinear least squares problem.
</p>
<p style="text-align: center;"><code class="reqn">argmin_x ||f(x)-y*||</code>
</p>

<p>where
</p>
<ol>
<li>
<p> f: nonlinear function (e.g., mathematical model)
</p>
</li>
<li>
<p> y*: target vector (e.g., observed data to fit the mathematical model)
</p>
</li>
<li>
<p> x: variable of the nonlinear function that we aim to find the values that minimize (minimizers) the differences between the nonlinear function and target vector (e.g., model parameter)
</p>
</li>
</ol>
<p>Parameter estimation problems of mathematical models can often be formulated as nonlinear least squares problems.  In this context f can be thought at a model, x is the parameter, and y* is the observation.
CGNM iteratively estimates the minimizer of the nonlinear least squares problem from various initial estimates hence finds multiple minimizers.
Full detail of the algorithm and comparison with conventional method is available in the following publication, also please cite this publication when this algorithm is used in your research: Aoki et al. (2020) &lt;doi.org/10.1007/s11081-020-09571-2&gt;. Cluster Gaussâ€“Newton method. Optimization and Engineering, 1-31.  As illustrated in this paper, CGNM is faster and more robust compared to repeatedly applying the conventional optimization/nonlinear least squares algorithm from various initial estimates. In addition, CGNM can realize this speed assuming the nonlinear function to be a black-box function (e.g. does not use things like adjoint equation of a system of ODE as the function does not have to be based on a system of ODEs.).
</p>


<h3>Usage</h3>

<pre><code class="language-R">Cluster_Gauss_Newton_method(
  nonlinearFunction,
  targetVector,
  initial_lowerRange,
  initial_upperRange,
  lowerBound = NA,
  upperBound = NA,
  ParameterNames = NA,
  stayIn_initialRange = FALSE,
  num_minimizersToFind = 250,
  num_iteration = 25,
  saveLog = TRUE,
  runName = "",
  textMemo = "",
  algorithmParameter_initialLambda = 1,
  algorithmParameter_gamma = 2,
  algorithmVersion = 3,
  initialIterateMatrix = NA,
  targetMatrix = NA,
  weightMatrix = NA,
  keepInitialDistribution = NA,
  MO_weights = NA,
  MO_values = NA,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>nonlinearFunction</code></td>
<td>
<p>(required input) <em>A function with input of a vector x of real number of length n and output a vector y of real number of length m.</em> In the context of model fitting the nonlinearFunction is <strong>the model</strong>.  Given the CGNM does not assume the uniqueness of the minimizer, m can be less than n.  Also CGNM does not assume any particular form of the nonlinear function and also does not require the function to be continuously differentiable (see Appendix D of our publication for an example when this function is discontinuous). Also this function can be matrix to matrix equation.  This can be used for parallerization, see vignettes for examples.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>targetVector</code></td>
<td>
<p>(required input) <em>A vector of real number of length m</em> where we minimize the Euclidean distance between the nonlinearFuncition and targetVector.  In the context of curve fitting targetVector can be though as <strong>the observational data</strong>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>initial_lowerRange</code></td>
<td>
<p>(required input) <em>A vector of real number of length n</em> where each element represents  <strong>the lower range of the initial iterate</strong>. Similarly to regular Gauss-Newton method, CGNM iteratively reduce the residual to find minimizers.  Essential differences is that CGNM start from the initial RANGE and not an initial point.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>initial_upperRange</code></td>
<td>
<p>(required input) <em>A vector of real number of length n</em> where each element represents  <strong>the upper range of the initial iterate</strong>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lowerBound</code></td>
<td>
<p>(default: NA) <em>A vector of real number or NA of length n</em> where each element represents  <strong>the lower bound of the parameter search</strong>.  If no lower bound set that element NA. Note that CGNM is an unconstraint optimization method so the final minimizer can be anywhere.  In the parameter estimation problem, there often is a constraints to the parameters (e.g., parameters cannot be negative). So when the upper or lower bound is set using this option, parameter transformation is conducted internally (e.g., if either the upper or lower bound is given parameters are log transformed, if the upper and lower bounds are given logit transform is used.)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>upperBound</code></td>
<td>
<p>(default: NA) <em>A vector of real number or NA of length n</em> where each element represents  <strong>the upper bound of the parameter search</strong>.  If no upper bound set that element NA.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>ParameterNames</code></td>
<td>
<p>(default: NA) <em>A vector of string</em> of length n User can specify names of the parameters that will be used for the plots.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>stayIn_initialRange</code></td>
<td>
<p>(default: FALSE) <em>TRUE or FALSE</em> if set TRUE, the parameter search will conducted strictly within the range specified by initial_lowerRange and initial_upperRange.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>num_minimizersToFind</code></td>
<td>
<p>(default: 250) <em>A positive integer</em> defining number of approximate minimizers CGNM will find. We usually <strong>use 250 when testing the model and 1000 for the final analysis</strong>.  The computational cost increase proportionally to this number; however, larger number algorithm becomes more stable and increase the chance of finding more better minimizers. See Appendix C of our paper for detail.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>num_iteration</code></td>
<td>
<p>(default: 25)  <em>A positive integer</em> defining maximum number of iterations. We usually <strong>set 25 while model building and 100 for final analysis</strong>.  Given each point terminates the computation when the convergence criterion is met the computation cost does not grow proportionally to the number of iterations (hence safe to increase this without significant increase in the computational cost).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>saveLog</code></td>
<td>
<p>(default: TRUE) <em>TRUE or FALSE</em> indicating either or not to save computation result from each iteration in CGNM_log folder. It requires disk write access right in the current working directory. <strong>Recommended to set TRUE if the computation is expected to take long time</strong> as user can retrieve intrim computation result even if the computation is terminated prematurely (or even during the computation).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>runName</code></td>
<td>
<p>(default: "") <em>string</em> that user can ue to identify the CGNM runs. The run history will be saved in the folder name CGNM_log_&lt;runName&gt;.  If this is set to "TIME" then runName is automatically set by the run start time.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>textMemo</code></td>
<td>
<p>(default: "") <em>string</em> that user can write an arbitrary text (without influencing computation). This text is stored with the computation result so that can be used for example to describe model so that the user can recognize the computation result.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>algorithmParameter_initialLambda</code></td>
<td>
<p>(default: 1) <em>A positive number</em> for initial value for the regularization coefficient lambda see Appendix B of of our paper for detail.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>algorithmParameter_gamma</code></td>
<td>
<p>(default: 2) <em>A positive number</em> a positive scalar value for adjusting the strength of the weighting for the linear approximation see Appendix A of our paper for detail.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>algorithmVersion</code></td>
<td>
<p>(default: 3.0) <em>A positive number</em> user can choose different version of CGNM algorithm currently 1.0 and 3.0 are available.  If number chosen other than 1.0 or 3.0 it will choose 1.0.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>initialIterateMatrix</code></td>
<td>
<p>(default: NA) <em>A matrix</em> with dimension num_minimizersToFind x n.  User can provide initial iterate as a matrix  This input is used when the user wishes not to generate initial iterate randomly from the initial range.  The user is responsible for ensuring all function evaluation at each initial iterate does not produce NaN.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>targetMatrix</code></td>
<td>
<p>(default: NA) <em>A matrix</em> with dimension num_minimizersToFind x m  User can define multiple target vectors in the matrix form.  This input is mainly used when running bootstrap method and not intended to be used for other purposes.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>weightMatrix</code></td>
<td>
<p>(default: NA) <em>A matrix</em> with dimension num_minimizersToFind x m  User can define multiple weight vectors in the matrix form to weight the observations.  This input is mainly used when running case sampling bootstrap method and not intended to be used for other purposes.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>keepInitialDistribution</code></td>
<td>
<p>(default: NA) <em>A vector of TRUE or FALSE</em> of length n User can specify if the initial distribution of one of the input variable (e.g. parameter) to be kept as the initial iterate throughout CGNM iterations.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>MO_weights</code></td>
<td>
<p>(default: NA) <em>A numeric vector</em> where the weights for the middle out methods are specified.  The length of the vector should be the same as the number of parameters. MO can be used to incoperate prior knowledge of the parameter to be estimated, weight indicate an arbitrary confidence for the prior information. (MO method is still under methodological development.)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>MO_values</code></td>
<td>
<p>(default: NA) <em>A numeric vector</em> where the values for the middle out methods are specified.  The length of the vector should be the same as the number of parameters. MO can be used to incoperate prior knowledge of the parameter to be estimated. (MO method is still under methodological development.)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>Further arguments to be supplied to nonlinearFunction</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>list of a matrix X, Y,residual_history and initialX, as well as a list runSetting
</p>
<ol>
<li>
<p> X: <em>a num_minimizersToFind by n matrix</em> which stores the approximate minimizers of the nonlinear least squares in each row. In the context of model fitting they are <strong>the estimated parameter sets</strong>.
</p>
</li>
<li>
<p> Y: <em>a num_minimizersToFind by m matrix</em> which stores the nonlinearFunction evaluated at the corresponding approximate minimizers in matrix X above. In the context of model fitting each row corresponds to <strong>the model simulations</strong>.
</p>
</li>
<li>
<p> residual_history: <em>a num_iteration by num_minimizersToFind matrix</em> storing sum of squares residual for all iterations.
</p>
</li>
<li>
<p> initialX: <em>a num_minimizersToFind by n matrix</em> which stores the set of initial iterates.
</p>
</li>
<li>
<p> runSetting: a list containing all the input variables to Cluster_Gauss_Newton_method (i.e., nonlinearFunction, targetVector, initial_lowerRange, initial_upperRange ,algorithmParameter_initialLambda, algorithmParameter_gamma, num_minimizersToFind, num_iteration, saveLog, runName, textMemo).</p>
</li>
</ol>
<h3>Examples</h3>

<pre><code class="language-R">##lip-flop kinetics (an example known to have two distinct solutions)

model_analytic_function=function(x){

 observation_time=c(0.1,0.2,0.4,0.6,1,2,3,6,12)
 Dose=1000
 F=1

 ka=x[1]
 V1=x[2]
 CL_2=x[3]
 t=observation_time

 Cp=ka*F*Dose/(V1*(ka-CL_2/V1))*(exp(-CL_2/V1*t)-exp(-ka*t))

 log10(Cp)
}

observation=log10(c(4.91, 8.65, 12.4, 18.7, 24.3, 24.5, 18.4, 4.66, 0.238))

CGNM_result=Cluster_Gauss_Newton_method(
nonlinearFunction=model_analytic_function,
targetVector = observation, num_iteration = 10, num_minimizersToFind = 100,
initial_lowerRange = c(0.1,0.1,0.1), initial_upperRange =  c(10,10,10),
saveLog = FALSE)

acceptedApproximateMinimizers(CGNM_result)

## Not run: 
library(RxODE)

model_text="
d/dt(X_1)=-ka*X_1
d/dt(C_2)=(ka*X_1-CL_2*C_2)/V1"

model=RxODE(model_text)
#define nonlinearFunction
model_function=function(x){

observation_time=c(0.1,0.2,0.4,0.6,1,2,3,6,12)

theta &lt;- c(ka=x[1],V1=x[2],CL_2=x[3])
ev &lt;- eventTable()
ev$add.dosing(dose = 1000, start.time =0)
ev$add.sampling(observation_time)
odeSol=model$solve(theta, ev)
log10(odeSol[,"C_2"])

}

observation=log10(c(4.91, 8.65, 12.4, 18.7, 24.3, 24.5, 18.4, 4.66, 0.238))

CGNM_result=Cluster_Gauss_Newton_method(nonlinearFunction=model_function,
targetVector = observation, saveLog = FALSE,
initial_lowerRange = c(0.1,0.1,0.1),initial_upperRange =  c(10,10,10))
## End(Not run)

</code></pre>


</div>