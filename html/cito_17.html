<div class="container">

<table style="width: 100%;"><tr>
<td>config_tuning</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Config hyperparameter tuning</h2>

<h3>Description</h3>

<p>Config hyperparameter tuning
</p>


<h3>Usage</h3>

<pre><code class="language-R">config_tuning(
  CV = 5,
  steps = 10,
  parallel = FALSE,
  NGPU = 1,
  cancel = TRUE,
  bootstrap_final = NULL,
  bootstrap_parallel = FALSE,
  return_models = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>CV</code></td>
<td>
<p>numeric, specifies k-folded cross validation</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>steps</code></td>
<td>
<p>numeric, number of random tuning steps</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>parallel</code></td>
<td>
<p>numeric, number of parallel cores (tuning steps are parallelized)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>NGPU</code></td>
<td>
<p>numeric, set if more than one GPU is available, tuning will be parallelized over CPU cores and GPUs, only works for NCPU &gt; 1</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>cancel</code></td>
<td>
<p>CV/tuning for specific hyperparameter set if model cannot reduce loss below baseline after burnin or returns NA loss</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>bootstrap_final</code></td>
<td>
<p>bootstrap final model, if all models should be boostrapped it must be set globally via the bootstrap argument in the <code>dnn()</code> function</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>bootstrap_parallel</code></td>
<td>
<p>should the bootstrapping be parallelized or not</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>return_models</code></td>
<td>
<p>return individual models</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Note that hyperparameter tuning can be expensive. We have implemented an option to parallelize hyperparameter tuning, including parallelization over one or more GPUs (the hyperparameter evaluation is parallelized, not the CV). This can be especially useful for small models. For example, if you have 4 GPUs, 20 CPU cores, and 20 steps (random samples from the random search), you could run ‘dnn(..., device="cuda",lr = tune(), batchsize=tune(), tuning=config_tuning(parallel=20, NGPU=4)’, which will distribute 20 model fits across 4 GPUs, so that each GPU will process 5 models (in parallel).
</p>


</div>