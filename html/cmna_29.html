<div class="container">

<table style="width: 100%;"><tr>
<td>gradient</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Gradient descent</h2>

<h3>Description</h3>

<p>Use gradient descent to find local minima
</p>


<h3>Usage</h3>

<pre><code class="language-R">graddsc(fp, x, h = 0.001, tol = 1e-04, m = 1000)

gradasc(fp, x, h = 0.001, tol = 1e-04, m = 1000)

gd(fp, x, h = 100, tol = 1e-04, m = 1000)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>fp</code></td>
<td>
<p>function representing the derivative of <code>f</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>an initial estimate of the minima</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>h</code></td>
<td>
<p>the step size</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>tol</code></td>
<td>
<p>the error tolerance</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>m</code></td>
<td>
<p>the maximum number of iterations</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Gradient descent can be used to find local minima of functions.  It
will return an approximation based on the step size <code>h</code> and
<code>fp</code>.  The <code>tol</code> is the error tolerance, <code>x</code> is the
initial guess at the minimum.  This implementation also stops after
<code>m</code> iterations.
</p>


<h3>Value</h3>

<p>the <code>x</code> value of the minimum found
</p>


<h3>See Also</h3>

<p>Other optimz: 
<code>bisection()</code>,
<code>goldsect</code>,
<code>hillclimbing()</code>,
<code>newton()</code>,
<code>sa()</code>,
<code>secant()</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">fp &lt;- function(x) { x^3 + 3 * x^2 - 1 }
graddsc(fp, 0)

f &lt;- function(x) { (x[1] - 1)^2 + (x[2] - 1)^2 }
fp &lt;-function(x) {
    x1 &lt;- 2 * x[1] - 2
    x2 &lt;- 8 * x[2] - 8

    return(c(x1, x2))
}
gd(fp, c(0, 0), 0.05)
</code></pre>


</div>