<div class="container">

<table style="width: 100%;"><tr>
<td>measures</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Goodness-of-fit, confidence and consistency measures</h2>

<h3>Description</h3>

<p>Wrapper function for calculating the predictive distribution model's
<code>confidence</code>, <code>consistency</code>, and optionally some
well-known goodness-of-fit measures as well. The calculated measures are as
follows: </p>
 <ul>
<li>
<p> confidence in predictions (CP) and confidence in
positive predictions (CPP) within known presences for the training and
evaluation subsets </p>
</li>
<li>
<p> consistency of predictions (difference of CPs; DCP)
and positive predictions (difference of CPPs; DCPP) </p>
</li>
<li>
<p> Area Under the ROC
Curve (AUC) - optional (see parameter <code>goodness</code>) </p>
</li>
<li>
<p> maximum of the
True Skill Statistic (maxTSS) - optional (see parameter <code>goodness</code>)</p>
</li>
</ul>
<h3>Usage</h3>

<pre><code class="language-R">measures(
  observations,
  predictions,
  evaluation_mask,
  goodness = FALSE,
  df = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>observations</code></td>
<td>
<p>Either an integer or logical vector containing the binary
observations where presences are encoded as <code>1</code>s/<code>TRUE</code>s and
absences as <code>0</code>s/<code>FALSE</code>s.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>predictions</code></td>
<td>
<p>A numeric vector containing the predicted probabilities of
occurrence typically within the <code>[0, 1]</code> interval.
<code>length(predictions)</code> should be equal to <code>length(observations)</code>
and the order of the elements should match.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>evaluation_mask</code></td>
<td>
<p>A logical vector (mask) of the evaluation subset. Its
<code>i</code>th element indicates whether the  <code>i</code>th element of
<code>observations</code> was used for evaluation (<code>TRUE</code>) or for training
(<code>FALSE</code>). <code>length(evaluation_mask)</code> should be equal to
<code>length(observations)</code> and the order of the elements should match,
i.e. <code>observations[evaluation_mask]</code> were the evaluation subset and
<code>observations[!evaluation_mask]</code> were the training subset.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>goodness</code></td>
<td>
<p>Logical vector of length one, defaults to <code>FALSE</code>.
Indicates, whether goodness-of-fit measures (AUC and maxTSS) should be
calculated. If set to <code>TRUE</code>, external package <span class="pkg">ROCR</span> (Sing et al.
2005) is needed for the calculation (see section 'Note').</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>df</code></td>
<td>
<p>Logical vector of length one, defaults to <code>FALSE</code>. Indicates,
whether the returned value should be a one-row <code>data.frame</code> that is
<code>rbind()</code>able if <code>measures()</code> is called on multiple models in a
<code>for</code> loop or a <code>lapply()</code>. See section 'Value' and 'Examples'
for details.</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>A named numeric vector (if <code>df</code> is <code>FALSE</code>; the default) or
a <code>data.frame</code> (if <code>df</code> is <code>TRUE</code>) of one row.
<code>length()</code> of the vector or <code>ncol()</code> of the <code>data.frame</code> is
6 (if <code>goodness</code> is <code>FALSE</code>; the default) or 8 (if
<code>goodness</code> is <code>TRUE</code>). The name of the elements/columns are as
follows: </p>
 <dl>
<dt>CP_train</dt>
<dd>
<p>confidence in predictions within known
presences (CP) for the training subset</p>
</dd> <dt>CP_eval</dt>
<dd>
<p>confidence in
predictions within known presences (CP) for the evaluation subset</p>
</dd>
<dt>DCP</dt>
<dd>
<p>consistency of predictions (difference of CPs)</p>
</dd> <dt>CPP_train</dt>
<dd>
<p>confidence in
positive predictions within known presences (CPP) for the training subset</p>
</dd>
<dt>CPP_eval</dt>
<dd>
<p>confidence in positive predictions within known presences
(CPP) for the evaluation subset</p>
</dd> <dt>DCPP</dt>
<dd>
<p>consistency of positive
predictions (difference of CPPs)</p>
</dd> <dt>AUC</dt>
<dd>
<p>Area Under the ROC Curve (Hanley and McNeil 1982;
calculated by <code>ROCR::performance()</code>). This
element/column is available only if parameter '<code>goodness</code>' is set to
<code>TRUE</code>. If package <span class="pkg">ROCR</span> is not available but parameter
'<code>goodness</code>' is set to <code>TRUE</code>, the value of AUC is
<code>NA_real_</code> and a warning is raised.</p>
</dd> <dt>maxTSS</dt>
<dd>
<p>Maximum of the True
Skill Statistic (Allouche et al. 2006; calculated by
<code>ROCR::performance()</code>). This element/column
is available only if parameter '<code>goodness</code>' is set to <code>TRUE</code>. If
package <span class="pkg">ROCR</span> is not available but parameter '<code>goodness</code>' is set
to <code>TRUE</code>, the value of maxTSS is <code>NA_real_</code> and a warning is
raised.</p>
</dd> </dl>
<h3>Note</h3>

<p>Since <span class="pkg">confcons</span> is a light-weight, stand-alone packages, it does
not import package <span class="pkg">ROCR</span> (Sing et al. 2005), i.e. installing
<span class="pkg">confcons</span> does not mean installing <span class="pkg">ROCR</span> automatically. If you
need AUC and maxTSS (i.e., parameter '<code>goodness</code>' is set to
<code>TRUE</code>), you should install <span class="pkg">ROCR</span> or install <span class="pkg">confcons</span> along
with its dependencies (i.e., <code>devtools::install_github(repo =
  "bfakos/confcons", dependencies = TRUE)</code>).
</p>


<h3>References</h3>

 <ul>
<li>
<p> Allouche O, Tsoar A, Kadmon R (2006): Assessing
the accuracy of species distribution models: prevalence, kappa and the true
skill statistic (TSS). Journal of Applied Ecology 43(6): 1223-1232.
<a href="https://doi.org/10.1111/j.1365-2664.2006.01214.x">doi:10.1111/j.1365-2664.2006.01214.x</a>. </p>
</li>
<li>
<p> Hanley JA, McNeil BJ (1982):
The meaning and use of the area under a receiver operating characteristic
(ROC) curve. Radiology 143(1): 29-36.
<a href="https://doi.org/10.1148/radiology.143.1.7063747">doi:10.1148/radiology.143.1.7063747</a>. </p>
</li>
<li>
<p> Sing T, Sander O, Beerenwinkel
N, Lengauer T. (2005): ROCR: visualizing classifier performance in R.
Bioinformatics 21(20): 3940-3941. <a href="https://doi.org/10.1093/bioinformatics/bti623">doi:10.1093/bioinformatics/bti623</a>. </p>
</li>
</ul>
<h3>See Also</h3>

<p><code>confidence</code> for calculating confidence,
<code>consistency</code> for calculating consistency,
<code>ROCR::performance()</code> for calculating AUC and
TSS
</p>


<h3>Examples</h3>

<pre><code class="language-R">set.seed(12345)
dataset &lt;- data.frame(
	observations = c(rep(x = FALSE, times = 500),
                  rep(x = TRUE, times = 500)),
	predictions_model1 = c(runif(n = 250, min = 0, max = 0.6),
                        runif(n = 250, min = 0.1, max = 0.7),
                        runif(n = 250, min = 0.4, max = 1),
                        runif(n = 250, min = 0.3, max = 0.9)),
	predictions_model2 = c(runif(n = 250, min = 0.1, max = 0.55),
                        runif(n = 250, min = 0.15, max = 0.6),
                        runif(n = 250, min = 0.3, max = 0.9),
                        runif(n = 250, min = 0.25, max = 0.8)),
	evaluation_mask = c(rep(x = FALSE, times = 250),
	                    rep(x = TRUE, times = 250),
	                    rep(x = FALSE, times = 250),
	                    rep(x = TRUE, times = 250))
)

# Default parameterization, return a vector without AUC and maxTSS:
conf_and_cons &lt;- measures(observations = dataset$observations,
                          predictions = dataset$predictions_model1,
                          evaluation_mask = dataset$evaluation_mask)
print(conf_and_cons)
names(conf_and_cons)
conf_and_cons[c("CPP_eval", "DCPP")]

# Calculate AUC and maxTSS as well if package ROCR is installed:
if (requireNamespace(package = "ROCR", quietly = TRUE)) {
  conf_and_cons_and_goodness &lt;- measures(observations = dataset$observations,
                                         predictions = dataset$predictions_model1,
                                         evaluation_mask = dataset$evaluation_mask,
                                         goodness = TRUE)
}

# Calculate the measures for multiple models in a for loop:
model_IDs &lt;- as.character(1:2)
for (model_ID in model_IDs) {
  column_name &lt;- paste0("predictions_model", model_ID)
  conf_and_cons &lt;- measures(observations = dataset$observations,
                            predictions = dataset[, column_name, drop = TRUE],
                            evaluation_mask = dataset$evaluation_mask,
                            df = TRUE)
  if (model_ID == model_IDs[1]) {
    conf_and_cons_df &lt;- conf_and_cons
  } else {
    conf_and_cons_df &lt;- rbind(conf_and_cons_df, conf_and_cons)
  }
}
conf_and_cons_df

# Calculate the measures for multiple models in a lapply():
conf_and_cons_list &lt;- lapply(X = model_IDs,
                             FUN = function(model_ID) {
                               column_name &lt;- paste0("predictions_model", model_ID)
                               measures(observations = dataset$observations,
                                        predictions = dataset[, column_name, drop = TRUE],
                                        evaluation_mask = dataset$evaluation_mask,
                                        df = TRUE)
                             })
conf_and_cons_df &lt;- do.call(what = rbind,
                            args = conf_and_cons_list)
conf_and_cons_df
</code></pre>


</div>