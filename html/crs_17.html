<div class="container">

<table style="width: 100%;"><tr>
<td>krscv</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Categorical Kernel Regression Spline Cross-Validation</h2>

<h3>Description</h3>

<p><code>krscv</code> computes exhaustive cross-validation directed search for
a regression spline estimate of a one (1) dimensional dependent
variable on an <code>r</code>-dimensional vector of continuous and
nominal/ordinal (<code>factor</code>/<code>ordered</code>)
predictors.
</p>


<h3>Usage</h3>

<pre><code class="language-R">krscv(xz,
      y,
      degree.max = 10, 
      segments.max = 10,
      degree.min = 0,
      segments.min = 1, 
      restarts = 0,
      complexity = c("degree-knots","degree","knots"),
      knots = c("quantiles","uniform","auto"),
      basis = c("additive","tensor","glp","auto"),
      cv.func = c("cv.ls","cv.gcv","cv.aic"),
      degree = degree,
      segments = segments,
      tau = NULL,
      weights = NULL,
      singular.ok = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>y</code></td>
<td>

<p>continuous univariate vector
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>xz</code></td>
<td>
<p> continuous and/or nominal/ordinal
(<code>factor</code>/<code>ordered</code>) predictors </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>degree.max</code></td>
<td>
<p> the maximum degree of the B-spline basis for
each of the continuous predictors (default <code>degree.max=10</code>)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>segments.max</code></td>
<td>
<p> the maximum segments of the B-spline basis for
each of the continuous predictors (default <code>segments.max=10</code>)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>degree.min</code></td>
<td>
<p> the minimum degree of the B-spline basis for
each of the continuous predictors (default <code>degree.min=0</code>)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>segments.min</code></td>
<td>
<p> the minimum segments of the B-spline basis for
each of the continuous predictors (default <code>segments.min=1</code>)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>restarts</code></td>
<td>

<p>number of times to restart <code>optim</code> from different initial
random values (default <code>restarts=0</code>) when searching for optimal
bandwidths for the categorical predictors for each unique <code>K</code>
combination (i.e.\ <code>degree</code>/<code>segments</code>)
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>complexity</code></td>
<td>
<p>a character string (default
<code>complexity="degree-knots"</code>) indicating whether model
‘complexity’ is determined by the degree of the spline or by
the number of segments (‘knots’). This option allows the user
to use cross-validation to select either the spline degree (number
of knots held fixed) or the number of knots (spline degree held
fixed) or both the spline degree and number of knots</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>knots</code></td>
<td>
<p> a character string (default <code>knots="quantiles"</code>)
specifying where knots are to be placed. ‘quantiles’ specifies
knots placed at equally spaced quantiles (equal number of observations
lie in each segment) and ‘uniform’ specifies knots placed at
equally spaced intervals. If <code>knots="auto"</code>, the knot type will
be automatically determined by cross-validation </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>basis</code></td>
<td>
<p> a character string (default <code>basis="additive"</code>)
indicating whether the additive or tensor product B-spline basis
matrix for a multivariate polynomial spline or generalized B-spline
polynomial basis should be used. Note this can be automatically
determined by cross-validation if <code>cv=TRUE</code> and
<code>basis="auto"</code>, and is an ‘all or none’ proposition
(i.e. interaction terms for all predictors or for no predictors
given the nature of ‘tensor products’). Note also that if
there is only one predictor this defaults to <code>basis="additive"</code>
to avoid unnecessary computation as the spline bases are equivalent
in this case </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>cv.func</code></td>
<td>
<p>a character string (default <code>cv.func="cv.ls"</code>)
indicating which method to use to select smoothing
parameters. <code>cv.gcv</code> specifies generalized cross-validation
(Craven and Wahba (1979)), <code>cv.aic</code> specifies expected
Kullback-Leibler cross-validation (Hurvich, Simonoff, and Tsai
(1998)), and <code>cv.ls</code> specifies least-squares
cross-validation  </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>degree</code></td>
<td>
<p> integer/vector specifying the degree of the B-spline
basis for each dimension of the continuous <code>x</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>segments</code></td>
<td>
<p> integer/vector specifying the number of segments of
the B-spline basis for each dimension of the continuous <code>x</code>
(i.e. number of knots minus one)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>tau</code></td>
<td>

<p>if non-null a number in (0,1) denoting the quantile for which a quantile
regression spline is to be estimated rather than estimating the
conditional mean (default <code>tau=NULL</code>)
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>weights</code></td>
<td>
<p> an optional vector of weights to be used in the
fitting process.  Should be ‘NULL’ or a numeric vector.  If
non-NULL, weighted least squares is used with weights
‘weights’ (that is, minimizing ‘sum(w*e^2)’);
otherwise ordinary least squares is used.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>singular.ok</code></td>
<td>

<p>a logical value (default <code>singular.ok=FALSE</code>) that, when
<code>FALSE</code>, discards singular bases during cross-validation (a check
for ill-conditioned bases is performed).
</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p><code>krscv</code> computes exhaustive cross-validation for a regression
spline estimate of a one (1) dimensional dependent variable on an
<code>r</code>-dimensional vector of continuous and nominal/ordinal
(<code>factor</code>/<code>ordered</code>) predictors. The optimal
<code>K</code>/<code>lambda</code> combination is returned along with other
results (see below for return values). The method uses kernel
functions appropriate for categorical (ordinal/nominal) predictors
which avoids the loss in efficiency associated with sample-splitting
procedures that are typically used when faced with a mix of continuous
and nominal/ordinal (<code>factor</code>/<code>ordered</code>)
predictors.
</p>
<p>For the continuous predictors the regression spline model employs
either the additive or tensor product B-spline basis matrix for a
multivariate polynomial spline via the B-spline routines in the GNU
Scientific Library (<a href="https://www.gnu.org/software/gsl/">https://www.gnu.org/software/gsl/</a>) and the
<code>tensor.prod.model.matrix</code> function.
</p>
<p>For the discrete predictors the product kernel function is of the
‘Li-Racine’ type (see Li and Racine (2007) for details).
</p>
<p>For each unique combination of <code>degree</code> and <code>segment</code>,
numerical search for the bandwidth vector <code>lambda</code> is undertaken
using <code>optim</code> and the box-constrained <code>L-BFGS-B</code>
method (see <code>optim</code> for details). The user may restart the
<code>optim</code> algorithm as many times as desired via the
<code>restarts</code> argument. The approach ascends from <code>K=0</code> through
<code>degree.max</code>/<code>segments.max</code> and for each value of <code>K</code>
searches for the optimal bandwidths for this value of <code>K</code>. After
the most complex model has been searched then the optimal
<code>K</code>/<code>lambda</code> combination is selected. If any element of the
optimal <code>K</code> vector coincides with
<code>degree.max</code>/<code>segments.max</code> a warning is produced and the
user ought to restart their search with a larger value of
<code>degree.max</code>/<code>segments.max</code>.
</p>


<h3>Value</h3>

<p><code>krscv</code> returns a <code>crscv</code> object. Furthermore, the
function <code>summary</code> supports objects of this type. The
returned objects have the following components:
</p>
<table>
<tr style="vertical-align: top;">
<td><code>K</code></td>
<td>
<p> scalar/vector containing optimal degree(s) of spline or
number of segments </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>K.mat</code></td>
<td>
<p> vector/matrix of values of <code>K</code> evaluated during search </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>restarts</code></td>
<td>
<p> number of restarts during search, if any </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lambda</code></td>
<td>
<p> optimal bandwidths for categorical predictors </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lambda.mat</code></td>
<td>
<p> vector/matrix of optimal bandwidths for each degree of spline </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>cv.func</code></td>
<td>
<p> objective function value at optimum </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>cv.func.vec</code></td>
<td>
<p> vector of objective function values at each degree
of spline or number of segments in <code>K.mat</code></p>
</td>
</tr>
</table>
<h3>Author(s)</h3>

<p>Jeffrey S. Racine <a href="mailto:racinej@mcmaster.ca">racinej@mcmaster.ca</a>
</p>


<h3>References</h3>

<p>Craven, P. and G. Wahba (1979), “Smoothing Noisy Data With
Spline Functions,” Numerische Mathematik, 13, 377-403.
</p>
<p>Hurvich, C.M. and J.S. Simonoff and C.L. Tsai (1998),
“Smoothing Parameter Selection in Nonparametric Regression
Using an Improved Akaike Information Criterion,” Journal of the
Royal Statistical Society B, 60, 271-293.
</p>
<p>Li, Q. and J.S. Racine (2007), <em>Nonparametric Econometrics:
Theory and Practice,</em> Princeton University Press.
</p>
<p>Ma, S. and J.S. Racine and L. Yang (2015), “Spline
Regression in the Presence of Categorical Predictors,” Journal of
Applied Econometrics, Volume 30, 705-717.
</p>
<p>Ma, S. and J.S. Racine (2013), “Additive Regression
Splines with Irrelevant Categorical and Continuous Regressors,”
Statistica Sinica, Volume 23, 515-541.
</p>


<h3>See Also</h3>

<p><code>loess</code>, <code>npregbw</code>, 
</p>


<h3>Examples</h3>

<pre><code class="language-R">set.seed(42)
## Simulated data
n &lt;- 1000

x &lt;- runif(n)
z &lt;- round(runif(n,min=-0.5,max=1.5))
z.unique &lt;- uniquecombs(as.matrix(z))
ind &lt;-  attr(z.unique,"index")
ind.vals &lt;-  sort(unique(ind))
dgp &lt;- numeric(length=n)
for(i in 1:nrow(z.unique)) {
  zz &lt;- ind == ind.vals[i]
  dgp[zz] &lt;- z[zz]+cos(2*pi*x[zz])
}
y &lt;- dgp + rnorm(n,sd=.1)

xdata &lt;- data.frame(x,z=factor(z))

## Compute the optimal K and lambda, determine optimal number of knots, set
## spline degree for x to 3

cv &lt;- krscv(x=xdata,y=y,complexity="knots",degree=c(3))
summary(cv)
</code></pre>


</div>