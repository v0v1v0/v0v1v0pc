<div class="container">

<table style="width: 100%;"><tr>
<td>bss</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Best subset feature selection</h2>

<h3>Description</h3>

<p>Evaluate all combinations of predictors during model training
</p>


<h3>Usage</h3>

<pre><code class="language-R">bss(
  predictors,
  response,
  method = "rf",
  metric = ifelse(is.factor(response), "Accuracy", "RMSE"),
  maximize = ifelse(metric == "RMSE", FALSE, TRUE),
  globalval = FALSE,
  trControl = caret::trainControl(),
  tuneLength = 3,
  tuneGrid = NULL,
  seed = 100,
  verbose = TRUE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>predictors</code></td>
<td>
<p>see <code>train</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>response</code></td>
<td>
<p>see <code>train</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>method</code></td>
<td>
<p>see <code>train</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>metric</code></td>
<td>
<p>see <code>train</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>maximize</code></td>
<td>
<p>see <code>train</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>globalval</code></td>
<td>
<p>Logical. Should models be evaluated based on 'global' performance? See <code>global_validation</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>trControl</code></td>
<td>
<p>see <code>train</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>tuneLength</code></td>
<td>
<p>see <code>train</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>tuneGrid</code></td>
<td>
<p>see <code>train</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>seed</code></td>
<td>
<p>A random number</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>verbose</code></td>
<td>
<p>Logical. Should information about the progress be printed?</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>arguments passed to the classification or regression routine
(such as randomForest).</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>bss is an alternative to <code>ffs</code> and ideal if the training
set is small. Models are iteratively fitted using all different combinations
of predictor variables. Hence, 2^X models are calculated. Don't try running bss
on very large datasets because the computation time is much higher compared to
<code>ffs</code>.
</p>
<p>The internal cross validation can be run in parallel. See information
on parallel processing of carets train functions for details.
</p>


<h3>Value</h3>

<p>A list of class train. Beside of the usual train content
the object contains the vector "selectedvars" and "selectedvars_perf"
that give the best variables selected as well as their corresponding
performance. It also contains "perf_all" that gives the performance of all model runs.
</p>


<h3>Note</h3>

<p>This variable selection is particularly suitable for spatial
cross validations where variable selection
MUST be based on the performance of the model for predicting new spatial units.
Note that bss is very slow since all combinations of variables are tested.
A more time efficient alternative is the forward feature selection (<code>ffs</code>).
</p>


<h3>Author(s)</h3>

<p>Hanna Meyer
</p>


<h3>See Also</h3>

<p><code>train</code>,<code>ffs</code>,
<code>trainControl</code>,<code>CreateSpacetimeFolds</code>,
<code>nndm</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">## Not run: 
data(iris)
bssmodel &lt;- bss(iris[,1:4],iris$Species)
bssmodel$perf_all
plot(bssmodel)

## End(Not run)
</code></pre>


</div>