<div class="container">

<table style="width: 100%;"><tr>
<td>entropy</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Information entropy</h2>

<h3>Description</h3>

<p>Computes the information entropy H=sum(p*log_b(p)), also known as Shannon entropy, of a probability vector p.
</p>


<h3>Usage</h3>

<pre><code class="language-R">entropy(p, b = exp(1), normalize = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>p</code></td>
<td>
<p>vector of probabilities; typically normalized, such that sum(p)=1.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>b</code></td>
<td>
<p>base of the logarithm (default is e)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>normalize</code></td>
<td>
<p>logical flag. If TRUE (default), the vector p is automatically normalized.</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>Returns the information entropy in units that depend on b. If b=2, the units are bits; if b=exp(1), the units are nats; if b=10, the units are dits.
</p>


<h3>Author(s)</h3>

<p>Danail Obreschkow
</p>


</div>