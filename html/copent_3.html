<div class="container">

<table style="width: 100%;"><tr>
<td>copent</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2> Estimating copula entropy </h2>

<h3>Description</h3>

<p>Estimating copula entropy nonparametrically.
</p>


<h3>Usage</h3>

<pre><code class="language-R">copent(x,k=3,dt=2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p> data with each row as a sample</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>k</code></td>
<td>
<p> kth nearest neighbour, default = 3</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>dt</code></td>
<td>
<p> the type of distance between samples, 1 for Eclidean distance; 2 for Maximum distance</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>This program involves estimating copula entropy from data nonparametrically. It was proposed in Ma and Sun (2008, 2011). 
</p>
<p>The algorithm composes of two simple steps: estimating empirical copula by rank statistic using <code>construct_empirical_copula</code> and then estimating copula entropy with kNN method using <code>entknn</code> proposed in Kraskov et al (2004).
</p>
<p>The argument <b>x</b> is for the data with each row as a sample from random variables. The argument <b>k</b> and <b>dt</b> is used in the kNN method for estimating entropy. <b>k</b> is for the kth nearest neighbour (default = 3) and <b>dt</b> is for the type of distance between samples which has currently two value options (1 for Eclidean distance, and 2(default) for Maximum distance).
</p>
<p>Copula Entropy is proved to be equivalent to negative mutual information so this program can also be used to estimate multivariate mutual information.
</p>


<h3>Value</h3>

<p>The function returns <em>negative</em> value of copula entropy of data <b>x</b>.
</p>


<h3>References</h3>

 
<p>Ma, J., &amp; Sun, Z. (2011). Mutual information is copula entropy. <em>Tsinghua Science &amp; Technology</em>, <b>16</b>(1): 51-54. See also arXiv preprint arXiv:0808.0845, 2008.
</p>
<p>Kraskov, A., St\"ogbauer, H., &amp; Grassberger, P. (2004). Estimating Mutual Information. <em>Physical Review E</em>, <b>69</b>(6), 66138.
</p>


<h3>Examples</h3>

<pre><code class="language-R">
library(mnormt)
rho &lt;- 0.5
sigma &lt;- matrix(c(1,rho,rho,1),2,2)
x &lt;- rmnorm(500,c(0,0),sigma)
ce1 &lt;- copent(x,3,2)

</code></pre>


</div>