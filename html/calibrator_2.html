<div class="container">

<table style="width: 100%;"><tr>
<td>beta1hat.fun</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>beta1 estimator</h2>

<h3>Description</h3>

<p>Least squares estimator for beta1
</p>


<h3>Usage</h3>

<pre><code class="language-R">beta1hat.fun(D1, H1, y, phi)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>D1</code></td>
<td>
<p>code run points</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>H1</code></td>
<td>
<p>regressor basis funs</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>y</code></td>
<td>
<p>code outputs</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>phi</code></td>
<td>
<p>hyperparameters</p>
</td>
</tr>
</table>
<h3>Author(s)</h3>

<p>Robin K. S. Hankin</p>


<h3>References</h3>


<ul>
<li>
<p>M. C. Kennedy and A. O'Hagan 2001. <em>Bayesian
calibration of computer models</em>.  Journal of the Royal Statistical
Society B, 63(3) pp425-464
</p>
</li>
<li>
<p>M. C. Kennedy and A. O'Hagan 2001.  <em>Supplementary details on
Bayesian calibration of computer models</em>, Internal report, University
of Sheffield.  Available at
<a href="http://www.tonyohagan.co.uk/academic/ps/calsup.ps">http://www.tonyohagan.co.uk/academic/ps/calsup.ps</a>
</p>
</li>
<li>
<p>R. K. S. Hankin 2005. <em>Introducing BACCO, an R bundle for
Bayesian analysis of computer code output</em>, Journal of Statistical
Software, 14(16)
</p>
</li>
</ul>
<h3>See Also</h3>

<p><code>beta2hat.fun</code></p>


<h3>Examples</h3>

<pre><code class="language-R">data(toys)
y.toy &lt;- create.new.toy.datasets(D1=D1.toy , D2=D2.toy)$y.toy
beta1hat.fun(D1=D1.toy, H1=H1.toy, y=y.toy, phi=phi.toy)

      # now cheat: force the hyperparameters to have the correct psi1:
 phi.fix &lt;- phi.change(old.phi=phi.toy,psi1=c(1, 0.5, 1.0, 1.0, 0.5, 0.4),phi.fun=phi.fun.toy)
      # The value for psi1 is obtained by cheating and #examining the source
      # code for computer.model(); see ?phi.change 


      # Create a new toy dataset with 40 observations:
D1.big &lt;- latin.hypercube(40,5)

jj &lt;- create.new.toy.datasets(D1=D1.big , D2=D2.toy)

      # We know that the real coefficients are 4:9 because we
      # we can cheat and look at the source code for computer.model()

      # Now estimate the coefficients without cheating:

beta1hat.fun(D1=D1.big, H1=H1.toy, jj$y, phi=phi.toy)

     # Not bad!



     # We can do slightly better by cheating and using the
     # correct value for the hyperparameters:

beta1hat.fun(D1=D1.big, H1=H1.toy, jj$y,phi=phi.true.toy(phi=phi.toy))

     #marginally worse.


</code></pre>


</div>