<div class="container">

<table style="width: 100%;"><tr>
<td>Divergence based regression for compositional data</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>
Divergence based regression for compositional data
</h2>

<h3>Description</h3>

<p>Regression for compositional data based on the Kullback-Leibler the Jensen-Shannon divergence and the symmetric Kullback-Leibler divergence.
</p>


<h3>Usage</h3>

<pre><code class="language-R">kl.compreg(y, x, con = TRUE, B = 1, ncores = 1, xnew = NULL, tol = 1e-07, maxiters = 50)
js.compreg(y, x, con = TRUE, B = 1, ncores = 1, xnew = NULL)
tv.compreg(y, x, con = TRUE, B = 1, ncores = 1, xnew = NULL)
symkl.compreg(y, x, con = TRUE, B = 1, ncores = 1, xnew = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>y</code></td>
<td>

<p>A matrix with the compositional data (dependent variable). Zero values are allowed.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>

<p>The predictor variable(s), they can be either continnuous or categorical or both.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>con</code></td>
<td>

<p>If this is TRUE (default) then the constant term is estimated, otherwise the model includes no constant term.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>B</code></td>
<td>

<p>If B is greater than 1 bootstrap estimates of the standard error are returned. If B=1, no standard errors are returned.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>ncores</code></td>
<td>

<p>If ncores is 2 or more parallel computing is performed. This is to be used for the case of bootstrap. If B=1, this is not taken into consideration.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>xnew</code></td>
<td>

<p>If you have new data use it, otherwise leave it NULL.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>tol</code></td>
<td>

<p>The tolerance value to terminate the Newton-Raphson procedure.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>maxiters</code></td>
<td>

<p>The maximum number of Newton-Raphson iterations.
</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>In the kl.compreg() the Kullback-Leibler divergence is adopted as the objective function. In case of problematic
convergence the "multinom" function by the "nnet" package is employed. This will obviously be slower. The
js.compreg() uses the Jensen-Shannon divergence and the symkl.compreg() uses the symmetric Kullback-Leibler divergence.
The tv.compreg() uses the Total Variation divergence. There is no actual log-likelihood for the last three regression models.
</p>


<h3>Value</h3>

<p>A list including:
</p>
<table>
<tr style="vertical-align: top;">
<td><code>runtime</code></td>
<td>

<p>The time required by the regression.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>iters</code></td>
<td>

<p>The number of iterations required by the Newton-Raphson in the kl.compreg function.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>loglik</code></td>
<td>

<p>The log-likelihood. This is actually a quasi multinomial regression. This is bascially half the negative deviance, or
<code class="reqn">- \sum_{i=1}^ny_i\log{y_i/\hat{y}_i}</code>.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>be</code></td>
<td>

<p>The beta coefficients.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>covbe</code></td>
<td>

<p>The covariance matrix of the beta coefficients, if bootstrap is chosen, i.e. if B &gt; 1.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>est</code></td>
<td>

<p>The fitted values of xnew if xnew is not NULL.
</p>
</td>
</tr>
</table>
<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>
and Giorgos Athineou &lt;gioathineou@gmail.com&gt;.
</p>


<h3>References</h3>

<p>Murteira, Jose MR, and Joaquim JS Ramalho 2016. Regression analysis of multivariate fractional data.
Econometric Reviews 35(4): 515-552.
</p>
<p>Tsagris, Michail (2015). A novel, divergence based, regression for compositional data.
Proceedings of the 28th Panhellenic Statistics Conference, 15-18/4/2015, Athens, Greece.
https://arxiv.org/pdf/1511.07600.pdf
</p>
<p>Endres, D. M. and Schindelin, J. E. (2003). A new metric for probability distributions.
Information Theory, IEEE Transactions on 49, 1858-1860.
</p>
<p>Osterreicher, F. and Vajda, I. (2003). A new class of metric divergences on probability
spaces and its applicability in statistics. Annals of the Institute of Statistical
Mathematics 55, 639-653.
</p>


<h3>See Also</h3>

<p><code>diri.reg, ols.compreg, comp.reg
</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">library(MASS)
x &lt;- as.vector(fgl[, 1])
y &lt;- as.matrix(fgl[, 2:9])
y &lt;- y / rowSums(y)
mod1&lt;- kl.compreg(y, x, B = 1, ncores = 1)
mod2 &lt;- js.compreg(y, x, B = 1, ncores = 1)
</code></pre>


</div>