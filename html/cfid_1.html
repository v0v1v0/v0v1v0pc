<div class="container">

<table style="width: 100%;"><tr>
<td>cfid-package</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>The <code>cfid</code> package</h2>

<h3>Description</h3>

<p>Identification of Counterfactual Queries in Causal Models
</p>


<h3>Details</h3>

<p>This package provides tools necessary for identifying counterfactual
queries in causal models. Causal graphs, counterfactual variables,
and counterfactual conjunctions are defined via simple interfaces.
</p>


<h3>Counterfactuals</h3>

<p>In simple terms, counterfactual are statements involving multiple
conceptual "worlds" where the observed state of the worlds is different.
As an example, consider two variables, Y = "headache", and X = "aspirin".
A counterfactual statement could be "If I have a headache and did not take
aspirin, would I not have a headache, had I taken aspirin instead".
This statement involves two worlds: the real or "actual" world, where
aspirin was not taken, and a hypothetical world, where it was taken.
In more formal terms, this statement involves a counterfactual variable
<code class="reqn">Y_x</code> that attains two different values in two different worlds, forming
a counterfactual conjunction: <code class="reqn">y_x \wedge y'_{x'}</code>, where <code class="reqn">y</code> and
<code class="reqn">y'</code> are two different values of <code class="reqn">Y</code>, and <code class="reqn">x</code> and <code class="reqn">x'</code> are
two different values of <code class="reqn">X</code>.
</p>


<h3>Identifiability</h3>

<p>Pearl's ladder of causation consists of the associational,
interventional and counterfactual levels, with counterfactual being
the highest level. The goal of identification is to find a transformation
of a higher level query into a lower level one.
For the interventional case, this transformation is known as causal effect
identifiability, where interventional distributions are expressed in terms
of observational quantities. Tools for this type of identification are
readily available,
such as in the <code>causaleffect</code>, <code>dagitty</code>, <code>pcalg</code>, and <code>dosearch</code>
packages. Transformation from the highest counterfactual level, is more
difficult, both conceptually and computationally, since to reach the
observational level, we must first find a transformation of our
counterfactual query into a interventional query, and then transform this
yet again to observational. Also, there transformations may not always
exist, for example in the presence of latent unobserved confounders, meaning
that the queries are non-identifiable. This package deals with the first
transformation, i.e., expressing the counterfactual queries in terms of
interventional queries (and observational, when possible), as well as the
second one, transforming interventional distributions to observational
quantities.
</p>


<h3>Algorithms</h3>

<p>Identification is carried out in terms of <code class="reqn">G</code> and <code class="reqn">P_*</code> and where
<code class="reqn">G</code> is a directed acyclic graph (DAG) depicting the causal model
in question (a causal graph for short), and
<code class="reqn">P_*</code> is the set of all interventional distributions
in causal models inducing <code class="reqn">G</code>. Identification is carried
out by the ID* and IDC* algorithms by Shpitser and Pearl (2008) which aim to
convert the input counterfactual probability into an expression which can
be represented solely in terms of interventional distributions. These
algorithms are sound and complete, meaning that their output is always
correct, and in the case of a non-identifiable counterfactual, one can
always construct a counterexample, witnessing non-identifiability.
</p>


<h3>Graphs</h3>

<p>The causal graph associated with the causal model is given via a simple
text-based interface, similar to <code>dagitty</code> package syntax. Directed edges
are given as <code>X -&gt; Y</code>, and bidirected edges as <code style="white-space: pre;">⁠X &lt;-&gt; Y⁠</code>, which is a
shorthand notation for latent confounders. For more details on graph
construction, see <code>dag()</code>.
</p>


<h3>Counterfactual variables and conjunctions</h3>

<p>Counterfactual variables are defined by their name, value and the conceptual
world that they belong to. A world is defined by a unique set of actions
(interventions) via the do-operator (Pearl, 2009). We can define the two
counterfactual variables of the headache/aspirin example as follows:
</p>
<div class="sourceCode"><pre>cf(var = "Y", obs = 0, sub = c(X = 0))
cf(var = "Y", obs = 1, sub = c(X = 1))
</pre></div>
<p>Here, <code>var</code> defines the name of the variable, <code>obs</code> gives level the variable
is assigned to (not the actual value), and <code>sub</code> defines the vector of
interventions that define the counterfactual world. For more details,
see <code>counterfactual_variable()</code>. Counterfactual conjunctions on the
other hand, are simply counterfactual statements (variables) that are
observed at the same time. For more details, see
<code>counterfactual_conjunction()</code>.
</p>
<p>For complete examples of identifiable counterfactual queries, see
<code>identifiable()</code>, which is the main function of the package.
</p>


<h3>References</h3>

<p>Tikka, S. (2023).
Identifying counterfactual queries with the R package <code>cfid</code>.
<em>The R Journal</em>, <strong>15(2)</strong>:330–343.
</p>
<p>Pearl, J. (1995) Causal diagrams for empirical research. <em>Biometrika</em>,
<strong>82(4)</strong>:669–688.
</p>
<p>Pearl, J. (2009) <em>Causality: Models, Reasoning, and Inference</em>. Cambridge
University Press, 2nd edition.
</p>
<p>Shpitser, I. and Pearl, J. (2007). What counterfactuals can be tested.
In <em>Proceedings of the 23rd Conference on Uncertainty</em>
<em>in Artificial Intelligence</em>, 352–359.
</p>
<p>Shpitser, I. and Pearl, J. (2008). Complete identification
methods for the causal hierarchy. <em>Journal of Machine Learning Research</em>,
<strong>9(64)</strong>:1941–1979.
</p>
<p>Makhlouf, K., Zhioua, S. and Palamidessi, C. (2021).
Survey on causal-based machine learning fairness notions.
<em>arXiv:2010.09553</em>
</p>


</div>