<div class="container">

<table style="width: 100%;"><tr>
<td>cl_dissimilarity</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Dissimilarity Between Partitions or Hierarchies</h2>

<h3>Description</h3>

<p>Compute the dissimilarity between (ensembles) of partitions
or hierarchies.</p>


<h3>Usage</h3>

<pre><code class="language-R">cl_dissimilarity(x, y = NULL, method = "euclidean", ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>an ensemble of partitions or hierarchies and dissimilarities,
or something coercible to that (see <code>cl_ensemble</code>).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>y</code></td>
<td>
<p><code>NULL</code> (default), or as for <code>x</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>method</code></td>
<td>
<p>a character string specifying one of the built-in
methods for computing dissimilarity, or a function to be taken as
a user-defined method.  If a character string, its lower-cased
version is matched against the lower-cased names of the available
built-in methods using <code>pmatch</code>.  See <b>Details</b> for
available built-in methods.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>further arguments to be passed to methods.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>If <code>y</code> is given, its components must be of the same kind as those
of <code>x</code> (i.e., components must either all be partitions, or all be
hierarchies or dissimilarities).
</p>
<p>If all components are partitions, the following built-in methods for
measuring dissimilarity between two partitions with respective
membership matrices <code class="reqn">u</code> and <code class="reqn">v</code> (brought to a common number of
columns) are available:
</p>

<dl>
<dt><code>"euclidean"</code></dt>
<dd>
<p>the Euclidean dissimilarity of the
memberships, i.e., the square root of the minimal sum of the
squared differences of <code class="reqn">u</code> and all column permutations of
<code class="reqn">v</code>.  See Dimitriadou, Weingessel and Hornik (2002).</p>
</dd>
<dt><code>"manhattan"</code></dt>
<dd>
<p>the Manhattan dissimilarity of the
memberships, i.e., the minimal sum of the absolute differences of
<code class="reqn">u</code> and all column permutations of <code class="reqn">v</code>.</p>
</dd>
<dt><code>"comemberships"</code></dt>
<dd>
<p>the Euclidean dissimilarity of the
elements of the co-membership matrices <code class="reqn">C(u) = u u'</code> and
<code class="reqn">C(v)</code>, i.e., the square root of the sum of the squared
differences of <code class="reqn">C(u)</code> and <code class="reqn">C(v)</code>.</p>
</dd>
<dt><code>"symdiff"</code></dt>
<dd>
<p>the cardinality of the symmetric set
difference of the sets of co-classified pairs of distinct objects
in the partitions.  I.e., the number of distinct pairs of objects
in the same class in exactly one of the partitions.
(Alternatively, the cardinality of the symmetric set difference
between the (binary) equivalence relations corresponding to the
partitions.)  For soft partitions, (currently) the symmetric set
difference of the corresponding nearest hard partitions is used.</p>
</dd>
<dt><code>"Rand"</code></dt>
<dd>
<p>the Rand distance, i.e., the rate of distinct
pairs of objects in the same class in exactly one of the
partitions.  (Related to the Rand index <code class="reqn">a</code> via the linear
transformation <code class="reqn">d = (1 - a) / 2</code>.)  For soft partitions,
(currently) the Rand distance of the corresponding nearest hard
partitions is used.</p>
</dd>
<dt><code>"GV1"</code></dt>
<dd>
<p>the square root of the dissimilarity
<code class="reqn">\Delta_1</code> used for the first model in Gordon and
Vichi (2001), i.e., the square root of the minimal sum of the
squared differences of the <em>matched</em> non-zero columns of
<code class="reqn">u</code> and <code class="reqn">v</code>.</p>
</dd>
<dt><code>"BA/<var>d</var>"</code></dt>
<dd>
<p>distance measures for hard partitions
discussed in Boorman and Arabie (1972), with <var>d</var> one of
‘<span class="samp">⁠A⁠</span>’, ‘<span class="samp">⁠C⁠</span>’, ‘<span class="samp">⁠D⁠</span>’, or ‘<span class="samp">⁠E⁠</span>’.  For soft partitions,
the distances of the corresponding nearest hard partitions are
used.
</p>
<p><code>"BA/A"</code> is the minimum number of single element moves (move
from one class to another or a new one) needed to transform one
partition into the other.  Introduced in Rubin (1967).
</p>
<p><code>"BA/C"</code> is the minimum number of lattice moves for
transforming one partition into the other, where partitions are
said to be connected by a lattice move if one is <em>just</em> finer
than the other (i.e., there is no other partition between them) in
the partition lattice (see <code>cl_meet</code>).  Equivalently,
with <code class="reqn">z</code> the join of <code>x</code> and <code>y</code> and <code class="reqn">S</code> giving
the number of classes, this can be written as <code class="reqn">S(x) + S(y) - 2
	S(z)</code>.  Attributed to David Pavy.
</p>
<p><code>"BA/D"</code> is the “pair-bonds” distance, which can be
defined as <code class="reqn">S(x) + S(y) - 2 S(z)</code>, with <code class="reqn">z</code> the meet of
<code>x</code> and <code>y</code> and <code class="reqn">S</code> the <em>supervaluation</em> (i.e.,
non-decreasing with respect to the partial order on the partition
lattice) function <code class="reqn">\sum_i (n_i (n_i - 1)) / (n (n - 1))</code>,
where the <code class="reqn">n_i</code> are the numbers of objects in the respective
classes of the partition (such that <code class="reqn">n_i (n_i - 1) / 2</code> are the
numbers of pair bonds in the classes), and <code class="reqn">n</code> the total
number of objects.
</p>
<p><code>"BA/E"</code> is the normalized information distance, defined as
<code class="reqn">1 - I / H</code>, where <code class="reqn">I</code> is the average mutual information
between the partitions, and <code class="reqn">H</code> is the average entropy of the
meet <code class="reqn">z</code> of the partitions.  Introduced in Rajski (1961).
</p>
<p>(Boorman and Arabie also discuss a distance measure (<code class="reqn">B</code>)
based on the minimum number of set moves needed to transform one
partition into the other, which, differently from the <code class="reqn">A</code> and
<code class="reqn">C</code> distance measures is hard to compute (Day, 1981) and
(currently) not provided.)</p>
</dd>
<dt><code>"VI"</code></dt>
<dd>
<p>Variation of Information, see Meila (2003).  If
<code>...</code> has an argument named <code>weights</code>, it is taken to
specify case weights.</p>
</dd>
<dt><code>"Mallows"</code></dt>
<dd>
<p>the Mallows-type distance by Zhou, Li and
Zha (2005), which is related to the Monge-Kantorovich mass
transfer problem, and given as the <code class="reqn">p</code>-th root of the minimal
value of the transportation problem <code class="reqn">\sum w_{jk} \sum_i
      |u_{ij} - v_{ik}| ^ p</code> with constraints <code class="reqn">w_{jk} \ge 0</code>,
<code class="reqn">\sum_j w_{jk} = \alpha_j</code>, <code class="reqn">\sum_k w_{jk} = \beta_k</code>,
where <code class="reqn">\sum_j \alpha_j = \sum_k \beta_k</code>.  The parameters
<code class="reqn">p</code>, <code class="reqn">\alpha</code> and <code class="reqn">\beta</code> all default to one (in this
case, the Mallows distance coincides with the Manhattan
dissimilarity), and can be specified via additional arguments
named <code>p</code>, <code>alpha</code>, and <code>beta</code>, respectively.</p>
</dd>
<dt><code>"CSSD"</code></dt>
<dd>
<p>the Cluster Similarity Sensitive Distance of
Zhou, Li and Zha (2005), which is given as the minimal value of
<code class="reqn">\sum_{k,l} (1 - 2 w_{kl} / (\alpha_k + \beta_l)) L_{kl}</code>,
where <code class="reqn">L_{kl} = \sum_i u_{ik} v_{il} d(p_{x;k}, p_{y;l})</code> with
<code class="reqn">p_{x;k}</code> and <code class="reqn">p_{y;l}</code> the prototype of the <code class="reqn">k</code>-th
class of <code>x</code> and the <code class="reqn">l</code>-th class of <code>y</code>,
respectively, <code class="reqn">d</code> is the distance between these, and the
<code class="reqn">w_{kl}</code> as for Mallows distance.  If prototypes are matrices,
the Euclidean distance between these is used as default.  Using
the additional argument <code>L</code>, one can give a matrix of
<code class="reqn">L_{kl}</code> values, or the function <code class="reqn">d</code>.  Parameters
<code class="reqn">\alpha</code> and <code class="reqn">\beta</code> all default to one, and can be
specified via additional arguments named <code>alpha</code> and
<code>beta</code>, respectively.</p>
</dd>
</dl>
<p>For hard partitions, both Manhattan and squared Euclidean
dissimilarity give twice the <em>transfer distance</em> (Charon et al.,
2005), which is the minimum number of objects that must be removed so
that the implied partitions (restrictions to the remaining objects)
are identical.  This is also known as the <em><code class="reqn">R</code>-metric</em> in Day
(1981), i.e., the number of augmentations and removals of single
objects needed to transform one partition into the other, and the
<em>partition-distance</em> in Gusfield (2002), and equals twice the
number of single element moves distance of Boorman and Arabie.
</p>
<p>For hard partitions, the pair-bonds (Boorman-Arabie <code class="reqn">D</code>) distance
is identical to the Rand distance, and can also be written as the
Manhattan distance between the co-membership matrices corresponding to
the partitions, or equivalently, their symdiff distance, normalized by
<code class="reqn">n (n - 1)</code>.
</p>
<p>If all components are hierarchies, available built-in methods for
measuring dissimilarity between two hierarchies with respective
ultrametrics <code class="reqn">u</code> and <code class="reqn">v</code> are as follows.
</p>

<dl>
<dt><code>"euclidean"</code></dt>
<dd>
<p>the Euclidean dissimilarity of the
ultrametrics (i.e., the square root of the sum of the squared
differences of <code class="reqn">u</code> and <code class="reqn">v</code>).</p>
</dd>
<dt><code>"manhattan"</code></dt>
<dd>
<p>the Manhattan dissimilarity of the
ultrametrics (i.e., the sum of the absolute differences of <code class="reqn">u</code>
and <code class="reqn">v</code>).</p>
</dd>
<dt><code>"cophenetic"</code></dt>
<dd>
<p><code class="reqn">1 - c^2</code>, where <code class="reqn">c</code> is the
cophenetic correlation coefficient (i.e., the product-moment
correlation of the ultrametrics).</p>
</dd>
<dt><code>"gamma"</code></dt>
<dd>
<p>the rate of inversions between the
ultrametrics (i.e., the rate of pairs <code class="reqn">(i,j)</code> and <code class="reqn">(k,l)</code>
for which <code class="reqn">u_{ij} &lt; u_{kl}</code> and <code class="reqn">v_{ij} &gt; v_{kl}</code>).</p>
</dd>
<dt><code>"symdiff"</code></dt>
<dd>
<p>the cardinality of the symmetric set
difference of the sets of classes (hierarchies in the strict
sense) induced by the dendrograms.  I.e., the number of sets of
objects obtained by a split in exactly one of the hierarchies.</p>
</dd>
<dt><code>"Chebyshev"</code></dt>
<dd>
<p>the Chebyshev (maximal) dissimilarity of
the ultrametrics (i.e., the maximum of the absolute differences of
<code class="reqn">u</code> and <code class="reqn">v</code>).</p>
</dd>
<dt><code>"Lyapunov"</code></dt>
<dd>
<p>the logarithm of the product of the
maximal and minimal ratios of the ultrametrics.  This is also
known as the “Hilbert projective metric” on the cone
represented by the ultrametrics (e.g., Jardine &amp; Sibson (1971),
page 107), and only defined for <em>strict</em> ultrametrics (which
are strictly positive for distinct objects).</p>
</dd>
<dt><code>"BO"</code></dt>
<dd>
<p>the <code class="reqn">m_\delta</code> family of tree metrics by
Boorman and Olivier (1973), which are of the form <code class="reqn">m_\delta =
	\int_0^\infty \delta(p(h), q(h)) dh</code>, where <code class="reqn">p(h)</code> and
<code class="reqn">q(h)</code> are the hard partitions obtaining by cutting the trees
(dendrograms) at height <code class="reqn">h</code>, and <code class="reqn">\delta</code> is a suitably
dissimilarity measure for partitions.  In particular, when taking
<code class="reqn">\delta</code> as symdiff or Rand dissimilarity, <code class="reqn">m_\delta</code> is
the Manhattan dissimilarity of the hierarchies.
</p>
<p>If <code>...</code> has an argument named <code>delta</code> it is taken to
specify the partition dissimilarity <code class="reqn">\delta</code> to be employed.</p>
</dd>
<dt><code>"spectral"</code></dt>
<dd>
<p>the spectral norm (2-norm) of the
differences of the ultrametrics, suggested in Mérigot, Durbec, and
Gaertner (2010).</p>
</dd>
</dl>
<p>The measures based on ultrametrics also allow computing dissimilarity
with “raw” dissimilarities on the underlying objects (R objects
inheriting from class <code>"dist"</code>).
</p>
<p>If a user-defined dissimilarity method is to be employed, it must be a
function taking two clusterings as its arguments.
</p>
<p>Symmetric dissimilarity objects of class <code>"cl_dissimilarity"</code> are
implemented as symmetric proximity objects with self-proximities
identical to zero, and inherit from class <code>"cl_proximity"</code>.  They
can be coerced to dense square matrices using <code>as.matrix</code>.  It
is possible to use 2-index matrix-style subscripting for such objects;
unless this uses identical row and column indices, this results in a
(non-symmetric dissimilarity) object of class
<code>"cl_cross_dissimilarity"</code>.
</p>
<p>Symmetric dissimilarity objects also inherit from class
<code>"dist"</code> (although they currently do not “strictly”
extend this class), thus making it possible to use them directly for
clustering algorithms based on dissimilarity matrices of this class,
see the examples.
</p>


<h3>Value</h3>

<p>If <code>y</code> is <code>NULL</code>, an object of class
<code>"cl_dissimilarity"</code> containing the dissimilarities between all
pairs of components of <code>x</code>.  Otherwise, an object of class
<code>"cl_cross_dissimilarity"</code> with the dissimilarities between the
components of <code>x</code> and the components of <code>y</code>.
</p>


<h3>References</h3>

<p>S. A. Boorman and P. Arabie (1972).
Structural measures and the method of sorting.
In R. N. Shepard, A. K. Romney, &amp; S. B. Nerlove (eds.),
<em>Multidimensional Scaling: Theory and Applications in the
Behavioral Sciences, 1: Theory</em> (pages 225–249).
New York: Seminar Press.
</p>
<p>S. A. Boorman and D. C. Olivier (1973).
Metrics on spaces of finite trees.
<em>Journal of Mathematical Psychology</em>, <b>10</b>, 26–59.
<a href="https://doi.org/10.1016/0022-2496%2873%2990003-5">doi:10.1016/0022-2496(73)90003-5</a>.
</p>
<p>I. Charon, L. Denoeud, A. Guénoche and O. Hudry (2006).
<em>Maximum Transfer Distance Between Partitions</em>.
<em>Journal of Classification</em>, <b>23</b>, 103–121.
<a href="https://doi.org/10.1007/s00357-006-0006-2">doi:10.1007/s00357-006-0006-2</a>.
</p>
<p>W. E. H. Day (1981).
The complexity of computing metric distances between partitions.
<em>Mathematical Social Sciences</em>, <b>1</b>, 269–287.
<a href="https://doi.org/10.1016/0165-4896%2881%2990042-1">doi:10.1016/0165-4896(81)90042-1</a>.
</p>
<p>E. Dimitriadou, A. Weingessel and K. Hornik (2002).
A combination scheme for fuzzy clustering.
<em>International Journal of Pattern Recognition and Artificial
Intelligence</em>, <b>16</b>, 901–912. <br><a href="https://doi.org/10.1142/S0218001402002052">doi:10.1142/S0218001402002052</a>.
</p>
<p>A. D. Gordon and M. Vichi (2001).
Fuzzy partition models for fitting a set of partitions.
<em>Psychometrika</em>, <b>66</b>, 229–248.
<a href="https://doi.org/10.1007/BF02294837">doi:10.1007/BF02294837</a>.
</p>
<p>D. Gusfield (2002).
Partition-distance: A problem and class of perfect graphs arising in
clustering.
<em>Information Processing Letters</em>, <b>82</b>, 159–164.
<a href="https://doi.org/10.1016/S0020-0190%2801%2900263-0">doi:10.1016/S0020-0190(01)00263-0</a>.
</p>
<p>N. Jardine and E. Sibson (1971).
<em>Mathematical Taxonomy</em>.
London: Wiley.
</p>
<p>M. Meila (2003).
Comparing clusterings by the variation of information.
In B. Schölkopf and M. K. Warmuth (eds.), <em>Learning Theory and
Kernel Machines</em>, pages 173–187.
Springer-Verlag: Lecture Notes in Computer Science 2777.
</p>
<p>B. Mérigot, J.-P. Durbec and J.-C. Gaertner (2010).
On goodness-of-fit measure for dendrogram-based analyses.
<em>Ecology</em>, <b>91</b>, 1850—-1859.
<a href="https://doi.org/10.1890/09-1387.1">doi:10.1890/09-1387.1</a>.
</p>
<p>C. Rajski (1961).
A metric space of discrete probability distributions,
<em>Information and Control</em>, <b>4</b>, 371–377.
<a href="https://doi.org/10.1016/S0019-9958%2861%2980055-7">doi:10.1016/S0019-9958(61)80055-7</a>.
</p>
<p>J. Rubin (1967).
Optimal classification into groups: An approach for solving the
taxonomy problem.
<em>Journal of Theoretical Biology</em>, <b>15</b>, 103–144.
<a href="https://doi.org/10.1016/0022-5193%2867%2990046-X">doi:10.1016/0022-5193(67)90046-X</a>.
</p>
<p>D. Zhou, J. Li and H. Zha (2005).
A new Mallows distance based metric for comparing clusterings.
In <em>Proceedings of the 22nd international Conference on Machine
Learning</em> (Bonn, Germany, August 07–11, 2005), pages 1028–1035.
ICML '05, volume 119.
ACM Press, New York, NY.
<a href="https://doi.org/10.1145/1102351.1102481">doi:10.1145/1102351.1102481</a>.
</p>


<h3>See Also</h3>

<p><code>cl_agreement</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">## An ensemble of partitions.
data("CKME")
pens &lt;- CKME[1 : 30]
diss &lt;- cl_dissimilarity(pens)
summary(c(diss))
cl_dissimilarity(pens[1:5], pens[6:7])
## Equivalently, using subscripting.
diss[1:5, 6:7]
## Can use the dissimilarities for "secondary" clustering
## (e.g. obtaining hierarchies of partitions):
hc &lt;- hclust(diss)
plot(hc)

## Example from Boorman and Arabie (1972).
P1 &lt;- as.cl_partition(c(1, 2, 2, 2, 3, 3, 2, 2))
P2 &lt;- as.cl_partition(c(1, 1, 2, 2, 3, 3, 4, 4))
cl_dissimilarity(P1, P2, "BA/A")
cl_dissimilarity(P1, P2, "BA/C")

## Hierarchical clustering.
d &lt;- dist(USArrests)
x &lt;- hclust(d)
cl_dissimilarity(x, d, "cophenetic")
cl_dissimilarity(x, d, "gamma")
</code></pre>


</div>