<div class="container">

<table style="width: 100%;"><tr>
<td>chattr_defaults</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Default arguments to use when making requests to the LLM</h2>

<h3>Description</h3>

<p>Default arguments to use when making requests to the LLM
</p>


<h3>Usage</h3>

<pre><code class="language-R">chattr_defaults(
  type = "default",
  prompt = NULL,
  max_data_files = NULL,
  max_data_frames = NULL,
  include_doc_contents = NULL,
  include_history = NULL,
  provider = NULL,
  path = NULL,
  model = NULL,
  model_arguments = NULL,
  system_msg = NULL,
  yaml_file = "chattr.yml",
  force = FALSE,
  label = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>type</code></td>
<td>
<p>Entry point to interact with the model. Accepted values:
'notebook', chat'</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>prompt</code></td>
<td>
<p>Request to send to LLM. Defaults to NULL</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>max_data_files</code></td>
<td>
<p>Sets the maximum number of data files to send to the
model. It defaults to 20. To send all, set to NULL</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>max_data_frames</code></td>
<td>
<p>Sets the maximum number of data frames loaded in the
current R session to send to the model. It defaults to 20. To send all,
set to NULL</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>include_doc_contents</code></td>
<td>
<p>Send the current code in the document</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>include_history</code></td>
<td>
<p>Indicates whether to include the chat history when
every time a new prompt is submitted</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>provider</code></td>
<td>
<p>The name of the provider of the LLM. Today, only "openai" is
is available</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>path</code></td>
<td>
<p>The location of the model. It could be an URL or a file path.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>model</code></td>
<td>
<p>The name or path to the model to use.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>model_arguments</code></td>
<td>
<p>Additional arguments to pass to the model as part of
the request, it requires a list. Examples of arguments: temperature, top_p,
max_tokens</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>system_msg</code></td>
<td>
<p>For OpenAI GPT 3.5 or above, the system message to send as
part of the request</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>yaml_file</code></td>
<td>
<p>The path to a valid <code>config</code> YAML file that contains the
defaults to use in a session</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>force</code></td>
<td>
<p>Re-process the base and any work space level file defaults</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>label</code></td>
<td>
<p>Label to display in the Shiny app, and other locations</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>Additional model arguments that are not standard for all
models/backends</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>The idea is that because we will use addin shortcut to execute the
request, all of the other arguments can be controlled via this function. By
default, it will try to load defaults from a <code>config</code> YAML file, if none are
found, then the defaults for GPT 3.5 will be used. The defaults can be
modified by calling this function, even after the interactive session has
started.
</p>


<h3>Value</h3>

<p>An 'ch_model' object that contains the current defaults that will be
used to communicate with the LLM.
</p>


</div>