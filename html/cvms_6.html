<div class="container">

<table style="width: 100%;"><tr>
<td>baseline_binomial</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Create baseline evaluations for binary classification</h2>

<h3>Description</h3>

<p><a href="https://lifecycle.r-lib.org/articles/stages.html#maturing"><img src="../help/figures/lifecycle-maturing.svg" alt="[Maturing]"></a>
</p>
<p>Create a baseline evaluation of a test set.
</p>
<p>In modelling, a <em>baseline</em> is a result that
is meaningful to compare the results from our models to. For instance, in
classification, we usually want our results to be better than <em>random guessing</em>.
E.g. if we have three classes, we can expect an accuracy of <code>33.33%</code>, as for every
observation we have <code>1/3</code> chance of guessing the correct class. So our model should achieve
a higher accuracy than <code>33.33%</code> before it is more useful to us than guessing.
</p>
<p>While this expected value is often fairly straightforward to find analytically, it
only represents what we can expect on average. In reality, it's possible to get far better
results than that by guessing.
<strong><code>baseline_binomial()</code></strong>
finds the range of likely values by evaluating multiple sets
of random predictions and summarizing them with a set of useful descriptors. Additionally,
it evaluates a set of all <code>0</code> predictions and
a set of all <code>1</code> predictions.
</p>


<h3>Usage</h3>

<pre><code class="language-R">baseline_binomial(
  test_data,
  dependent_col,
  n = 100,
  metrics = list(),
  positive = 2,
  cutoff = 0.5,
  parallel = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>test_data</code></td>
<td>
<p><code>data.frame</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>dependent_col</code></td>
<td>
<p>Name of dependent variable in the supplied test and training sets.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>n</code></td>
<td>
<p>The number of sets of random predictions to evaluate. (Default is <code>100</code>)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>metrics</code></td>
<td>
<p><code>list</code> for enabling/disabling metrics.
</p>
<p>E.g. <code>list("F1" = FALSE)</code> would remove <code>F1</code> from the results,
and <code>list("Accuracy" = TRUE)</code> would add the regular <code>Accuracy</code> metric
to the results.
Default values (<code>TRUE</code>/<code>FALSE</code>) will be used for the remaining available metrics.
</p>
<p>You can enable/disable all metrics at once by including
<code>"all" = TRUE/FALSE</code> in the <code>list</code>. This is done prior to enabling/disabling
individual metrics, why f.i. <code>list("all" = FALSE, "Accuracy" = TRUE)</code>
would return only the <code>Accuracy</code> metric.
</p>
<p>The <code>list</code> can be created with
<code>binomial_metrics()</code>.
</p>
<p>Also accepts the string <code>"all"</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>positive</code></td>
<td>
<p>Level from dependent variable to predict.
Either as character (<em>preferable</em>) or level index (<code>1</code> or <code>2</code> - alphabetically).
</p>
<p>E.g. if we have the levels <code>"cat"</code> and <code>"dog"</code> and we want <code>"dog"</code> to be the positive class,
we can either provide <code>"dog"</code> or <code>2</code>, as alphabetically, <code>"dog"</code> comes after <code>"cat"</code>.
</p>
<p><strong>Note:</strong> For <em>reproducibility</em>, it's preferable to <strong>specify the name directly</strong>, as
different <code>locales</code> may sort the levels differently.
</p>
<p>Used when calculating confusion matrix metrics and creating <code>ROC</code> curves.
</p>
<p>N.B. Only affects evaluation metrics, not the returned predictions.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>cutoff</code></td>
<td>
<p>Threshold for predicted classes. (Numeric)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>parallel</code></td>
<td>
<p>Whether to run the <code>`n`</code> evaluations in parallel. (Logical)
</p>
<p>Remember to register a parallel backend first.
E.g. with <code>doParallel::registerDoParallel</code>.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Packages used:
</p>
<p><code>ROC</code> and <code>AUC</code>: <code>pROC::roc</code>
</p>


<h3>Value</h3>

<p><code>list</code> containing:
</p>

<ol>
<li>
<p> a <code>tibble</code> with summarized results (called <code>summarized_metrics</code>)
</p>
</li>
<li>
<p> a <code>tibble</code> with random evaluations (<code>random_evaluations</code>)
</p>
</li>
</ol>
<p>....................................................................
</p>
<p>Based on the generated test set predictions,
a confusion matrix and <code>ROC</code> curve are used to get the following:
</p>
<p><code>ROC</code>:
</p>
<p><strong><code>AUC</code></strong>, <strong><code>Lower CI</code></strong>, and <strong><code>Upper CI</code></strong>
</p>
<p>Note, that the <code>ROC</code> curve is only computed when <code>AUC</code> is enabled.
</p>
<p><code>Confusion Matrix</code>:
</p>
<p><strong><code>Balanced Accuracy</code></strong>,
<strong><code>Accuracy</code></strong>,
<strong><code>F1</code></strong>,
<strong><code>Sensitivity</code></strong>, <strong><code>Specificity</code></strong>,
<strong><code>Positive Predictive Value</code></strong>,
<strong><code>Negative Predictive Value</code></strong>,
<strong><code>Kappa</code></strong>,
<strong><code>Detection Rate</code></strong>,
<strong><code>Detection Prevalence</code></strong>,
<strong><code>Prevalence</code></strong>, and
<strong><code>MCC</code></strong> (Matthews correlation coefficient).
</p>
<p>....................................................................
</p>
<p>The <strong>Summarized Results</strong> <code>tibble</code> contains:
</p>
<p>The <strong>Measure</strong> column indicates the statistical descriptor used on the evaluations.
The row where <code>Measure == All_0</code> is the evaluation when all predictions are <code>0</code>.
The row where <code>Measure == All_1</code> is the evaluation when all predictions are <code>1</code>.
</p>
<p>The <strong>aggregated metrics</strong>.
</p>
<p>....................................................................
</p>
<p>The <strong>Random Evaluations</strong> <code>tibble</code> contains:
</p>
<p>The <strong>non-aggregated metrics</strong>.
</p>
<p>A nested <code>tibble</code> with the <strong>predictions</strong> and targets.
</p>
<p>A <code>list</code> of <strong>ROC</strong> curve objects (if computed).
</p>
<p>A nested <code>tibble</code> with the <strong>confusion matrix</strong>.
The <code>Pos_</code> columns tells you whether a row is a
True Positive (<code>TP</code>), True Negative (<code>TN</code>), False Positive (<code>FP</code>),
or False Negative (<code>FN</code>), depending on which level is the "positive" class.
I.e. the level you wish to predict.
</p>
<p>A nested <strong>Process</strong> information object with information
about the evaluation.
</p>
<p>Name of <strong>dependent</strong> variable.
</p>


<h3>Author(s)</h3>

<p>Ludvig Renbo Olsen, <a href="mailto:r-pkgs@ludvigolsen.dk">r-pkgs@ludvigolsen.dk</a>
</p>


<h3>See Also</h3>

<p>Other baseline functions: 
<code>baseline()</code>,
<code>baseline_gaussian()</code>,
<code>baseline_multinomial()</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">
# Attach packages
library(cvms)
library(groupdata2) # partition()
library(dplyr) # %&gt;% arrange()

# Data is part of cvms
data &lt;- participant.scores

# Set seed for reproducibility
set.seed(1)

# Partition data
partitions &lt;- partition(data, p = 0.7, list_out = TRUE)
train_set &lt;- partitions[[1]]
test_set &lt;- partitions[[2]]

# Create baseline evaluations
# Note: usually n=100 is a good setting

baseline_binomial(
  test_data = test_set,
  dependent_col = "diagnosis",
  n = 2
)

# Parallelize evaluations

# Attach doParallel and register four cores
# Uncomment:
# library(doParallel)
# registerDoParallel(4)

# Make sure to uncomment the parallel argument
baseline_binomial(
  test_data = test_set,
  dependent_col = "diagnosis",
  n = 4
  #, parallel = TRUE  # Uncomment
)

</code></pre>


</div>