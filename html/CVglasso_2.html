<div class="container">

<table style="width: 100%;"><tr>
<td>CVglasso</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Penalized precision matrix estimation</h2>

<h3>Description</h3>

<p>Penalized precision matrix estimation using the graphical lasso (glasso) algorithm.
Consider the case where <code class="reqn">X_{1}, ..., X_{n}</code> are iid <code class="reqn">N_{p}(\mu,
\Sigma)</code> and we are tasked with estimating the precision matrix,
denoted <code class="reqn">\Omega \equiv \Sigma^{-1}</code>. This function solves the
following optimization problem:
</p>

<dl>
<dt>Objective:</dt>
<dd>
<p><code class="reqn">\hat{\Omega}_{\lambda} = \arg\min_{\Omega \in S_{+}^{p}}
\left\{ Tr\left(S\Omega\right) - \log \det\left(\Omega \right) +
\lambda \left\| \Omega \right\|_{1} \right\}</code></p>
</dd>
</dl>
<p>where <code class="reqn">\lambda &gt; 0</code> and we define
<code class="reqn">\left\|A \right\|_{1} = \sum_{i, j} \left| A_{ij} \right|</code>.
</p>


<h3>Usage</h3>

<pre><code class="language-R">CVglasso(X = NULL, S = NULL, nlam = 10, lam.min.ratio = 0.01,
  lam = NULL, diagonal = FALSE, path = FALSE, tol = 1e-04,
  maxit = 10000, adjmaxit = NULL, K = 5, crit.cv = c("loglik", "AIC",
  "BIC"), start = c("warm", "cold"), cores = 1, trace = c("progress",
  "print", "none"), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>X</code></td>
<td>
<p>option to provide a nxp data matrix. Each row corresponds to a single observation and each column contains n observations of a single feature/variable.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>S</code></td>
<td>
<p>option to provide a pxp sample covariance matrix (denominator n). If argument is <code>NULL</code> and <code>X</code> is provided instead then <code>S</code> will be computed automatically.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nlam</code></td>
<td>
<p>number of <code>lam</code> tuning parameters for penalty term generated from <code>lam.min.ratio</code> and <code>lam.max</code> (automatically generated). Defaults to 10.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lam.min.ratio</code></td>
<td>
<p>smallest <code>lam</code> value provided as a fraction of <code>lam.max</code>. The function will automatically generate <code>nlam</code> tuning parameters from <code>lam.min.ratio*lam.max</code> to <code>lam.max</code> in log10 scale. <code>lam.max</code> is calculated to be the smallest <code>lam</code> such that all off-diagonal entries in <code>Omega</code> are equal to zero (<code>alpha</code> = 1). Defaults to 1e-2.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lam</code></td>
<td>
<p>option to provide positive tuning parameters for penalty term. This will cause <code>nlam</code> and <code>lam.min.ratio</code> to be disregarded. If a vector of parameters is provided, they should be in increasing order. Defaults to NULL.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>diagonal</code></td>
<td>
<p>option to penalize the diagonal elements of the estimated precision matrix (<code class="reqn">\Omega</code>). Defaults to <code>FALSE</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>path</code></td>
<td>
<p>option to return the regularization path. This option should be used with extreme care if the dimension is large. If set to TRUE, cores must be set to 1 and errors and optimal tuning parameters will based on the full sample. Defaults to FALSE.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>tol</code></td>
<td>
<p>convergence tolerance. Iterations will stop when the average absolute difference in parameter estimates in less than <code>tol</code> times multiple. Defaults to 1e-4.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>maxit</code></td>
<td>
<p>maximum number of iterations. Defaults to 1e4.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>adjmaxit</code></td>
<td>
<p>adjusted maximum number of iterations. During cross validation this option allows the user to adjust the maximum number of iterations after the first <code>lam</code> tuning parameter has converged. This option is intended to be paired with <code>warm</code> starts and allows for 'one-step' estimators. Defaults to NULL.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>K</code></td>
<td>
<p>specify the number of folds for cross validation.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>crit.cv</code></td>
<td>
<p>cross validation criterion (<code>loglik</code>, <code>AIC</code>, or <code>BIC</code>). Defaults to <code>loglik</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>start</code></td>
<td>
<p>specify <code>warm</code> or <code>cold</code> start for cross validation. Default is <code>warm</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>cores</code></td>
<td>
<p>option to run CV in parallel. Defaults to <code>cores = 1</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>trace</code></td>
<td>
<p>option to display progress of CV. Choose one of <code>progress</code> to print a progress bar, <code>print</code> to print completed tuning parameters, or <code>none</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>additional arguments to pass to <code>glasso</code>.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>For details on the implementation of the 'glasso' function, see Tibshirani's website.
<a href="http://statweb.stanford.edu/~tibs/glasso/">http://statweb.stanford.edu/~tibs/glasso/</a>.
</p>


<h3>Value</h3>

<p>returns class object <code>CVglasso</code> which includes:
</p>
<table>
<tr style="vertical-align: top;">
<td><code>Call</code></td>
<td>
<p>function call.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Iterations</code></td>
<td>
<p>number of iterations</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Tuning</code></td>
<td>
<p>optimal tuning parameters (lam and alpha).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Lambdas</code></td>
<td>
<p>grid of lambda values for CV.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>maxit</code></td>
<td>
<p>maximum number of iterations for outer (blockwise) loop.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Omega</code></td>
<td>
<p>estimated penalized precision matrix.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Sigma</code></td>
<td>
<p>estimated covariance matrix from the penalized precision matrix (inverse of Omega).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Path</code></td>
<td>
<p>array containing the solution path. Solutions will be ordered by ascending lambda values.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>MIN.error</code></td>
<td>
<p>minimum average cross validation error (cv.crit) for optimal parameters.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>AVG.error</code></td>
<td>
<p>average cross validation error (cv.crit) across all folds.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>CV.error</code></td>
<td>
<p>cross validation errors (cv.crit).</p>
</td>
</tr>
</table>
<h3>Author(s)</h3>

<p>Matt Galloway <a href="mailto:gall0441@umn.edu">gall0441@umn.edu</a>
</p>


<h3>References</h3>


<ul>
<li>
<p> Friedman, Jerome, Trevor Hastie, and Robert Tibshirani. 'Sparse inverse covariance estimation with the graphical lasso.' <em>Biostatistics</em> 9.3 (2008): 432-441.
</p>
</li>
<li>
<p> Banerjee, Onureen, Ghauoui, Laurent El, and d'Aspremont, Alexandre. 2008. 'Model Selection through Sparse Maximum Likelihood Estimation for Multivariate Gaussian or Binary Data.' <em>Journal of Machine Learning Research</em> 9: 485-516.
</p>
</li>
<li>
<p> Tibshirani, Robert. 1996. 'Regression Shrinkage and Selection via the Lasso.' <em>Journal of the Royal Statistical Society. Series B (Methodological)</em>. JSTOR: 267-288.
</p>
</li>
<li>
<p> Meinshausen, Nicolai and Buhlmann, Peter. 2006. 'High-Dimensional Graphs and Variable Selection with the Lasso.' <em>The Annals of Statistics</em>. JSTOR: 1436-1462.
</p>
</li>
<li>
<p> Witten, Daniela M, Friedman, Jerome H, and Simon, Noah. 2011. 'New Insights and Faster computations for the Graphical Lasso.' <em>Journal of Computation and Graphical Statistics</em>. Taylor and Francis: 892-900.
</p>
</li>
<li>
<p> Tibshirani, Robert, Bien, Jacob, Friedman, Jerome, Hastie, Trevor, Simon, Noah, Jonathan, Taylor, and Tibshirani, Ryan J. 'Strong Rules for Discarding Predictors in Lasso-Type Problems.' <em>Journal of the Royal Statistical Society: Series B (Statistical Methodology)</em>. Wiley Online Library 74 (2): 245-266.
</p>
</li>
<li>
<p> Ghaoui, Laurent El, Viallon, Vivian, and Rabbani, Tarek. 2010. 'Safe Feature Elimination for the Lasso and Sparse Supervised Learning Problems.' <em>arXiv preprint arXiv: 1009.4219</em>.
</p>
</li>
<li>
<p> Osborne, Michael R, Presnell, Brett, and Turlach, Berwin A. 'On the Lasso and its Dual.' <em>Journal of Computational and Graphical Statistics</em>. Taylor and Francis 9 (2): 319-337.
</p>
</li>
<li>
<p> Rothman, Adam. 2017. 'STAT 8931 notes on an algorithm to compute the Lasso-penalized Gausssian likelihood precision matrix estimator.' 
</p>
</li>
</ul>
<h3>See Also</h3>

<p><code>plot.CVglasso</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R"># generate data from a sparse matrix
# first compute covariance matrix
S = matrix(0.7, nrow = 5, ncol = 5)
for (i in 1:5){
 for (j in 1:5){
   S[i, j] = S[i, j]^abs(i - j)
 }
}

# generate 100 x 5 matrix with rows drawn from iid N_p(0, S)
Z = matrix(rnorm(100*5), nrow = 100, ncol = 5)
out = eigen(S, symmetric = TRUE)
S.sqrt = out$vectors %*% diag(out$values^0.5)
S.sqrt = S.sqrt %*% t(out$vectors)
X = Z %*% S.sqrt

# lasso penalty CV
CVglasso(X)
</code></pre>


</div>