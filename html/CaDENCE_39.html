<div class="container">

<table style="width: 100%;"><tr>
<td>rprop</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>
Resilient backpropagation (Rprop) optimization algorithm
</h2>

<h3>Description</h3>

<p>From Riedmiller (1994): Rprop stands for 'Resilient backpropagation' and is a
local adaptive learning scheme. The basic principle of Rprop is to eliminate the
harmful influence of the size of the partial derivative on the weight step. As a
consequence, only the sign of the derivative is considered to indicate the
direction of the weight update. The size of the weight change is exclusively
determined by a weight-specific, so called 'update-value'.
</p>
<p>This function implements the iRprop+ algorithm from Igel and Huesken (2003).
</p>


<h3>Usage</h3>

<pre><code class="language-R">rprop(w, f, iterlim = 100, print.level = 1, delta.0 = 0.1,
      delta.min = 1e-06, delta.max = 50, epsilon = 1e-08,
      step.tol = 1e-06, f.target = -Inf, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>w</code></td>
<td>

<p>the starting parameters for the minimization.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>f</code></td>
<td>

<p>the function to be minimized. If the function value has an attribute called <code>gradient</code>,
this will be used in the calculation of updated parameter values. Otherwise, numerical
derivatives will be used.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>iterlim</code></td>
<td>

<p>the maximum number of iterations before the optimization is stopped.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>print.level</code></td>
<td>

<p>the level of printing which is done during optimization. A value of <code>0</code> suppresses
any progress reporting, whereas positive values report the value of <code>f</code> and the
mean change in <code>f</code> over the previous three iterations.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>delta.0</code></td>
<td>

<p>size of the initial Rprop update-value.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>delta.min</code></td>
<td>

<p>minimum value for the adaptive Rprop update-value.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>delta.max</code></td>
<td>

<p>maximum value for the adaptive Rprop update-value.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>epsilon</code></td>
<td>

<p>step-size used in the finite difference calculation of the gradient.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>step.tol</code></td>
<td>

<p>convergence criterion. Optimization will stop if the change in <code>f</code> over the previous three iterations falls below this value.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>f.target</code></td>
<td>

<p>target value of <code>f</code>. Optimization will stop if <code>f</code> falls below this value.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>

<p>further arguments to be passed to <code>f</code>.
</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>A list with elements:
</p>
<table>
<tr style="vertical-align: top;">
<td><code>par</code></td>
<td>
<p>The best set of parameters found.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>value</code></td>
<td>
<p>The value of <code>f</code> corresponding to <code>par</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>gradient</code></td>
<td>
<p>An estimate of the gradient at the solution found.</p>
</td>
</tr>
</table>
<h3>References</h3>

<p>Igel, C. and M. Huesken, 2003. Empirical evaluation of the improved Rprop
learning algorithms. Neurocomputing 50: 105-123.
</p>
<p>Riedmiller, M., 1994. Advanced supervised learning in multilayer perceptrons -
from backpropagation to adaptive learning techniques. Computer Standards and
Interfaces 16(3): 265-278.
</p>


</div>