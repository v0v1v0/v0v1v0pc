<div class="container">

<table style="width: 100%;"><tr>
<td>npglpreg</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Generalized Local Polynomial Regression</h2>

<h3>Description</h3>

<p><code>npglpreg</code> computes a generalized local polynomial kernel
regression estimate (Hall and Racine (2015)) of a one (1)
dimensional dependent variable on an <code>r</code>-dimensional vector of
continuous and categorical
(<code>factor</code>/<code>ordered</code>) predictors.
</p>


<h3>Usage</h3>

<pre><code class="language-R">npglpreg(...)

## Default S3 method:
npglpreg(tydat = NULL,
         txdat = NULL,
         eydat = NULL,
         exdat = NULL,
         bws = NULL,
         degree = NULL,
         leave.one.out = FALSE,
         ckertype = c("gaussian", "epanechnikov", "uniform", "truncated gaussian"),
         ckerorder = 2,
         ukertype = c("liracine", "aitchisonaitken"),
         okertype = c("liracine", "wangvanryzin"),
         bwtype = c("fixed", "generalized_nn", "adaptive_nn", "auto"),
         gradient.vec = NULL,
         gradient.categorical = FALSE,
         cv.shrink = TRUE,
         cv.maxPenalty = sqrt(.Machine$double.xmax),
         cv.warning = FALSE,
         Bernstein = TRUE,
         mpi = FALSE,
         ...)

## S3 method for class 'formula'
npglpreg(formula,
         data = list(),
         tydat = NULL,
         txdat = NULL,
         eydat = NULL,
         exdat = NULL,
         bws = NULL,
         degree = NULL,
         leave.one.out = FALSE,
         ckertype = c("gaussian", "epanechnikov","uniform","truncated gaussian"),
         ckerorder = 2,
         ukertype = c("liracine", "aitchisonaitken"),
         okertype = c("liracine", "wangvanryzin"),
         bwtype = c("fixed", "generalized_nn", "adaptive_nn", "auto"),
         cv = c("degree-bandwidth", "bandwidth", "none"),
         cv.func = c("cv.ls", "cv.aic"),
         nmulti = 5,
         random.seed = 42,
         degree.max = 10,
         degree.min = 0,
         bandwidth.max = .Machine$double.xmax,
         bandwidth.min = sqrt(.Machine$double.eps),
         bandwidth.min.numeric = 1.0e-02,
         bandwidth.switch = 1.0e+06,
         bandwidth.scale.categorical = 1.0e+04,
         max.bb.eval = 10000,
         min.epsilon = .Machine$double.eps,
         initial.mesh.size.real = 1,
         initial.mesh.size.integer = 1,
         min.mesh.size.real = sqrt(.Machine$double.eps),
         min.mesh.size.integer = 1, 
         min.poll.size.real = 1,
         min.poll.size.integer = 1, 
         opts=list(),
         restart.from.min = FALSE,
         gradient.vec = NULL,
         gradient.categorical = FALSE,
         cv.shrink = TRUE,
         cv.maxPenalty = sqrt(.Machine$double.xmax),
         cv.warning = FALSE,
         Bernstein = TRUE,
         mpi = FALSE,
         ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>formula</code></td>
<td>
<p> a symbolic description of the model to be fit </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>data</code></td>
<td>
<p> an optional data frame containing the variables in the
model </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>tydat</code></td>
<td>

<p>a one (1) dimensional numeric or integer vector of dependent data, each
element <code class="reqn">i</code> corresponding to each observation (row) <code class="reqn">i</code> of
<code>txdat</code>. Defaults to the training data used to
compute the bandwidth object
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>txdat</code></td>
<td>

<p>a <code class="reqn">p</code>-variate data frame of explanatory data (training data) used to
calculate the regression estimators. Defaults to the training data used to
compute the bandwidth object
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>eydat</code></td>
<td>

<p>a one (1) dimensional numeric or integer vector of the true values
of the dependent variable. Optional, and used only to calculate the
true errors
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>exdat</code></td>
<td>

<p>a <code class="reqn">p</code>-variate data frame of points on which the regression will be
estimated (evaluation data). By default,
evaluation takes place on the data provided by <code>txdat</code>
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>bws</code></td>
<td>

<p>a  vector of bandwidths, with each element <code class="reqn">i</code> corresponding
to the bandwidth for column <code class="reqn">i</code> in <code>txdat</code>
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>degree</code></td>
<td>
<p> integer/vector specifying the polynomial degree of the
for each dimension of the continuous <code>x</code> in <code>txdat</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>leave.one.out</code></td>
<td>

<p>a logical value to specify whether or not to compute the leave one
out sums. Will not work if <code>exdat</code> is specified. Defaults to
<code>FALSE</code>
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>ckertype</code></td>
<td>

<p>character string used to specify the continuous kernel type.
Can be set as <code>gaussian</code>, <code>epanechnikov</code>, or
<code>uniform</code>. Defaults to <code>gaussian</code>.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>ckerorder</code></td>
<td>

<p>numeric value specifying kernel order (one of
<code>(2,4,6,8)</code>). Kernel order specified along with a
<code>uniform</code> continuous kernel type will be ignored. Defaults to
<code>2</code>.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>ukertype</code></td>
<td>

<p>character string used to specify the unordered categorical kernel type.
Can be set as <code>aitchisonaitken</code> or <code>liracine</code>. Defaults to
<code>liracine</code>
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>okertype</code></td>
<td>

<p>character string used to specify the ordered categorical kernel type.
Can be set as <code>wangvanryzin</code> or <code>liracine</code>. Defaults to
<code>liracine</code>
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>bwtype</code></td>
<td>

<p>character string used for the continuous variable bandwidth type,
specifying the type of bandwidth to compute and return in the
<code>bandwidth</code> object. If <code>bwtype="auto"</code>, the bandwidth type
type will be automatically determined by cross-validation. Defaults
to <code>fixed</code>. Option summary:<br><code>fixed</code>: compute fixed bandwidths <br><code>generalized_nn</code>: compute generalized nearest neighbor bandwidths <br><code>adaptive_nn</code>: compute adaptive nearest neighbor bandwidths
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>cv</code></td>
<td>
<p> a character string (default <code>cv="nomad"</code>) indicating
whether to use nonsmooth mesh adaptive direct search, or no search
(i.e. use supplied values for <code>degree</code> and <code>bws</code>)
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>cv.func</code></td>
<td>
<p>a character string (default <code>cv.func="cv.ls"</code>)
indicating which method to use to select smoothing
parameters. <code>cv.aic</code> specifies expected Kullback-Leibler
cross-validation (Hurvich, Simonoff, and Tsai (1998)), and
<code>cv.ls</code> specifies least-squares cross-validation
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>max.bb.eval</code></td>
<td>

<p>argument passed to the NOMAD solver (see <code>snomadr</code> for
further details)
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>min.epsilon</code></td>
<td>

<p>argument passed to the NOMAD solver (see <code>snomadr</code> for
further details)
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>initial.mesh.size.real</code></td>
<td>

<p>argument passed to the NOMAD solver (see <code>snomadr</code> for
further details)
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>initial.mesh.size.integer</code></td>
<td>

<p>argument passed to the NOMAD solver (see <code>snomadr</code> for
further details)
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>min.mesh.size.real</code></td>
<td>

<p>argument passed to the NOMAD solver (see <code>snomadr</code> for
further details)
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>min.mesh.size.integer</code></td>
<td>

<p>arguments passed to the NOMAD solver (see <code>snomadr</code> for
further details)
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>min.poll.size.real</code></td>
<td>

<p>arguments passed to the NOMAD solver (see <code>snomadr</code> for
further details)
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>min.poll.size.integer</code></td>
<td>

<p>arguments passed to the NOMAD solver (see <code>snomadr</code> for
further details)
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>opts</code></td>
<td>
<p> list of optional arguments passed to the NOMAD solver
(see <code>snomadr</code> for further details) </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nmulti</code></td>
<td>

<p>integer number of times to restart the process of finding extrema of
the cross-validation function from different (random) initial
points (default <code>nmulti=5</code>)
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>random.seed</code></td>
<td>
<p> when it is not missing and not equal to 0, the
initial points will be generated using this seed when using
<code>snomadr</code> </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>degree.max</code></td>
<td>
<p> the maximum degree of the polynomial for
each of the continuous predictors (default <code>degree.max=10</code>)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>degree.min</code></td>
<td>
<p> the minimum degree of the polynomial for
each of the continuous predictors (default <code>degree.min=0</code>)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>bandwidth.max</code></td>
<td>
<p> the maximum bandwidth scale (i.e. number of
scaled standard deviations) for each of the continuous predictors
(default <code>bandwidth.max=.Machine$double.xmax</code>)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>bandwidth.min</code></td>
<td>
<p> the minimum bandwidth scale for each of the
categorical predictors (default <code>sqrt(.Machine$double.eps)</code>)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>bandwidth.min.numeric</code></td>
<td>
<p> the minimum bandwidth scale (i.e. number
of scaled standard deviations) for each of the continuous predictors
(default <code>bandwidth.min=1.0e-02</code>)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>bandwidth.switch</code></td>
<td>
<p> the minimum bandwidth scale (i.e. number of
scaled standard deviations) for each of the continuous predictors
(default <code>bandwidth.switch=1.0e+06</code>) before the local polynomial
is treated as global during cross-validation at which point a global
categorical kernel weighted least squares fit is used for
computational efficiency</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>bandwidth.scale.categorical</code></td>
<td>
<p> the upper end for the rescaled
bandwidths for the categorical predictors (default
<code>bandwidth.scale.categorical=1.0e+04</code>) - the aim is to ‘even up’
the scale of the search parameters as much as possible, so when very
large scale factors are selected for the continuous predictors, a
larger value may improve search</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>restart.from.min</code></td>
<td>
<p> a logical value indicating to recommence
<code>snomadr</code> with the optimal values found from its first
invocation (typically quick but sometimes recommended in the field of
optimization)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>gradient.vec</code></td>
<td>
<p> a vector corresponding to the order of the
partial (or cross-partial) and which variable the partial (or
cross-partial) derivative(s) are required </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>gradient.categorical</code></td>
<td>
<p> a logical value indicating whether
discrete gradients (i.e. differences in the response from the base
value for each categorical predictor) are to be computed </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>cv.shrink</code></td>
<td>
<p> a logical value indicating whether to use ridging
(Seifert and Gasser (2000)) for ill-conditioned inversion during
cross-validation (default <code>cv.shrink=TRUE</code>) or to instead test
for ill-conditioned matrices and penalize heavily when this is the
case (much stronger condition imposed on cross-validation)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>cv.maxPenalty</code></td>
<td>
<p> a penalty applied during cross-validation when a
delete-one estimate is not finite or the polynomial basis is not of
full column rank </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>cv.warning</code></td>
<td>
<p> a logical value indicating whether to issue an
immediate warning message when ill conditioned bases are encountered
during cross-validation (default <code>cv.warning=FALSE</code>) </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Bernstein</code></td>
<td>
<p> a logical value indicating whether to use raw
polynomials or Bernstein polynomials (default) (note that a Bernstein
polynomial is also know as a Bezier curve which is also a
B-spline with no interior knots)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>mpi</code></td>
<td>
<p> a logical value (default <code>mpi=FALSE</code>) that, when
<code>mpi=TRUE</code>, can call the <code>npRmpi</code> rather than the <code>np</code>
package (note - code needs to mirror examples in the demo directory of
the <code>npRmpi</code> package, you need to broadcast loading of the
<code>crs</code> package, and need <code>.Rprofile</code> in your current
directory)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>

<p>additional arguments supplied to specify the regression type,
bandwidth type, kernel types, training data, and so on, detailed
below
</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Typical usages are (see below for a  list of options and also
the examples at the end of this help file)
</p>
<pre>
    ## Conduct generalized local polynomial estimation
    
    model &lt;- npglpreg(y~x1+x2)
    
    ## Conduct degree 0 local polynomial estimation
    ## (i.e. Nadaraya-Watson)
    
    model &lt;- npglpreg(y~x1+x2,cv="bandwidth",degree=c(0,0))    
    
    ## Conduct degree 1 local polynomial estimation (i.e. local linear)
    
    model &lt;- npglpreg(y~x1+x2,cv="bandwidth",degree=c(1,1))    
    
    ## Conduct degree 2 local polynomial estimation (i.e. local
    ## quadratic)
    
    model &lt;- npglpreg(y~x1+x2,cv="bandwidth",degree=c(2,2))

    ## Plot the mean and bootstrap confidence intervals

    plot(model,ci=TRUE)

    ## Plot the first partial derivatives and bootstrap confidence
    ## intervals

    plot(model,deriv=1,ci=TRUE)

    ## Plot the first second partial derivatives and bootstrap
    ## confidence intervals

    plot(model,deriv=2,ci=TRUE)        
    
  </pre>
<p>This function is in beta status until further notice (eventually it
will be rolled into the np/npRmpi packages after the final status of
snomadr/NOMAD gets sorted out).
</p>
<p>Optimizing the cross-validation function jointly for bandwidths
(vectors of continuous parameters) and polynomial degrees (vectors of
integer parameters) constitutes a mixed-integer optimization
problem. These problems are not only ‘hard’ from the numerical
optimization perspective, but are also computationally intensive
(contrast this to where we conduct, say, local linear regression which
sets the degree of the polynomial vector to a global value
<code>degree=1</code> hence we only need to optimize with respect to the
continuous bandwidths). Because of this we must be mindful of the
presence of local optima (the objective function is non-convex and
non-differentiable). Restarting search from different initial starting
points is recommended (see <code>nmulti</code>) and by default this is done
more than once. We encourage users to adopt ‘multistarting’ and
to investigate the impact of changing default search parameters such
as <code>initial.mesh.size.real</code>, <code>initial.mesh.size.integer</code>,
<code>min.mesh.size.real</code>,
<code>min.mesh.size.integer</code>,<code>min.poll.size.real</code>, and
<code>min.poll.size.integer</code>. The default values were chosen based on
extensive simulation experiments and were chosen so as to yield robust
performance while being mindful of excessive computation - of course,
no one setting can be globally optimal.
</p>


<h3>Value</h3>

<p><code>npglpreg</code> returns a <code>npglpreg</code> object.  The generic
functions <code>fitted</code> and <code>residuals</code> extract
(or generate) estimated values and residuals. Furthermore, the
functions <code>summary</code>, <code>predict</code>, and
<code>plot</code> (options <code>deriv=0</code>, <code>ci=FALSE</code>
[<code>ci=TRUE</code> produces pointwise bootstrap error bounds],
<code>persp.rgl=FALSE</code>,
<code>plot.behavior=c("plot","plot-data","data")</code>,
<code>plot.errors.boot.num=99</code>,
<code>plot.errors.type=c("quantiles","standard")</code>
[<code>"quantiles"</code> produces percentiles determined by
<code>plot.errors.quantiles</code> below, <code>"standard"</code> produces error
bounds given by +/- 1.96 bootstrap standard deviations],
<code>plot.errors.quantiles=c(.025,.975)</code>, <code>xtrim=0.0</code>,
<code>xq=0.5</code>) support objects of this type. The returned object has
the following components:
</p>
<table>
<tr style="vertical-align: top;">
<td><code>fitted.values</code></td>
<td>
<p> estimates of the regression function
(conditional mean) at the sample points or evaluation points </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>residuals</code></td>
<td>
<p> residuals computed at the sample points or
evaluation points </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>degree</code></td>
<td>
<p> integer/vector specifying the degree of the polynomial
for each dimension of the continuous <code>x</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>gradient</code></td>
<td>
<p> the estimated gradient (vector) corresponding to the vector
<code>gradient.vec</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>gradient.categorical.mat</code></td>
<td>
<p> the estimated gradient (matrix) for
the categorical predictors </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>gradient.vec</code></td>
<td>
<p> the supplied <code>gradient.vec</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>bws</code></td>
<td>
<p> vector of bandwidths </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>bwtype</code></td>
<td>
<p> the supplied <code>bwtype</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>call</code></td>
<td>
<p> a symbolic description of the model  </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>r.squared</code></td>
<td>
<p> coefficient of determination (Doksum and Samarov (1995))</p>
</td>
</tr>
</table>
<h3>Note</h3>

<p>Note that the use of raw polynomials (<code>Bernstein=FALSE</code>) for
approximation is appealing as they can be computed and differentiated
easily, however, they can be unstable (their inversion can be ill
conditioned) which can cause problems in some instances as the order
of the polynomial increases. This can hamper search when excessive
reliance on ridging to overcome ill conditioned inversion becomes
computationally burdensome.
</p>
<p><code>npglpreg</code> tries to detect whether this is an issue or not when
<code>Bernstein=FALSE</code> for each <code>numeric</code> predictor and will
adjust the search range for <code>snomadr</code> and the degree fed
to <code>npglpreg</code> if appropriate.
</p>
<p>However, if you suspect that this might be an issue for your specific
problem and you are using raw polynomials (<code>Bernstein=FALSE</code>),
you are encouraged to investigate this by limiting <code>degree.max</code>
to value less than the default value (say <code>3</code>). Alternatively,
you might consider re-scaling your <code>numeric</code> predictors to lie in
<code class="reqn">[0,1]</code> using <code>scale</code>.
</p>
<p>For a given predictor <code class="reqn">x</code> you can readily determine if this is an
issue by considering the following: Suppose <code class="reqn">x</code> is given by
</p>
<pre>
    x &lt;- runif(100,10000,11000)
    y &lt;- x + rnorm(100,sd=1000)
  </pre>
<p>so that a polynomial of order, say, <code class="reqn">5</code> would be ill
conditioned. This would be apparent if you considered
</p>
<pre>
    X &lt;- poly(x,degree=5,raw=TRUE)
    solve(t(X)%*%X)
  </pre>
<p>which will throw an error when the polynomial is ill conditioned,
or
</p>
<pre>
    X &lt;- poly(x,degree=5,raw=TRUE)
    lm(y~X)
  </pre>
<p>which will return <code>NA</code> for one or more coefficients when this is
an issue.
</p>
<p>In such cases you might consider transforming your <code>numeric</code>
predictors along the lines of the following:
</p>
<pre>
    x &lt;- as.numeric(scale(x))
    X &lt;- poly(x,degree=5,raw=TRUE)
    solve(t(X)%*%X)
    lm(y~X)    
  </pre>
<p>Note that now your least squares coefficients (i.e. first derivative
of <code class="reqn">y</code> with respect to <code class="reqn">x</code>) represent the effect of a one
standard deviation change in <code class="reqn">x</code> and not a one unit change.
</p>
<p>Alternatively, you can use Bernstein polynomials by not setting
<code>Bernstein=FALSE</code>.
</p>


<h3>Author(s)</h3>

<p>Jeffrey S. Racine <a href="mailto:racinej@mcmaster.ca">racinej@mcmaster.ca</a> and Zhenghua Nie <a href="mailto:niez@mcmaster.ca">niez@mcmaster.ca</a>
</p>


<h3>References</h3>

<p>Doksum, K. and A. Samarov (1995), “Nonparametric Estimation of
Global Functionals and a Measure of the Explanatory Power of
Covariates in Regression,” The Annals of Statistics, 23, 1443-1473.
</p>
<p>Hall, P. and J.S. Racine (2015), “Infinite Order
Cross-Validated Local Polynomial Regression,” Journal of Econometrics,
185, 510-525.
</p>
<p>Li, Q. and J.S. Racine (2007), <em>Nonparametric Econometrics:
Theory and Practice,</em> Princeton University Press.
</p>
<p>Seifert, B. and T. Gasser (2000), “Data Adaptive Ridging in
Local Polynomial Regression,” Journal of Computational and Graphical
Statistics, 9(2), 338-360.
</p>


<h3>See Also</h3>

<p><code>npreg</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">## Not run: 
set.seed(42)
n &lt;- 100
x1 &lt;- runif(n,-2,2)
x2 &lt;- runif(n,-2,2)
y &lt;- x1^3 + rnorm(n,sd=1)

## Ideally the method should choose large bandwidths for x1 and x2 and a
## generalized polynomial that is a cubic for x1 and degree 0 for x2.

model &lt;- npglpreg(y~x1+x2,nmulti=1)
summary(model)

## Plot the partial means and percentile confidence intervals
plot(model,ci=T)
## Extract the data from the plot object and plot it separately
myplot.dat &lt;- plot(model,plot.behavior="data",ci=T)
matplot(myplot.dat[[1]][,1],myplot.dat[[1]][,-1],type="l")
matplot(myplot.dat[[2]][,1],myplot.dat[[2]][,-1],type="l")

## End(Not run)
</code></pre>


</div>