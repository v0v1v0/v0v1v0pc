<div class="container">

<table style="width: 100%;"><tr>
<td>kullCOP</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Kullback–Leibler Divergence, Jeffrey Divergence, and Kullback–Leibler Sample Size</h2>

<h3>Description</h3>

<p>Compute the <em>Kullback–Leibler Divergence</em>, <em>Jeffrey Divergence</em>, and <em>Kullback–Leibler sample size</em> following Joe (2014, pp. 234–237). Consider two densities <code class="reqn">f = c_1(u,v; \Theta_f)</code> and <code class="reqn">g = c_2(u,v; \Theta_g)</code> for two different bivariate copulas <code class="reqn">\mathbf{C}_1(\Theta_1)</code> and <code class="reqn">\mathbf{C}_2(\Theta_2)</code> having respective parameters <code class="reqn">\Theta</code>, then the Kullback–Leibler Divergence of <code class="reqn">f</code> relative to <code class="reqn">g</code> is
</p>
<p style="text-align: center;"><code class="reqn">\mathrm{KL}(f {\mid} g) = \int\!\!\int_{\mathcal{I}^2} g\, \log(g/f)\,\mathrm{d}u\mathrm{d}v\mbox{,}</code>
</p>

<p>and Kullback–Leibler Divergence of <code class="reqn">g</code> relative to <code class="reqn">f</code> is
</p>
<p style="text-align: center;"><code class="reqn">\mathrm{KL}(g {\mid} f) = \int\!\!\int_{\mathcal{I}^2} f\, \log(f/g)\,\mathrm{d}u\mathrm{d}v\mbox{,}</code>
</p>

<p>where the limits of integration <code class="reqn">\mathcal{I}^2</code> theoretically are closed on <code class="reqn">[0,1]^2</code> but an open interval <code class="reqn">(0,1)^2</code> might be needed for numerical integration. Note, in general <code class="reqn">\mathrm{KL}(f {\mid} g) \ne \mathrm{KL}(g {\mid} f)</code>. The <code class="reqn">\mathrm{KL}(f {\mid} g)</code> is the expected log-likelihood ratios of <code class="reqn">g</code> to <code class="reqn">f</code> when <code class="reqn">g</code> is the true density (Joe, 2014, p. 234), whereas <code class="reqn">\mathrm{KL}(g {\mid} f)</code> is the opposite.
</p>
<p>This asymmetry leads to Jeffrey Divergence, which is defined as a symmetrized version of the two Kullback–Leibler Divergences, and is
</p>
<p style="text-align: center;"><code class="reqn">J(f,g) = \mathrm{KL}(f {\mid} g) + \mathrm{KL}(g {\mid} f) = \int\!\!\int_{\mathcal{I}^2} (g-f)\, \log(g/f)\,\mathrm{d}u\mathrm{d}v\mbox{.}</code>
</p>

<p>The variances of the Kullback–Leibler Divergences are defined as
</p>
<p style="text-align: center;"><code class="reqn">\sigma^2_{\mathrm{KL}(f {\mid} g)} = \int\!\!\int_{\mathcal{I}^2} g\,[\log(g/f)]^2\,\mathrm{d}u\mathrm{d}v - [\mathrm{KL}(f|g)]^2\mbox{,}</code>
</p>

<p>and
</p>
<p style="text-align: center;"><code class="reqn">\sigma^2_{\mathrm{KL}(g {\mid} f)} = \int\!\!\int_{\mathcal{I}^2} f\,[\log(f/g)]^2\,\mathrm{d}u\mathrm{d}v - [\mathrm{KL}(g|f)]^2\mbox{.}</code>
</p>

<p>For comparison of copula families <code class="reqn">f</code> and <code class="reqn">g</code> and taking an <code class="reqn">\alpha = 0.05</code>, the Kullback–Leibler sample size is defined as
</p>
<p style="text-align: center;"><code class="reqn">n_{f\!g} = \bigl[\Phi^{(-1)}(1-\alpha) \times \eta_\mathrm{KL}\bigr]^2\mbox{,}</code>
</p>

<p>where <code class="reqn">\Phi^{(-1)}(t)</code> is the quantile function for the standard normal distribution <code class="reqn">\sim</code> N(0,1) for nonexceedance probability <code class="reqn">t</code>, and <code class="reqn">\eta_\mathrm{KL}</code> is the maximum of
</p>
<p style="text-align: center;"><code class="reqn">\eta_\mathrm{KL} = \mathrm{max}\bigl[\sigma_{\mathrm{KL}(f {\mid} g)}/\mathrm{KL}(f {\mid} g),\, \sigma_{\mathrm{KL}(g {\mid} f)}/\mathrm{KL}(g {\mid} f)\bigr]\mbox{.}</code>
</p>

<p>The <code class="reqn">n_{f\!g}</code> gives an indication of the sample size needed to distinguish <code class="reqn">f</code> and <code class="reqn">g</code> with a probability of at least <code class="reqn">1 - \alpha = 1 - 0.05 = 0.95</code> or 95 percent.
</p>
<p>The <span class="pkg">copBasic</span> features a naïve <em>Monte Carlo integration</em> scheme in the primary interface <code>kullCOP</code>, although the function <code>kullCOPint</code> provides for nested numerical integration. This later function is generally fast but suffers too much for general application from integral divergencies issued from the <code>integrate()</code> function in <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span>—this must be judged in the light that the <span class="pkg">copBasic</span> package focuses only on implementation of the function of the copula itself and numerical estimation of copula density (<code>densityCOP</code>) and not analytical copula densities or hybrid representations thereof. Sufficient “bread crumbs” are left among the code and documentation for users to re-implement if speed is paramount. Numerical comparison to the results of Joe (2014) (see <b>Examples</b>) suggests that the default Monte Carlo sample size should be more than sufficient for general inference with the expense of considerable CPU time; however, a couple of repeated calls of <code>kullCOP</code> would be advised and compute say the mean of the resulting sample sizes.
</p>


<h3>Usage</h3>

<pre><code class="language-R">kullCOP(cop1=NULL, cop2=NULL, para1=NULL, para2=NULL, alpha=0.05,
           del=0, n=1E5, verbose=TRUE, sobol=FALSE, scrambling=0, ...)

kullCOPint(cop1=NULL, cop2=NULL, para1=NULL, para2=NULL, alpha=0.05,
           del=.Machine$double.eps^0.25, verbose=TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>cop1</code></td>
<td>
<p>A copula function corresponding to copula <code class="reqn">f</code> in Joe (2014);</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>para1</code></td>
<td>
<p>Vector of parameters or other data structure, if needed, to pass to the copula <code class="reqn">f</code>;</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>cop2</code></td>
<td>
<p>A copula function corresponding to copula <code class="reqn">g</code> in Joe (2014);</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>para2</code></td>
<td>
<p>Vector of parameters or other data structure, if needed, to pass to the copula <code class="reqn">g</code>;</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>alpha</code></td>
<td>
<p>The <code class="reqn">\alpha</code> in the Kullback–Leibler sample size equation;</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>del</code></td>
<td>
<p>A small value used to denote the <code>lo</code> and <code>hi</code> values of the numerical integration: <code>lo = del</code> and <code>hi = 1 - del</code>. If <code>del == 0</code>, then <code>lo = 0</code> and <code>hi = 1</code>, which corresponds to the theoretical limits <code class="reqn">\mathcal{I}^2 = [0,1]^2</code> and are defaulted here to <code class="reqn">[0,1]^2</code> because the Monte Carlo algorithm is preferred for general application. The end point control, however, is maintained just in case pathological situations should arise;</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>n</code></td>
<td>
<p><code>kullCOP</code> (Monte Carlo integration) only—the Monte Carlo integration simulation size;</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>verbose</code></td>
<td>
<p>A logical trigging a couple of status lines of output through the <code>message()</code> function in <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span>;</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>sobol</code></td>
<td>
<p>A logical trigging <em>Sobol sequences</em> for the Monte Carlo integration instead of the bivariate uniform distribution. The Sobol sequences are dependent on the <span class="pkg">randtoolbox</span> package and the <code>sobol()</code> function of the <span class="pkg">randtoolbox</span> package, and the Sobol sequences canvas the <code class="reqn">\mathcal{I}^2</code> domain for smaller <code class="reqn">n</code> values than required if statistical independence is used for the Monte Carlo integration. Note, the <span class="pkg">randtoolbox</span> at least at version 2.0.+ has “scrambling” of Sobol sequences temporarily disabled, and hence <code>scrambling=0</code> as default for <code>kullCOP</code>;</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>scrambling</code></td>
<td>
<p>The argument of the same name for <code>randtoolbox::sobol</code>; and</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>Additional arguments to pass to the <code>densityCOP</code> function.</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>An <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> <code>list</code> is returned having the following components:
</p>
<table>
<tr style="vertical-align: top;">
<td><code>MonteCarlo.sim.size</code></td>
<td>
<p><code>kullCOP</code> (Monte Carlo integration) only—The simulation size for numerical integration;</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>divergences</code></td>
<td>
<p>A vector of the Kullback–Leibler Divergences and their standard deviations: <code class="reqn">\mathrm{KL}(f {\mid} g)</code>, <code class="reqn">\sigma_{\mathrm{KL}(f {\mid} g)}</code>, <code class="reqn">\mathrm{KL}(g {\mid} f)</code>, and <code class="reqn">\sigma_{\mathrm{KL}(g {\mid} f)}</code>, respectively;</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>stdev.divergences</code></td>
<td>
<p><code>kullCOP</code> (Monte Carlo integration) only—The standard deviation of the divergences and the variances;</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Jeffrey.divergence</code></td>
<td>
<p>Jeffrey Divergence <code class="reqn">J(f,g)</code>;</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>KL.sample.size</code></td>
<td>
<p>Kullback–Leibler sample size <code class="reqn">n_{f\!g}</code>; and</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>integrations</code></td>
<td>
<p><code>kullCOPint</code> (numerical integration) only—An <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> <code>list</code> of the outer call of the <code>integrate()</code> function for the respective numerical integrals shown in this documentation.</p>
</td>
</tr>
</table>
<h3>Author(s)</h3>

<p>W.H. Asquith</p>


<h3>References</h3>

<p>Joe, H., 2014, Dependence modeling with copulas: Boca Raton, CRC Press, 462 p.
</p>


<h3>See Also</h3>

<p><code>densityCOP</code>, <code>vuongCOP</code></p>


<h3>Examples</h3>

<pre><code class="language-R"># See another demonstration under the Note section of statTn().
## Not run: 
# Joe (2014, p. 237, table 5.2)
# Gumbel-Hougaard and Plackett copulas below each have a Kendall Tau of about 0.5, and
# Joe (2014) lists in the table that Jeffrey Divergence is about 0.110 and Kullback-Leibler
# sample size is 133. Joe (2014) does not list the copula parameters just says Tau = 0.5.
# Joe (2014) likely did the numerical integrations using analytical solutions to probability
# densities and not rectangular approximations as in copBasic::densityCOP().
set.seed(1)
KL &lt;- kullCOP(cop1=GHcop,       para1=2,
              cop2=PLACKETTcop, para2=11.40484, sobol=FALSE)
message("Jeffery Divergence is ",          round(KL$Jeffrey.divergence, digits=4),
        " and Kullback-Leibler sample size is ", KL$KL.sample.size, ".")
# Jeffery Divergence is 0.1106 and Kullback-Leibler sample size is 137.
set.seed(1)
KL &lt;- kullCOP(cop1=GHcop,       para1=2,
              cop2=PLACKETTcop, para2=11.40484, sobol=TRUE )
message("Jeffery Divergence is ",          round(KL$Jeffrey.divergence, digits=4),
        " and Kullback-Leibler sample size is ", KL$KL.sample.size, ".")
# Jeffery Divergence is 0.3062 and Kullback-Leibler sample size is 136.


set.seed(1)
S &lt;- replicate(20, kullCOP(cop1=GHcop, para1=2, cop2=PLACKETTcop, sobol=FALSE,
                           para2=11.40484, verbose=FALSE)$KL.sample.size)
print(as.integer(c(mean(S), sd(S)))) # 132 plus/minus 5
S &lt;- replicate(2 , kullCOP(cop1=GHcop, para1=2, cop2=PLACKETTcop,  sobol=TRUE,
                           para2=11.40484, verbose=FALSE)$KL.sample.size)
# The two S in the later replication are both the same (136) for a sobol=TRUE
# does not produce variation and this is thought (June 2023) as a result
# of the disabled scrambling in the randtoolbox::sobol() function. 
## End(Not run)

## Not run: 
# Joe (2014, p. 237, table 5.3)
# Gumbel-Hougaard and Plackett copulas below each have a Spearman Rho of about 0.5, and
# Joe (2014) lists in the table that Jeffrey Divergence is about 0.063 and Kullback-Leibler
# sample size is 210. Joe (2014) does not list the parameters and just says that Rho = 0.5.
# Joe (2014) likely did the numerical integrations using analytical solutions to probability
# densities and not rectangular approximations as in copBasic::densityCOP().
set.seed(1)
KL &lt;- kullCOP(cop1=GHcop,       para1=1.541071,
              cop2=PLACKETTcop, para2=5.115658, sobol=FALSE)
message("Jeffery Divergence is ",          round(KL$Jeffrey.divergence, digits=4),
        " and Kullback-Leibler sample size is ", KL$KL.sample.size, ".")
# Jeffery Divergence is 0.0642 and Kullback-Leibler sample size is 213.
set.seed(1)
KL &lt;- kullCOP(cop1=GHcop,       para1=1.541071,
              cop2=PLACKETTcop, para2=5.115658, sobol=TRUE )
message("Jeffery Divergence is ",          round(KL$Jeffrey.divergence, digits=4),
        " and Kullback-Leibler sample size is ", KL$KL.sample.size, ".")
# Jeffery Divergence is 0.2001 and Kullback-Leibler sample size is 206.


set.seed(1)
S &lt;- replicate(20, kullCOP(cop1=GHcop, para1=1.541071, cop2=PLACKETTcop,
                           para2=5.115658, verbose=FALSE)$KL.sample.size)
print(as.integer(c(mean(S), sd(S))))  # 220 plus/minus 19 
## End(Not run)

## Not run: 
# Compare Jeffery Divergence estimates as functions of sample size when computed
# using Sobol sequences or not for Gumbel-Hougaard and Pareto copulas.
GHpar &lt;- PApar &lt;- 2 # Spearman Rho = 0.6822339
Ns &lt;- as.integer(10^c(seq(2.0, 3.5, by=0.01), seq(3.6, 5, by=0.05)))
JDuni &lt;- sapply(1:length(Ns), function(i) {
                  kullCOP(cop1=GHcop, para1=GHpar, verbose=FALSE,
                          cop2=PAcop, para2=PApar, n=Ns[i],
                          sobol=FALSE)$Jeffrey.divergence })
JDsob &lt;- sapply(1:length(Ns), function(i) {
                  kullCOP(cop1=GHcop, para1=GHpar, verbose=FALSE,
                          cop2=PAcop, para2=PApar, n=Ns[i],
                          sobol=TRUE )$Jeffrey.divergence })
plot(Ns, JDuni, type="l", log="x", # black line, notice likely outliers too
     xlab="Simulation Sample Size", ylab="Jeffery Divergence")
lines(Ns, JDsob, col="red") # red line
legend("topright", c("Monte Carlo", "Sobol sequence"),
                   lwd=c(1,1), col=c("black", "red"), bty="n")
print( c( mean(JDuni), sd(JDuni) ) ) # [1] 0.05915608 0.01284682
print( c( mean(JDsob), sd(JDsob) ) ) # [1] 0.07274190 0.01838939

# The developer notes that plotting KL.sample.size for sobol=TRUE shows
# what appears to be numerical blow up but the Jeffery Divergence does not.
KLuni &lt;- sapply(1:length(Ns), function(i) {
                  kullCOP(cop1=GHcop, para1=GHpar, verbose=FALSE,
                          cop2=PAcop, para2=PApar, n=Ns[i],
                          sobol=FALSE)$KL.sample.size })
KLsob &lt;- sapply(1:length(Ns), function(i) {
                  kullCOP(cop1=GHcop, para1=GHpar, verbose=FALSE,
                          cop2=PAcop, para2=PApar, n=Ns[i],
                          sobol=TRUE )$KL.sample.size })
plot(Ns, KLuni, type="l", log="xy", # black line, notice likely outliers too
     xlab="Simulation Sample Size", ylab="Kullback-Leibler Sample Size")
lines(Ns, KLsob, col="red") # red line
nideal &lt;- kullCOPint(cop1=GHcop, para1=GHpar, cop2=PAcop, para2=PApar)$KL.sample.size
abline(h=nideal, col="green", lwd=3) # nideal sample size is about 210
legend("topright", c("Monte Carlo", "Sobol sequence", "Judged ideal sample size"),
                   lwd=c(1,1,3), col=c("black", "red", "green"), bty="n")

# Let us attempt a visualization to highlight the differences in the two copula by
# simulation. First, using this n = nideal, being the apparent sample size to distinguish
# generally between the two copula having the same Spearman Rho. Do the segments help
# to visually highlight the differences? Next, ask would one judge the parents in the
# simulation being different knowing same Spearman Rho? (Note, the segments are all
# vertical because the U axis is the simulation and the V axis is the conditional
# probability given the U.)
set.seed(1); UVgh &lt;- simCOP(nideal, GHcop, para=GHpar, graphics=FALSE)
set.seed(1); UVpa &lt;- simCOP(nideal, PAcop, para=PApar, graphics=FALSE)
plot(c(0,1), c(0,1), type="n", xlab="U, nonexceedance probability",
                               ylab="V, nonexceedance probability")
segments(UVgh[,1], UVgh[,2], x1=UVpa[,1], y1=UVpa[,2])
points(UVgh, col="lightgreen", pch=16, cex=0.8) # dots
points(UVpa, col="darkgreen",  pch= 1, lwd=0.8) # circles
# Repeat the above n = nideal visualizations but with a change to n = nideal*10, and see
# then that there are visually striking shifts systematically in both both tails but also
# in the U in the interval (0.3, 0.7) belt but to a smaller degree than seen in the tails. 
## End(Not run)
</code></pre>


</div>